{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb70d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes sliding window classification accuracy for orofacial movements, single phonemes, and single words for all 4\n",
    "#arrays (Fig 1e)\n",
    "baseDir = '/oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3131d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "phonemesDat = scipy.io.loadmat(baseDir+'tuningTasks/t12.2022.04.26_phonemes.mat')\n",
    "orofacialDat = scipy.io.loadmat(baseDir+'tuningTasks/t12.2022.04.21_orofacial.mat')\n",
    "fiftyWordDat = scipy.io.loadmat(baseDir+'tuningTasks/t12.2022.05.03_fiftyWordSet.mat')\n",
    "allDatasets = [orofacialDat, phonemesDat, fiftyWordDat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757d3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean-subtract within block\n",
    "import numpy as np\n",
    "def meanSubtract(dat):\n",
    "    dat['feat'] = dat['tx2'].astype(np.float32)    \n",
    "    blockList = np.squeeze(np.unique(dat['blockNum']))\n",
    "    for b in blockList:\n",
    "        loopIdx = np.squeeze(dat['blockNum']==b)\n",
    "        dat['feat'][loopIdx,:] -= np.mean(dat['feat'][loopIdx,:],axis=0,keepdims=True)\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a6dd7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 13:58:58.245981: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 13:58:58.448383: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-14 13:59:04.434291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/software/user/open/gcc/10.1.0/lib64:/share/software/user/open/gcc/10.1.0/lib/gcc/x86_64-pc-linux-gnu:/share/software/user/open/gcc/10.1.0/lib:/share/software/user/open/cudnn/8.6.0.163/lib:/usr/lib64/nvidia:/share/software/user/open/cuda/11.5.0/targets/x86_64-linux/lib64:/share/software/user/open/cuda/11.5.0/lib64:/share/software/user/open/cuda/11.5.0/nvvm/lib64:/share/software/user/open/cuda/11.5.0/extras/Debugger/lib64:/share/software/user/open/cuda/11.5.0/extras/CUPTI/lib64:/share/software/user/open/python/3.9.0/lib:/share/software/user/open/libffi/3.2.1/lib64:/share/software/user/open/sqlite/3.37.2/lib:/share/software/user/open/readline/7.0/lib:/share/software/user/open/tcltk/8.6.6/lib:/share/software/user/open/libressl/3.2.1/lib:/share/software/user/open/zlib/1.2.11/lib\n",
      "2023-06-14 13:59:04.434458: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/software/user/open/gcc/10.1.0/lib64:/share/software/user/open/gcc/10.1.0/lib/gcc/x86_64-pc-linux-gnu:/share/software/user/open/gcc/10.1.0/lib:/share/software/user/open/cudnn/8.6.0.163/lib:/usr/lib64/nvidia:/share/software/user/open/cuda/11.5.0/targets/x86_64-linux/lib64:/share/software/user/open/cuda/11.5.0/lib64:/share/software/user/open/cuda/11.5.0/nvvm/lib64:/share/software/user/open/cuda/11.5.0/extras/Debugger/lib64:/share/software/user/open/cuda/11.5.0/extras/CUPTI/lib64:/share/software/user/open/python/3.9.0/lib:/share/software/user/open/libffi/3.2.1/lib64:/share/software/user/open/sqlite/3.37.2/lib:/share/software/user/open/readline/7.0/lib:/share/software/user/open/tcltk/8.6.6/lib:/share/software/user/open/libressl/3.2.1/lib:/share/software/user/open/zlib/1.2.11/lib\n",
      "2023-06-14 13:59:04.434471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#mean subtraction\n",
    "from analysis import unscrambleChans, triggeredAvg\n",
    "for x in range(len(allDatasets)):\n",
    "    allDatasets[x] = meanSubtract(allDatasets[x])\n",
    "    allDatasets[x]['feat'][:,0:128] = unscrambleChans(allDatasets[x]['feat'][:,0:128])\n",
    "    allDatasets[x]['feat'][:,128:] = unscrambleChans(allDatasets[x]['feat'][:,128:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8808094d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_trials = []\n",
    "for x in range(len(allDatasets)):\n",
    "    fAvg, fCI, trials_go = triggeredAvg(allDatasets[x]['feat'].astype(np.float32), \n",
    "                             allDatasets[x]['goTrialEpochs'][:,0],\n",
    "                             np.squeeze(allDatasets[x]['trialCues']), [-25,125], smoothSD=4)\n",
    "\n",
    "    fAvg, fCI, trials_delay = triggeredAvg(allDatasets[x]['feat'].astype(np.float32), \n",
    "                             allDatasets[x]['delayTrialEpochs'][:,0],\n",
    "                             np.squeeze(allDatasets[x]['trialCues']), [0,150], smoothSD=4)\n",
    "    \n",
    "    dataset_trials.append([trials_go, trials_delay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26fa9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiftyWordCuesToUse = np.arange(1,51).astype(np.int32)\n",
    "phonemeCuesToUse = np.array([0,1,3,4,5,6,7,8,9,10,11,12,14,15,16,17,19,20,21,22,23,24,\n",
    "                      26,27,28,29,30,31,32,33,34,35,36,37,38,39]).astype(np.int32)\n",
    "oroCuesToUse = np.array([0,1,2,3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,\n",
    "                         21,22,23,24,25,26,27,28,29,30,31,32,33]).astype(np.int32)\n",
    "allCues = [oroCuesToUse, phonemeCuesToUse, fiftyWordCuesToUse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc4162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1244309559939302\n",
      "0.1456752655538695\n",
      "0.13808801213960548\n",
      "0.12898330804248861\n",
      "0.13201820940819423\n",
      "0.1456752655538695\n",
      "0.1471927162367223\n",
      "0.15629742033383914\n",
      "0.1471927162367223\n",
      "0.1411229135053111\n",
      "0.13960546282245828\n",
      "0.13808801213960548\n",
      "0.11077389984825493\n",
      "0.12291350531107739\n",
      "0.11229135053110774\n",
      "0.11836115326251896\n",
      "0.13657056145675264\n",
      "0.11987860394537178\n",
      "0.1456752655538695\n",
      "0.15326251896813353\n",
      "0.14871016691957512\n",
      "0.15174506828528073\n",
      "0.1471927162367223\n",
      "0.17147192716236723\n",
      "0.165402124430956\n",
      "0.165402124430956\n",
      "0.17147192716236723\n",
      "0.1669195751138088\n",
      "0.13960546282245828\n",
      "0.1456752655538695\n",
      "0.1638846737481032\n",
      "0.13353566009104703\n",
      "0.15781487101669195\n",
      "0.1669195751138088\n",
      "0.16995447647951442\n",
      "0.19575113808801214\n",
      "0.18816388467374812\n",
      "0.19878603945371776\n",
      "0.18512898330804248\n",
      "0.22154779969650987\n",
      "0.2276176024279211\n",
      "0.24279210925644917\n",
      "0.26555386949924126\n",
      "0.27921092564491656\n",
      "0.27617602427921095\n",
      "0.291350531107739\n",
      "0.291350531107739\n",
      "0.2852807283763278\n",
      "0.26100151745068284\n",
      "0.26858877086494687\n",
      "0.27769347496206376\n",
      "0.27769347496206376\n",
      "0.30197268588770865\n",
      "0.3216995447647951\n",
      "0.30349013657056145\n",
      "0.30652503793626706\n",
      "0.29893778452200304\n",
      "0.29893778452200304\n",
      "0.2867981790591806\n",
      "0.2731411229135053\n",
      "0.26251896813353565\n",
      "0.26707132018209406\n",
      "0.2746585735963581\n",
      "0.2898330804248862\n",
      "0.26707132018209406\n",
      "0.2701062215477997\n",
      "0.2822458270106222\n",
      "0.2898330804248862\n",
      "0.23975720789074356\n",
      "0.2488619119878604\n",
      "0.2564491654021244\n",
      "0.23672230652503792\n",
      "0.2321699544764795\n",
      "0.21851289833080426\n",
      "0.23520485584218512\n",
      "0.22610015174506828\n",
      "0.212443095599393\n",
      "0.19878603945371776\n",
      "0.18209408194233687\n",
      "0.19575113808801214\n",
      "0.19878603945371776\n",
      "0.20182094081942337\n",
      "0.18664643399089528\n",
      "0.18664643399089528\n",
      "0.17147192716236723\n",
      "0.15477996965098634\n",
      "0.1456752655538695\n",
      "0.15326251896813353\n",
      "0.1441578148710167\n",
      "0.13505311077389984\n",
      "0.12898330804248861\n",
      "0.13657056145675264\n",
      "0.12898330804248861\n",
      "0.13201820940819423\n",
      "0.09711684370257967\n",
      "0.11077389984825493\n",
      "0.1274658573596358\n",
      "0.11532625189681335\n",
      "0.1244309559939302\n",
      "0.12898330804248861\n",
      "0.1062215477996965\n",
      "0.10166919575113809\n",
      "0.08801213960546282\n",
      "0.0622154779969651\n",
      "0.08801213960546282\n",
      "0.08801213960546282\n",
      "0.07738998482549317\n",
      "0.07132018209408195\n",
      "0.07587253414264036\n"
     ]
    }
   ],
   "source": [
    "from analysis import gnb_loo, bootCI\n",
    "#get classification accuracy for each window & task (plus 95% CIs computed via bootstrap), this may take a while\n",
    "dataset_results = []\n",
    "for datasetIdx in range(len(allDatasets)):\n",
    "    trialSets = [dataset_trials[datasetIdx][0], dataset_trials[datasetIdx][1]]\n",
    "    all_results = []\n",
    "\n",
    "    for trialSetIdx in range(len(trialSets)):\n",
    "        trials_subset = []\n",
    "        for t in range(len(allCues[datasetIdx])):\n",
    "            trials_subset.append(trialSets[trialSetIdx][allCues[datasetIdx][t]])\n",
    "\n",
    "        chanSets = [np.arange(0,64).astype(np.int32), np.arange(64,128).astype(np.int32),\n",
    "                   np.arange(128,192).astype(np.int32), np.arange(192,256).astype(np.int32)]\n",
    "\n",
    "        all_vals = []\n",
    "        for chanSetIdx in range(len(chanSets)):\n",
    "            mns = []\n",
    "            cis = []\n",
    "            for n in range(125):   \n",
    "                y_pred, unroll_y = gnb_loo(trials_subset, [n, n+5], chanSets[chanSetIdx])\n",
    "                mns.append(np.mean(y_pred==unroll_y))\n",
    "                cis.append(bootCI(y_pred, unroll_y))\n",
    "                print(np.mean(y_pred==unroll_y))\n",
    "\n",
    "            all_vals.append(np.concatenate([np.array(mns)[:,np.newaxis], np.array(cis)], axis=1))\n",
    "\n",
    "        all_results.append(all_vals)\n",
    "    \n",
    "    dataset_results.append(all_results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cc646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get average audio volume envelope\n",
    "fAvg_phonemes, fCI_phonemes, trials_phonemes = triggeredAvg(phonemesDat['audioEnvelope'].astype(np.float32), \n",
    "                         phonemesDat['goTrialEpochs'][:,0],\n",
    "                         np.squeeze(phonemesDat['trialCues']), [-25,100], smoothSD=0)\n",
    "audioPhonemes = np.mean(fAvg_phonemes[:,:,0],axis=0)\n",
    "\n",
    "fAvg_fiftyWord, fCI_fiftyWord, trials_fiftyWord = triggeredAvg(fiftyWordDat['audioEnvelope'].astype(np.float32), \n",
    "                         fiftyWordDat['goTrialEpochs'][:,0],\n",
    "                         np.squeeze(fiftyWordDat['trialCues']), [-25,100], smoothSD=0)\n",
    "audioFiftyWord = np.mean(fAvg_fiftyWord[:,:,0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot sliding window classification for each array and task\n",
    "import matplotlib.pyplot as plt\n",
    "from analysis import plotPreamble\n",
    "plotPreamble()\n",
    "\n",
    "chance = [1/33, 1/39, 1/50]\n",
    "colors = ['firebrick','tomato','royalblue','blueviolet']\n",
    "taskTitles = ['Orofacial Movements','Single Phonemes','Words']\n",
    "plt.figure(dpi=300,figsize=(6,1))\n",
    "for taskIdx in range(len(chance)):\n",
    "    plt.subplot(1,3,taskIdx+1)\n",
    "    \n",
    "    goDat = dat['results'][taskIdx,0,:,:,:]*100\n",
    "    delayDat = dat['results'][taskIdx,1,:,0:100,:]*100\n",
    "            \n",
    "    lHandles = []\n",
    "    for arrIdx in range(4):\n",
    "        lHandle = plt.plot(np.arange(0,100,5)*0.02, delayDat[arrIdx,0:-1:5,0],color=colors[arrIdx])\n",
    "        lHandles.append(lHandle[0])\n",
    "        \n",
    "        plt.fill_between(np.arange(0,100,5)*0.02, \n",
    "             delayDat[arrIdx,0:-1:5,1], \n",
    "             delayDat[arrIdx,0:-1:5,2],alpha=0.3,color=colors[arrIdx])\n",
    "            \n",
    "        plt.plot(150*0.02 + np.arange(-25,100,5)*0.02, goDat[arrIdx,0:-1:5,0],color=colors[arrIdx])\n",
    "        plt.fill_between(150*0.02 + np.arange(-25,100,5)*0.02, \n",
    "             goDat[arrIdx,0:-1:5,1], \n",
    "             goDat[arrIdx,0:-1:5,2],alpha=0.3,color=colors[arrIdx])\n",
    "        \n",
    "    plt.xlabel('Time (s)')\n",
    "    if taskIdx==0:\n",
    "        plt.ylabel('Classification\\nAccuracy (%)')\n",
    "    plt.title(taskTitles[taskIdx])\n",
    "    plt.ylim([0,65])\n",
    "    if taskIdx==2:\n",
    "        plt.gca().legend(lHandles, ['6v dorsal','6v ventral','44 dorsal','44 ventral'],frameon=False)\n",
    "        \n",
    "    if taskIdx==1:\n",
    "        speechAudioLine = audioPhonemes.copy()\n",
    "        speechAudioLine = speechAudioLine - min(speechAudioLine)\n",
    "        speechAudioLine = speechAudioLine / max(speechAudioLine)\n",
    "        plt.plot(150*0.02 + np.arange(-25,100)*0.02, speechAudioLine*65,color='darkgray',alpha=0.5,linewidth=2)\n",
    "    \n",
    "    if taskIdx==2:\n",
    "        speechAudioLine = audioFiftyWord.copy()\n",
    "        speechAudioLine = speechAudioLine - min(speechAudioLine)\n",
    "        speechAudioLine = speechAudioLine / max(speechAudioLine)\n",
    "        plt.plot(150*0.02 + np.arange(-25,100)*0.02, speechAudioLine*65,color='darkgray',alpha=0.5,linewidth=2)\n",
    "            \n",
    "    plt.xticks(ticks=[0,1,2,3,4,5], labels=['0','1','2','0','1','2'])\n",
    "    plt.plot([0,0],plt.gca().get_ylim(),'--k',linewidth=0.75)\n",
    "    plt.plot([3,3],plt.gca().get_ylim(),'--k',linewidth=0.75)\n",
    "    plt.plot(plt.gca().get_xlim(),[chance[taskIdx]*100, chance[taskIdx]*100],'--k',linewidth=0.75)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f13a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

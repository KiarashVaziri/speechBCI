{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b39317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to run on your machine, you'll need to change the directories specified in \n",
    "#NeuralDecoder/neuralDecoder/configs/dataset/speech_release_baseline.yaml\n",
    "#and change \"outputDir\" specified below\n",
    "\n",
    "import os\n",
    "baseDir = '/oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final'\n",
    "os.makedirs(baseDir+'/derived/rnns', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808335b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 02:41:57.698355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-10 02:41:57.822816: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-10 02:42:09.977171: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/software/user/open/gcc/10.1.0/lib64:/share/software/user/open/gcc/10.1.0/lib/gcc/x86_64-pc-linux-gnu:/share/software/user/open/gcc/10.1.0/lib:/share/software/user/open/cudnn/8.6.0.163/lib:/usr/lib64/nvidia:/share/software/user/open/cuda/11.5.0/targets/x86_64-linux/lib64:/share/software/user/open/cuda/11.5.0/lib64:/share/software/user/open/cuda/11.5.0/nvvm/lib64:/share/software/user/open/cuda/11.5.0/extras/Debugger/lib64:/share/software/user/open/cuda/11.5.0/extras/CUPTI/lib64:/share/software/user/open/python/3.9.0/lib:/share/software/user/open/libffi/3.2.1/lib64:/share/software/user/open/sqlite/3.37.2/lib:/share/software/user/open/readline/7.0/lib:/share/software/user/open/tcltk/8.6.6/lib:/share/software/user/open/libressl/3.2.1/lib:/share/software/user/open/zlib/1.2.11/lib\n",
      "2023-06-10 02:42:09.978229: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/software/user/open/gcc/10.1.0/lib64:/share/software/user/open/gcc/10.1.0/lib/gcc/x86_64-pc-linux-gnu:/share/software/user/open/gcc/10.1.0/lib:/share/software/user/open/cudnn/8.6.0.163/lib:/usr/lib64/nvidia:/share/software/user/open/cuda/11.5.0/targets/x86_64-linux/lib64:/share/software/user/open/cuda/11.5.0/lib64:/share/software/user/open/cuda/11.5.0/nvvm/lib64:/share/software/user/open/cuda/11.5.0/extras/Debugger/lib64:/share/software/user/open/cuda/11.5.0/extras/CUPTI/lib64:/share/software/user/open/python/3.9.0/lib:/share/software/user/open/libffi/3.2.1/lib64:/share/software/user/open/sqlite/3.37.2/lib:/share/software/user/open/readline/7.0/lib:/share/software/user/open/tcltk/8.6.6/lib:/share/software/user/open/libressl/3.2.1/lib:/share/software/user/open/zlib/1.2.11/lib\n",
      "2023-06-10 02:42:09.978240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-06-10 02:42:32.011785: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-10 02:42:47.351310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78961 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "/home/groups/shenoy/fwillett/code/speechRelease/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "/home/groups/shenoy/fwillett/code/speechRelease/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting CUDA_VISIBLE_DEVICES to 0\n",
      "Output dir /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2\n",
      "Model: \"gru\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_1 (GRU)                 multiple                  28317696  \n",
      "                                                                 \n",
      " gru_2 (GRU)                 multiple                  6297600   \n",
      "                                                                 \n",
      " gru_3 (GRU)                 multiple                  6297600   \n",
      "                                                                 \n",
      " gru_4 (GRU)                 multiple                  6297600   \n",
      "                                                                 \n",
      " gru_5 (GRU)                 multiple                  6297600   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  42025     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,551,145\n",
      "Trainable params: 53,551,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.04.28/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.04.28/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.05.05/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.05.05/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.05.17/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.05.17/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.05.19/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.05.19/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.05.24/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.05.24/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.05.26/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.05.26/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.02/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.02/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.07/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.07/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.14/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.14/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.16/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.16/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.21/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.21/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.28/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.06.28/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.07.05/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.07.05/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.07.14/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.07.14/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.07.21/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.07.21/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.07.27/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.07.27/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.08.02/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.08.02/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.08.11/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.08.11/test\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.08.13/train\n",
      "Load data from /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/tfRecords/t12.2022.08.13/test\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 256)         0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, None, 256)         0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_17 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_19 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 02:43:13.056840: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-06-10 02:43:13.222014: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestValCer: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n",
      "Train batch 0: loss: 218.60 gradNorm: 67.93 time 21.24\n",
      "Val batch 0: CER: 0.89 time 3.47\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-0\n",
      "Train batch 1: loss: 227.35 gradNorm: 83.64 time 1.01\n",
      "Train batch 2: loss: 154.19 gradNorm: 57.67 time 0.73\n",
      "Train batch 3: loss: 148.64 gradNorm: 65.80 time 0.07\n",
      "Train batch 4: loss: 160.66 gradNorm: 93.89 time 0.92\n",
      "Train batch 5: loss: 151.54 gradNorm: 114.95 time 0.75\n",
      "Train batch 6: loss: 138.02 gradNorm: 130.73 time 0.90\n",
      "Train batch 7: loss: 128.22 gradNorm: 148.98 time 0.09\n",
      "Train batch 8: loss: 108.50 gradNorm: 133.96 time 0.17\n",
      "Train batch 9: loss: 92.54 gradNorm: 39.18 time 0.07\n",
      "Train batch 10: loss: 94.77 gradNorm: 93.11 time 0.07\n",
      "Train batch 11: loss: 129.90 gradNorm: 248.91 time 0.75\n",
      "Train batch 12: loss: 117.56 gradNorm: 215.14 time 0.93\n",
      "Train batch 13: loss: 134.19 gradNorm: 265.13 time 1.08\n",
      "Train batch 14: loss: 111.93 gradNorm: 215.21 time 0.94\n",
      "Train batch 15: loss: 114.58 gradNorm: 206.59 time 0.09\n",
      "Train batch 16: loss: 95.27 gradNorm: 124.56 time 0.08\n",
      "Train batch 17: loss: 89.84 gradNorm: 65.12 time 0.83\n",
      "Train batch 18: loss: 105.20 gradNorm: 88.86 time 0.82\n",
      "Train batch 19: loss: 101.17 gradNorm: 132.45 time 0.10\n",
      "Train batch 20: loss: 97.20 gradNorm: 91.23 time 0.08\n",
      "Train batch 21: loss: 90.09 gradNorm: 92.82 time 0.90\n",
      "Train batch 22: loss: 157.24 gradNorm: 172.92 time 1.33\n",
      "Train batch 23: loss: 87.24 gradNorm: 53.53 time 0.89\n",
      "Train batch 24: loss: 97.56 gradNorm: 20.06 time 0.09\n",
      "Train batch 25: loss: 153.63 gradNorm: 85.14 time 0.12\n",
      "Train batch 26: loss: 111.90 gradNorm: 76.90 time 0.08\n",
      "Train batch 27: loss: 94.53 gradNorm: 59.72 time 0.86\n",
      "Train batch 28: loss: 102.51 gradNorm: 19.44 time 0.08\n",
      "Train batch 29: loss: 143.08 gradNorm: 75.06 time 1.18\n",
      "Train batch 30: loss: 82.57 gradNorm: 15.25 time 0.07\n",
      "Train batch 31: loss: 96.06 gradNorm: 17.19 time 0.09\n",
      "Train batch 32: loss: 87.29 gradNorm: 19.60 time 0.07\n",
      "Train batch 33: loss: 82.88 gradNorm: 14.84 time 0.46\n",
      "Train batch 34: loss: 98.04 gradNorm: 21.02 time 0.09\n",
      "Train batch 35: loss: 94.48 gradNorm: 10.17 time 0.09\n",
      "Train batch 36: loss: 88.07 gradNorm: 20.25 time 0.07\n",
      "Train batch 37: loss: 91.21 gradNorm: 14.90 time 0.09\n",
      "Train batch 38: loss: 151.53 gradNorm: 16.80 time 0.13\n",
      "Train batch 39: loss: 98.82 gradNorm: 13.15 time 0.07\n",
      "Train batch 40: loss: 73.08 gradNorm: 12.02 time 0.07\n",
      "Train batch 41: loss: 88.97 gradNorm: 13.05 time 0.11\n",
      "Train batch 42: loss: 77.98 gradNorm: 35.45 time 0.08\n",
      "Train batch 43: loss: 145.34 gradNorm: 51.28 time 0.13\n",
      "Train batch 44: loss: 86.94 gradNorm: 46.40 time 0.82\n",
      "Train batch 45: loss: 80.40 gradNorm: 38.74 time 0.08\n",
      "Train batch 46: loss: 84.83 gradNorm: 27.58 time 0.08\n",
      "Train batch 47: loss: 86.88 gradNorm: 25.51 time 0.08\n",
      "Train batch 48: loss: 100.07 gradNorm: 46.48 time 0.08\n",
      "Train batch 49: loss: 80.91 gradNorm: 30.44 time 0.07\n",
      "Train batch 50: loss: 143.71 gradNorm: 28.65 time 0.12\n",
      "Val batch 50: CER: 1.00 time 2.43\n",
      "Train batch 51: loss: 86.58 gradNorm: 42.00 time 0.08\n",
      "Train batch 52: loss: 97.26 gradNorm: 49.40 time 0.09\n",
      "Train batch 53: loss: 86.76 gradNorm: 39.50 time 0.07\n",
      "Train batch 54: loss: 81.42 gradNorm: 15.33 time 0.07\n",
      "Train batch 55: loss: 78.84 gradNorm: 40.31 time 0.08\n",
      "Train batch 56: loss: 77.99 gradNorm: 47.93 time 0.08\n",
      "Train batch 57: loss: 134.78 gradNorm: 57.59 time 0.13\n",
      "Train batch 58: loss: 85.93 gradNorm: 21.64 time 0.09\n",
      "Train batch 59: loss: 91.84 gradNorm: 43.77 time 0.09\n",
      "Train batch 60: loss: 137.20 gradNorm: 40.41 time 0.73\n",
      "Train batch 61: loss: 96.02 gradNorm: 20.84 time 0.08\n",
      "Train batch 62: loss: 76.05 gradNorm: 17.25 time 0.07\n",
      "Train batch 63: loss: 91.85 gradNorm: 28.30 time 0.09\n",
      "Train batch 64: loss: 82.49 gradNorm: 22.82 time 0.07\n",
      "Train batch 65: loss: 95.91 gradNorm: 22.38 time 0.09\n",
      "Train batch 66: loss: 77.18 gradNorm: 19.70 time 0.07\n",
      "Train batch 67: loss: 92.74 gradNorm: 26.26 time 0.08\n",
      "Train batch 68: loss: 76.63 gradNorm: 16.22 time 0.08\n",
      "Train batch 69: loss: 95.88 gradNorm: 17.08 time 0.11\n",
      "Train batch 70: loss: 78.34 gradNorm: 19.92 time 0.07\n",
      "Train batch 71: loss: 75.28 gradNorm: 14.93 time 0.07\n",
      "Train batch 72: loss: 79.95 gradNorm: 21.51 time 0.07\n",
      "Train batch 73: loss: 76.91 gradNorm: 44.05 time 0.08\n",
      "Train batch 74: loss: 77.04 gradNorm: 36.15 time 0.08\n",
      "Train batch 75: loss: 133.01 gradNorm: 42.85 time 0.12\n",
      "Train batch 76: loss: 92.22 gradNorm: 30.42 time 0.08\n",
      "Train batch 77: loss: 73.17 gradNorm: 23.89 time 0.08\n",
      "Train batch 78: loss: 76.11 gradNorm: 22.88 time 0.09\n",
      "Train batch 79: loss: 86.93 gradNorm: 27.66 time 0.09\n",
      "Train batch 80: loss: 72.82 gradNorm: 19.87 time 0.07\n",
      "Train batch 81: loss: 76.49 gradNorm: 26.06 time 0.08\n",
      "Train batch 82: loss: 79.86 gradNorm: 21.60 time 0.07\n",
      "Train batch 83: loss: 92.38 gradNorm: 27.38 time 0.09\n",
      "Train batch 84: loss: 77.59 gradNorm: 37.73 time 0.84\n",
      "Train batch 85: loss: 85.24 gradNorm: 22.94 time 0.08\n",
      "Train batch 86: loss: 80.95 gradNorm: 21.14 time 0.08\n",
      "Train batch 87: loss: 86.25 gradNorm: 34.96 time 0.08\n",
      "Train batch 88: loss: 91.81 gradNorm: 40.17 time 0.08\n",
      "Train batch 89: loss: 147.45 gradNorm: 39.88 time 0.13\n",
      "Train batch 90: loss: 81.37 gradNorm: 25.66 time 0.08\n",
      "Train batch 91: loss: 74.64 gradNorm: 23.11 time 0.07\n",
      "Train batch 92: loss: 79.63 gradNorm: 21.03 time 0.08\n",
      "Train batch 93: loss: 76.02 gradNorm: 25.82 time 0.08\n",
      "Train batch 94: loss: 81.33 gradNorm: 25.89 time 0.10\n",
      "Train batch 95: loss: 80.03 gradNorm: 21.72 time 0.08\n",
      "Train batch 96: loss: 74.96 gradNorm: 23.73 time 0.08\n",
      "Train batch 97: loss: 145.16 gradNorm: 53.05 time 0.14\n",
      "Train batch 98: loss: 146.44 gradNorm: 40.97 time 0.14\n",
      "Train batch 99: loss: 134.92 gradNorm: 38.93 time 0.13\n",
      "Train batch 100: loss: 73.20 gradNorm: 23.33 time 0.08\n",
      "Val batch 100: CER: 0.94 time 1.31\n",
      "Train batch 101: loss: 86.30 gradNorm: 33.95 time 0.08\n",
      "Train batch 102: loss: 139.96 gradNorm: 45.34 time 0.13\n",
      "Train batch 103: loss: 87.86 gradNorm: 27.86 time 0.08\n",
      "Train batch 104: loss: 77.67 gradNorm: 21.25 time 0.08\n",
      "Train batch 105: loss: 79.72 gradNorm: 23.67 time 0.08\n",
      "Train batch 106: loss: 79.33 gradNorm: 24.24 time 0.08\n",
      "Train batch 107: loss: 73.17 gradNorm: 34.92 time 0.08\n",
      "Train batch 108: loss: 72.93 gradNorm: 27.02 time 0.07\n",
      "Train batch 109: loss: 87.98 gradNorm: 29.76 time 0.09\n",
      "Train batch 110: loss: 79.10 gradNorm: 20.53 time 0.09\n",
      "Train batch 111: loss: 71.76 gradNorm: 32.71 time 0.08\n",
      "Train batch 112: loss: 73.21 gradNorm: 22.11 time 0.08\n",
      "Train batch 113: loss: 70.15 gradNorm: 25.71 time 0.08\n",
      "Train batch 114: loss: 77.91 gradNorm: 21.04 time 0.08\n",
      "Train batch 115: loss: 87.56 gradNorm: 31.90 time 0.08\n",
      "Train batch 116: loss: 73.63 gradNorm: 30.08 time 0.09\n",
      "Train batch 117: loss: 67.60 gradNorm: 24.05 time 0.07\n",
      "Train batch 118: loss: 72.58 gradNorm: 22.98 time 0.08\n",
      "Train batch 119: loss: 130.60 gradNorm: 60.50 time 0.12\n",
      "Train batch 120: loss: 79.65 gradNorm: 44.66 time 0.08\n",
      "Train batch 121: loss: 87.00 gradNorm: 29.87 time 0.09\n",
      "Train batch 122: loss: 68.10 gradNorm: 26.27 time 0.07\n",
      "Train batch 123: loss: 78.12 gradNorm: 28.25 time 0.11\n",
      "Train batch 124: loss: 69.95 gradNorm: 22.09 time 0.08\n",
      "Train batch 125: loss: 134.96 gradNorm: 52.15 time 0.14\n",
      "Train batch 126: loss: 68.41 gradNorm: 36.31 time 0.08\n",
      "Train batch 127: loss: 81.99 gradNorm: 26.79 time 0.09\n",
      "Train batch 128: loss: 67.84 gradNorm: 24.17 time 0.08\n",
      "Train batch 129: loss: 92.60 gradNorm: 31.18 time 0.11\n",
      "Train batch 130: loss: 73.17 gradNorm: 23.89 time 0.09\n",
      "Train batch 131: loss: 63.24 gradNorm: 21.30 time 0.08\n",
      "Train batch 132: loss: 88.37 gradNorm: 32.28 time 0.08\n",
      "Train batch 133: loss: 66.97 gradNorm: 24.53 time 0.07\n",
      "Train batch 134: loss: 68.80 gradNorm: 26.08 time 0.09\n",
      "Train batch 135: loss: 76.94 gradNorm: 31.32 time 0.08\n",
      "Train batch 136: loss: 69.22 gradNorm: 24.41 time 0.09\n",
      "Train batch 137: loss: 125.48 gradNorm: 53.89 time 0.14\n",
      "Train batch 138: loss: 62.86 gradNorm: 23.06 time 0.08\n",
      "Train batch 139: loss: 77.83 gradNorm: 29.30 time 0.08\n",
      "Train batch 140: loss: 76.46 gradNorm: 31.69 time 0.08\n",
      "Train batch 141: loss: 75.26 gradNorm: 27.55 time 0.09\n",
      "Train batch 142: loss: 71.07 gradNorm: 27.02 time 0.08\n",
      "Train batch 143: loss: 73.54 gradNorm: 39.56 time 0.07\n",
      "Train batch 144: loss: 70.95 gradNorm: 28.48 time 0.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 145: loss: 64.51 gradNorm: 32.03 time 0.08\n",
      "Train batch 146: loss: 67.98 gradNorm: 26.14 time 0.07\n",
      "Train batch 147: loss: 70.67 gradNorm: 28.42 time 0.08\n",
      "Train batch 148: loss: 78.55 gradNorm: 28.70 time 0.08\n",
      "Train batch 149: loss: 122.52 gradNorm: 51.28 time 0.13\n",
      "Train batch 150: loss: 67.58 gradNorm: 29.98 time 0.08\n",
      "Val batch 150: CER: 0.70 time 1.28\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-150\n",
      "Train batch 151: loss: 66.41 gradNorm: 27.38 time 0.08\n",
      "Train batch 152: loss: 71.43 gradNorm: 26.75 time 0.08\n",
      "Train batch 153: loss: 64.56 gradNorm: 25.39 time 0.08\n",
      "Train batch 154: loss: 75.84 gradNorm: 34.01 time 0.10\n",
      "Train batch 155: loss: 73.53 gradNorm: 27.17 time 0.08\n",
      "Train batch 156: loss: 71.26 gradNorm: 25.98 time 0.08\n",
      "Train batch 157: loss: 73.23 gradNorm: 25.62 time 0.09\n",
      "Train batch 158: loss: 82.98 gradNorm: 37.07 time 0.08\n",
      "Train batch 159: loss: 64.77 gradNorm: 27.91 time 0.07\n",
      "Train batch 160: loss: 65.71 gradNorm: 24.61 time 0.46\n",
      "Train batch 161: loss: 62.88 gradNorm: 26.29 time 0.07\n",
      "Train batch 162: loss: 62.67 gradNorm: 27.09 time 0.07\n",
      "Train batch 163: loss: 70.86 gradNorm: 34.46 time 0.08\n",
      "Train batch 164: loss: 63.57 gradNorm: 26.81 time 0.07\n",
      "Train batch 165: loss: 81.10 gradNorm: 40.22 time 0.09\n",
      "Train batch 166: loss: 67.04 gradNorm: 29.94 time 0.08\n",
      "Train batch 167: loss: 61.45 gradNorm: 26.10 time 0.08\n",
      "Train batch 168: loss: 80.39 gradNorm: 35.23 time 0.08\n",
      "Train batch 169: loss: 76.62 gradNorm: 34.30 time 0.08\n",
      "Train batch 170: loss: 79.74 gradNorm: 31.67 time 0.08\n",
      "Train batch 171: loss: 71.32 gradNorm: 55.09 time 0.07\n",
      "Train batch 172: loss: 125.33 gradNorm: 51.76 time 0.13\n",
      "Train batch 173: loss: 82.14 gradNorm: 31.59 time 0.13\n",
      "Train batch 174: loss: 60.10 gradNorm: 25.81 time 0.08\n",
      "Train batch 175: loss: 64.11 gradNorm: 30.73 time 0.08\n",
      "Train batch 176: loss: 74.14 gradNorm: 35.00 time 0.11\n",
      "Train batch 177: loss: 56.44 gradNorm: 26.41 time 0.07\n",
      "Train batch 178: loss: 115.62 gradNorm: 49.89 time 0.13\n",
      "Train batch 179: loss: 66.76 gradNorm: 31.57 time 0.08\n",
      "Train batch 180: loss: 60.54 gradNorm: 25.29 time 0.08\n",
      "Train batch 181: loss: 63.90 gradNorm: 28.05 time 0.08\n",
      "Train batch 182: loss: 58.13 gradNorm: 25.15 time 0.07\n",
      "Train batch 183: loss: 69.45 gradNorm: 36.57 time 0.07\n",
      "Train batch 184: loss: 76.79 gradNorm: 34.47 time 0.08\n",
      "Train batch 185: loss: 61.08 gradNorm: 29.67 time 0.07\n",
      "Train batch 186: loss: 66.02 gradNorm: 28.19 time 0.09\n",
      "Train batch 187: loss: 76.67 gradNorm: 28.94 time 0.08\n",
      "Train batch 188: loss: 61.32 gradNorm: 25.20 time 0.07\n",
      "Train batch 189: loss: 59.50 gradNorm: 25.90 time 0.08\n",
      "Train batch 190: loss: 65.04 gradNorm: 30.37 time 0.08\n",
      "Train batch 191: loss: 73.31 gradNorm: 33.30 time 0.08\n",
      "Train batch 192: loss: 57.38 gradNorm: 25.07 time 0.34\n",
      "Train batch 193: loss: 70.13 gradNorm: 34.28 time 0.07\n",
      "Train batch 194: loss: 59.84 gradNorm: 24.84 time 0.08\n",
      "Train batch 195: loss: 60.32 gradNorm: 28.98 time 0.09\n",
      "Train batch 196: loss: 118.51 gradNorm: 51.17 time 0.12\n",
      "Train batch 197: loss: 59.76 gradNorm: 26.37 time 0.08\n",
      "Train batch 198: loss: 134.92 gradNorm: 64.16 time 0.14\n",
      "Train batch 199: loss: 57.14 gradNorm: 28.01 time 0.07\n",
      "Train batch 200: loss: 61.36 gradNorm: 28.75 time 0.08\n",
      "Val batch 200: CER: 0.60 time 2.49\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-200\n",
      "Train batch 201: loss: 75.82 gradNorm: 33.15 time 0.09\n",
      "Train batch 202: loss: 60.30 gradNorm: 29.20 time 0.08\n",
      "Train batch 203: loss: 67.12 gradNorm: 35.31 time 0.10\n",
      "Train batch 204: loss: 61.45 gradNorm: 30.57 time 0.09\n",
      "Train batch 205: loss: 59.19 gradNorm: 25.31 time 0.09\n",
      "Train batch 206: loss: 60.29 gradNorm: 27.48 time 0.07\n",
      "Train batch 207: loss: 62.83 gradNorm: 26.22 time 0.09\n",
      "Train batch 208: loss: 58.48 gradNorm: 27.30 time 0.08\n",
      "Train batch 209: loss: 62.05 gradNorm: 28.93 time 0.10\n",
      "Train batch 210: loss: 61.52 gradNorm: 27.87 time 0.10\n",
      "Train batch 211: loss: 117.27 gradNorm: 49.51 time 0.14\n",
      "Train batch 212: loss: 60.68 gradNorm: 30.33 time 0.09\n",
      "Train batch 213: loss: 124.44 gradNorm: 47.54 time 0.14\n",
      "Train batch 214: loss: 58.37 gradNorm: 23.89 time 0.08\n",
      "Train batch 215: loss: 55.31 gradNorm: 24.47 time 0.07\n",
      "Train batch 216: loss: 57.64 gradNorm: 27.99 time 0.18\n",
      "Train batch 217: loss: 57.46 gradNorm: 35.88 time 0.08\n",
      "Train batch 218: loss: 60.34 gradNorm: 26.98 time 0.08\n",
      "Train batch 219: loss: 66.75 gradNorm: 40.80 time 0.07\n",
      "Train batch 220: loss: 65.85 gradNorm: 31.93 time 0.10\n",
      "Train batch 221: loss: 59.57 gradNorm: 29.47 time 0.08\n",
      "Train batch 222: loss: 62.17 gradNorm: 28.29 time 0.08\n",
      "Train batch 223: loss: 56.83 gradNorm: 30.79 time 0.09\n",
      "Train batch 224: loss: 73.11 gradNorm: 32.48 time 0.08\n",
      "Train batch 225: loss: 55.63 gradNorm: 26.64 time 0.07\n",
      "Train batch 226: loss: 123.25 gradNorm: 55.95 time 0.13\n",
      "Train batch 227: loss: 56.92 gradNorm: 30.21 time 0.07\n",
      "Train batch 228: loss: 66.11 gradNorm: 36.34 time 0.11\n",
      "Train batch 229: loss: 55.66 gradNorm: 28.53 time 0.07\n",
      "Train batch 230: loss: 60.63 gradNorm: 31.10 time 0.08\n",
      "Train batch 231: loss: 59.33 gradNorm: 30.13 time 0.09\n",
      "Train batch 232: loss: 66.56 gradNorm: 33.31 time 0.09\n",
      "Train batch 233: loss: 59.15 gradNorm: 30.94 time 0.08\n",
      "Train batch 234: loss: 115.87 gradNorm: 51.36 time 0.13\n",
      "Train batch 235: loss: 57.89 gradNorm: 29.15 time 0.07\n",
      "Train batch 236: loss: 56.05 gradNorm: 28.01 time 0.09\n",
      "Train batch 237: loss: 56.35 gradNorm: 30.62 time 0.07\n",
      "Train batch 238: loss: 61.21 gradNorm: 32.00 time 0.08\n",
      "Train batch 239: loss: 56.86 gradNorm: 30.66 time 0.09\n",
      "Train batch 240: loss: 59.26 gradNorm: 31.99 time 0.09\n",
      "Train batch 241: loss: 54.25 gradNorm: 26.79 time 0.07\n",
      "Train batch 242: loss: 59.25 gradNorm: 34.19 time 0.09\n",
      "Train batch 243: loss: 54.98 gradNorm: 27.13 time 0.07\n",
      "Train batch 244: loss: 53.57 gradNorm: 29.09 time 0.09\n",
      "Train batch 245: loss: 59.08 gradNorm: 30.83 time 0.10\n",
      "Train batch 246: loss: 72.24 gradNorm: 35.16 time 0.09\n",
      "Train batch 247: loss: 62.39 gradNorm: 34.77 time 0.07\n",
      "Train batch 248: loss: 59.09 gradNorm: 33.02 time 0.07\n",
      "Train batch 249: loss: 56.58 gradNorm: 29.23 time 0.08\n",
      "Train batch 250: loss: 53.15 gradNorm: 31.76 time 0.07\n",
      "Val batch 250: CER: 0.55 time 2.77\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-250\n",
      "Train batch 251: loss: 126.18 gradNorm: 70.98 time 0.15\n",
      "Train batch 252: loss: 64.61 gradNorm: 37.70 time 0.10\n",
      "Train batch 253: loss: 54.51 gradNorm: 29.11 time 0.08\n",
      "Train batch 254: loss: 63.38 gradNorm: 43.04 time 0.08\n",
      "Train batch 255: loss: 67.09 gradNorm: 35.43 time 0.11\n",
      "Train batch 256: loss: 59.12 gradNorm: 30.05 time 0.10\n",
      "Train batch 257: loss: 53.86 gradNorm: 27.87 time 0.08\n",
      "Train batch 258: loss: 67.67 gradNorm: 35.72 time 0.08\n",
      "Train batch 259: loss: 59.79 gradNorm: 34.58 time 0.08\n",
      "Train batch 260: loss: 59.51 gradNorm: 29.91 time 0.09\n",
      "Train batch 261: loss: 52.15 gradNorm: 28.07 time 0.08\n",
      "Train batch 262: loss: 52.69 gradNorm: 27.47 time 0.08\n",
      "Train batch 263: loss: 53.55 gradNorm: 26.28 time 0.08\n",
      "Train batch 264: loss: 60.32 gradNorm: 31.56 time 0.08\n",
      "Train batch 265: loss: 57.18 gradNorm: 37.93 time 0.08\n",
      "Train batch 266: loss: 55.42 gradNorm: 27.23 time 0.08\n",
      "Train batch 267: loss: 50.37 gradNorm: 29.77 time 0.07\n",
      "Train batch 268: loss: 53.23 gradNorm: 28.01 time 0.07\n",
      "Train batch 269: loss: 64.37 gradNorm: 34.12 time 0.09\n",
      "Train batch 270: loss: 53.02 gradNorm: 30.67 time 0.08\n",
      "Train batch 271: loss: 54.47 gradNorm: 32.51 time 0.09\n",
      "Train batch 272: loss: 55.08 gradNorm: 29.24 time 0.10\n",
      "Train batch 273: loss: 54.18 gradNorm: 30.39 time 0.08\n",
      "Train batch 274: loss: 58.52 gradNorm: 32.41 time 0.09\n",
      "Train batch 275: loss: 50.91 gradNorm: 31.05 time 0.27\n",
      "Train batch 276: loss: 60.83 gradNorm: 32.03 time 0.08\n",
      "Train batch 277: loss: 55.15 gradNorm: 26.74 time 0.07\n",
      "Train batch 278: loss: 53.31 gradNorm: 28.38 time 0.08\n",
      "Train batch 279: loss: 53.22 gradNorm: 29.52 time 0.07\n",
      "Train batch 280: loss: 54.55 gradNorm: 30.13 time 0.08\n",
      "Train batch 281: loss: 53.90 gradNorm: 33.24 time 0.08\n",
      "Train batch 282: loss: 54.57 gradNorm: 33.05 time 0.08\n",
      "Train batch 283: loss: 53.74 gradNorm: 37.27 time 0.09\n",
      "Train batch 284: loss: 51.06 gradNorm: 27.18 time 0.08\n",
      "Train batch 285: loss: 56.98 gradNorm: 32.12 time 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 286: loss: 52.59 gradNorm: 29.24 time 0.07\n",
      "Train batch 287: loss: 49.26 gradNorm: 29.14 time 0.08\n",
      "Train batch 288: loss: 50.58 gradNorm: 30.48 time 0.43\n",
      "Train batch 289: loss: 52.41 gradNorm: 37.44 time 0.08\n",
      "Train batch 290: loss: 62.88 gradNorm: 52.56 time 0.08\n",
      "Train batch 291: loss: 74.58 gradNorm: 41.91 time 0.09\n",
      "Train batch 292: loss: 48.00 gradNorm: 30.42 time 0.08\n",
      "Train batch 293: loss: 54.09 gradNorm: 32.82 time 0.10\n",
      "Train batch 294: loss: 121.24 gradNorm: 64.51 time 0.13\n",
      "Train batch 295: loss: 52.59 gradNorm: 34.73 time 0.07\n",
      "Train batch 296: loss: 110.75 gradNorm: 63.81 time 0.13\n",
      "Train batch 297: loss: 54.89 gradNorm: 33.67 time 0.08\n",
      "Train batch 298: loss: 55.05 gradNorm: 30.95 time 0.10\n",
      "Train batch 299: loss: 52.28 gradNorm: 29.78 time 0.09\n",
      "Train batch 300: loss: 50.97 gradNorm: 29.64 time 0.09\n",
      "Val batch 300: CER: 0.51 time 2.47\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-300\n",
      "Train batch 301: loss: 55.38 gradNorm: 29.54 time 0.10\n",
      "Train batch 302: loss: 66.73 gradNorm: 37.22 time 0.10\n",
      "Train batch 303: loss: 50.26 gradNorm: 28.47 time 0.07\n",
      "Train batch 304: loss: 56.88 gradNorm: 33.52 time 0.08\n",
      "Train batch 305: loss: 47.89 gradNorm: 27.21 time 0.07\n",
      "Train batch 306: loss: 50.06 gradNorm: 27.74 time 0.08\n",
      "Train batch 307: loss: 46.68 gradNorm: 27.90 time 0.08\n",
      "Train batch 308: loss: 123.22 gradNorm: 68.75 time 0.14\n",
      "Train batch 309: loss: 56.39 gradNorm: 32.48 time 0.09\n",
      "Train batch 310: loss: 60.81 gradNorm: 30.68 time 0.10\n",
      "Train batch 311: loss: 46.42 gradNorm: 29.95 time 0.08\n",
      "Train batch 312: loss: 48.62 gradNorm: 33.57 time 0.08\n",
      "Train batch 313: loss: 65.46 gradNorm: 39.87 time 0.08\n",
      "Train batch 314: loss: 46.66 gradNorm: 28.04 time 0.07\n",
      "Train batch 315: loss: 52.14 gradNorm: 34.00 time 0.07\n",
      "Train batch 316: loss: 49.32 gradNorm: 33.43 time 0.07\n",
      "Train batch 317: loss: 47.50 gradNorm: 27.82 time 0.07\n",
      "Train batch 318: loss: 52.87 gradNorm: 34.09 time 0.07\n",
      "Train batch 319: loss: 56.93 gradNorm: 47.09 time 0.09\n",
      "Train batch 320: loss: 57.14 gradNorm: 35.84 time 0.11\n",
      "Train batch 321: loss: 110.64 gradNorm: 69.33 time 0.13\n",
      "Train batch 322: loss: 48.69 gradNorm: 30.25 time 0.07\n",
      "Train batch 323: loss: 54.91 gradNorm: 37.75 time 0.10\n",
      "Train batch 324: loss: 53.83 gradNorm: 33.51 time 0.08\n",
      "Train batch 325: loss: 54.01 gradNorm: 33.18 time 0.09\n",
      "Train batch 326: loss: 47.91 gradNorm: 32.33 time 0.08\n",
      "Train batch 327: loss: 49.12 gradNorm: 29.34 time 0.08\n",
      "Train batch 328: loss: 70.78 gradNorm: 46.50 time 0.08\n",
      "Train batch 329: loss: 49.73 gradNorm: 34.67 time 0.07\n",
      "Train batch 330: loss: 57.13 gradNorm: 31.55 time 0.08\n",
      "Train batch 331: loss: 58.26 gradNorm: 34.16 time 0.07\n",
      "Train batch 332: loss: 47.44 gradNorm: 27.68 time 0.25\n",
      "Train batch 333: loss: 51.21 gradNorm: 31.08 time 0.08\n",
      "Train batch 334: loss: 49.05 gradNorm: 34.53 time 0.08\n",
      "Train batch 335: loss: 46.11 gradNorm: 28.72 time 0.08\n",
      "Train batch 336: loss: 48.25 gradNorm: 27.23 time 0.08\n",
      "Train batch 337: loss: 115.91 gradNorm: 64.75 time 0.14\n",
      "Train batch 338: loss: 46.83 gradNorm: 29.84 time 0.08\n",
      "Train batch 339: loss: 51.72 gradNorm: 35.63 time 0.08\n",
      "Train batch 340: loss: 45.85 gradNorm: 28.15 time 0.08\n",
      "Train batch 341: loss: 65.52 gradNorm: 36.15 time 0.08\n",
      "Train batch 342: loss: 51.38 gradNorm: 33.71 time 0.08\n",
      "Train batch 343: loss: 49.86 gradNorm: 32.15 time 0.07\n",
      "Train batch 344: loss: 66.84 gradNorm: 39.59 time 0.07\n",
      "Train batch 345: loss: 50.29 gradNorm: 32.26 time 0.08\n",
      "Train batch 346: loss: 70.95 gradNorm: 42.51 time 0.08\n",
      "Train batch 347: loss: 60.61 gradNorm: 33.63 time 0.08\n",
      "Train batch 348: loss: 66.22 gradNorm: 38.56 time 0.09\n",
      "Train batch 349: loss: 44.48 gradNorm: 28.46 time 0.12\n",
      "Train batch 350: loss: 55.77 gradNorm: 37.66 time 0.09\n",
      "Val batch 350: CER: 0.46 time 2.42\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-350\n",
      "Train batch 351: loss: 53.12 gradNorm: 30.02 time 0.11\n",
      "Train batch 352: loss: 47.62 gradNorm: 36.23 time 0.09\n",
      "Train batch 353: loss: 50.69 gradNorm: 31.80 time 0.08\n",
      "Train batch 354: loss: 46.75 gradNorm: 33.24 time 0.10\n",
      "Train batch 355: loss: 61.14 gradNorm: 35.33 time 0.09\n",
      "Train batch 356: loss: 44.15 gradNorm: 28.91 time 0.08\n",
      "Train batch 357: loss: 48.28 gradNorm: 31.00 time 0.07\n",
      "Train batch 358: loss: 46.59 gradNorm: 31.57 time 0.07\n",
      "Train batch 359: loss: 54.96 gradNorm: 33.82 time 0.09\n",
      "Train batch 360: loss: 51.11 gradNorm: 34.45 time 0.07\n",
      "Train batch 361: loss: 47.74 gradNorm: 30.46 time 0.08\n",
      "Train batch 362: loss: 50.13 gradNorm: 34.44 time 0.08\n",
      "Train batch 363: loss: 50.26 gradNorm: 31.86 time 0.08\n",
      "Train batch 364: loss: 62.74 gradNorm: 37.72 time 0.08\n",
      "Train batch 365: loss: 45.06 gradNorm: 30.47 time 0.07\n",
      "Train batch 366: loss: 48.52 gradNorm: 34.29 time 0.09\n",
      "Train batch 367: loss: 43.97 gradNorm: 31.69 time 0.07\n",
      "Train batch 368: loss: 50.05 gradNorm: 32.68 time 0.09\n",
      "Train batch 369: loss: 44.65 gradNorm: 27.45 time 0.07\n",
      "Train batch 370: loss: 44.20 gradNorm: 28.22 time 0.08\n",
      "Train batch 371: loss: 48.04 gradNorm: 30.89 time 0.08\n",
      "Train batch 372: loss: 44.42 gradNorm: 26.06 time 0.07\n",
      "Train batch 373: loss: 47.60 gradNorm: 30.20 time 0.08\n",
      "Train batch 374: loss: 65.05 gradNorm: 40.73 time 0.11\n",
      "Train batch 375: loss: 49.18 gradNorm: 30.28 time 0.08\n",
      "Train batch 376: loss: 45.88 gradNorm: 32.82 time 0.07\n",
      "Train batch 377: loss: 54.74 gradNorm: 32.60 time 0.09\n",
      "Train batch 378: loss: 65.87 gradNorm: 40.83 time 0.08\n",
      "Train batch 379: loss: 65.13 gradNorm: 41.41 time 0.08\n",
      "Train batch 380: loss: 47.32 gradNorm: 31.84 time 0.08\n",
      "Train batch 381: loss: 44.55 gradNorm: 31.35 time 0.07\n",
      "Train batch 382: loss: 45.13 gradNorm: 29.42 time 0.08\n",
      "Train batch 383: loss: 41.50 gradNorm: 28.06 time 0.08\n",
      "Train batch 384: loss: 53.18 gradNorm: 32.65 time 0.09\n",
      "Train batch 385: loss: 49.92 gradNorm: 30.85 time 0.07\n",
      "Train batch 386: loss: 53.89 gradNorm: 34.24 time 0.10\n",
      "Train batch 387: loss: 48.59 gradNorm: 33.33 time 0.09\n",
      "Train batch 388: loss: 48.08 gradNorm: 28.23 time 0.08\n",
      "Train batch 389: loss: 42.72 gradNorm: 30.80 time 0.08\n",
      "Train batch 390: loss: 55.27 gradNorm: 37.90 time 0.11\n",
      "Train batch 391: loss: 42.68 gradNorm: 30.25 time 0.08\n",
      "Train batch 392: loss: 48.84 gradNorm: 35.86 time 0.08\n",
      "Train batch 393: loss: 44.08 gradNorm: 30.99 time 0.07\n",
      "Train batch 394: loss: 42.82 gradNorm: 30.56 time 0.08\n",
      "Train batch 395: loss: 108.56 gradNorm: 60.55 time 0.13\n",
      "Train batch 396: loss: 46.24 gradNorm: 30.88 time 0.08\n",
      "Train batch 397: loss: 43.59 gradNorm: 28.57 time 0.08\n",
      "Train batch 398: loss: 59.50 gradNorm: 41.14 time 0.08\n",
      "Train batch 399: loss: 44.93 gradNorm: 31.80 time 0.08\n",
      "Train batch 400: loss: 58.97 gradNorm: 40.67 time 0.08\n",
      "Val batch 400: CER: 0.44 time 2.34\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-400\n",
      "Train batch 401: loss: 60.43 gradNorm: 40.97 time 0.13\n",
      "Train batch 402: loss: 44.19 gradNorm: 30.34 time 0.08\n",
      "Train batch 403: loss: 111.91 gradNorm: 80.80 time 0.14\n",
      "Train batch 404: loss: 59.33 gradNorm: 40.83 time 0.08\n",
      "Train batch 405: loss: 105.68 gradNorm: 68.65 time 0.84\n",
      "Train batch 406: loss: 61.88 gradNorm: 36.99 time 0.07\n",
      "Train batch 407: loss: 46.54 gradNorm: 30.00 time 0.08\n",
      "Train batch 408: loss: 46.61 gradNorm: 31.80 time 0.07\n",
      "Train batch 409: loss: 56.39 gradNorm: 37.20 time 0.16\n",
      "Train batch 410: loss: 44.67 gradNorm: 28.96 time 0.09\n",
      "Train batch 411: loss: 44.71 gradNorm: 29.33 time 0.08\n",
      "Train batch 412: loss: 48.01 gradNorm: 34.35 time 0.08\n",
      "Train batch 413: loss: 105.04 gradNorm: 64.13 time 0.14\n",
      "Train batch 414: loss: 44.86 gradNorm: 30.56 time 0.08\n",
      "Train batch 415: loss: 56.81 gradNorm: 42.46 time 0.08\n",
      "Train batch 416: loss: 43.73 gradNorm: 30.11 time 0.08\n",
      "Train batch 417: loss: 49.07 gradNorm: 33.79 time 0.08\n",
      "Train batch 418: loss: 45.08 gradNorm: 31.76 time 0.08\n",
      "Train batch 419: loss: 50.42 gradNorm: 33.01 time 0.09\n",
      "Train batch 420: loss: 44.72 gradNorm: 34.37 time 0.08\n",
      "Train batch 421: loss: 108.67 gradNorm: 97.17 time 0.15\n",
      "Train batch 422: loss: 43.91 gradNorm: 30.87 time 0.09\n",
      "Train batch 423: loss: 43.43 gradNorm: 34.03 time 0.09\n",
      "Train batch 424: loss: 46.79 gradNorm: 34.71 time 0.09\n",
      "Train batch 425: loss: 43.66 gradNorm: 31.49 time 0.08\n",
      "Train batch 426: loss: 46.16 gradNorm: 32.95 time 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 427: loss: 44.40 gradNorm: 32.05 time 0.08\n",
      "Train batch 428: loss: 51.57 gradNorm: 35.58 time 0.07\n",
      "Train batch 429: loss: 59.25 gradNorm: 43.16 time 0.08\n",
      "Train batch 430: loss: 51.85 gradNorm: 36.41 time 0.55\n",
      "Train batch 431: loss: 43.87 gradNorm: 34.24 time 0.07\n",
      "Train batch 432: loss: 47.54 gradNorm: 34.54 time 0.08\n",
      "Train batch 433: loss: 50.24 gradNorm: 32.73 time 0.08\n",
      "Train batch 434: loss: 47.10 gradNorm: 34.11 time 0.08\n",
      "Train batch 435: loss: 49.22 gradNorm: 38.86 time 0.09\n",
      "Train batch 436: loss: 41.48 gradNorm: 31.63 time 0.07\n",
      "Train batch 437: loss: 41.57 gradNorm: 31.25 time 0.07\n",
      "Train batch 438: loss: 49.25 gradNorm: 33.50 time 0.10\n",
      "Train batch 439: loss: 40.01 gradNorm: 32.94 time 0.08\n",
      "Train batch 440: loss: 44.45 gradNorm: 36.15 time 0.08\n",
      "Train batch 441: loss: 44.55 gradNorm: 32.65 time 0.08\n",
      "Train batch 442: loss: 40.99 gradNorm: 32.40 time 0.08\n",
      "Train batch 443: loss: 41.54 gradNorm: 39.87 time 0.09\n",
      "Train batch 444: loss: 59.62 gradNorm: 46.27 time 0.08\n",
      "Train batch 445: loss: 45.34 gradNorm: 33.46 time 0.08\n",
      "Train batch 446: loss: 44.06 gradNorm: 34.66 time 0.08\n",
      "Train batch 447: loss: 40.81 gradNorm: 32.12 time 0.07\n",
      "Train batch 448: loss: 106.21 gradNorm: 91.36 time 0.14\n",
      "Train batch 449: loss: 42.93 gradNorm: 32.28 time 0.09\n",
      "Train batch 450: loss: 104.22 gradNorm: 65.49 time 0.14\n",
      "Val batch 450: CER: 0.42 time 2.36\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-450\n",
      "Train batch 451: loss: 41.22 gradNorm: 32.53 time 0.08\n",
      "Train batch 452: loss: 41.13 gradNorm: 34.53 time 0.08\n",
      "Train batch 453: loss: 49.81 gradNorm: 34.77 time 0.07\n",
      "Train batch 454: loss: 42.19 gradNorm: 33.80 time 0.09\n",
      "Train batch 455: loss: 57.29 gradNorm: 37.36 time 0.09\n",
      "Train batch 456: loss: 42.75 gradNorm: 29.93 time 0.09\n",
      "Train batch 457: loss: 59.38 gradNorm: 39.81 time 0.08\n",
      "Train batch 458: loss: 42.25 gradNorm: 33.21 time 0.08\n",
      "Train batch 459: loss: 42.78 gradNorm: 31.53 time 0.08\n",
      "Train batch 460: loss: 55.15 gradNorm: 39.04 time 0.08\n",
      "Train batch 461: loss: 57.22 gradNorm: 37.24 time 0.08\n",
      "Train batch 462: loss: 48.98 gradNorm: 32.76 time 0.10\n",
      "Train batch 463: loss: 43.11 gradNorm: 30.69 time 0.08\n",
      "Train batch 464: loss: 39.85 gradNorm: 30.45 time 0.07\n",
      "Train batch 465: loss: 101.73 gradNorm: 62.82 time 0.14\n",
      "Train batch 466: loss: 43.25 gradNorm: 33.34 time 0.09\n",
      "Train batch 467: loss: 42.87 gradNorm: 31.37 time 0.08\n",
      "Train batch 468: loss: 45.55 gradNorm: 34.19 time 0.08\n",
      "Train batch 469: loss: 57.04 gradNorm: 36.56 time 0.08\n",
      "Train batch 470: loss: 40.77 gradNorm: 28.90 time 0.08\n",
      "Train batch 471: loss: 56.07 gradNorm: 34.49 time 0.09\n",
      "Train batch 472: loss: 100.18 gradNorm: 62.46 time 0.14\n",
      "Train batch 473: loss: 102.69 gradNorm: 55.10 time 0.13\n",
      "Train batch 474: loss: 45.39 gradNorm: 34.79 time 0.08\n",
      "Train batch 475: loss: 42.86 gradNorm: 30.02 time 0.09\n",
      "Train batch 476: loss: 41.82 gradNorm: 30.05 time 0.08\n",
      "Train batch 477: loss: 39.49 gradNorm: 29.05 time 0.07\n",
      "Train batch 478: loss: 38.81 gradNorm: 29.58 time 0.07\n",
      "Train batch 479: loss: 56.58 gradNorm: 42.60 time 0.08\n",
      "Train batch 480: loss: 46.20 gradNorm: 35.96 time 0.09\n",
      "Train batch 481: loss: 38.47 gradNorm: 31.77 time 0.08\n",
      "Train batch 482: loss: 47.48 gradNorm: 32.26 time 0.09\n",
      "Train batch 483: loss: 99.06 gradNorm: 67.46 time 0.13\n",
      "Train batch 484: loss: 41.03 gradNorm: 31.55 time 0.07\n",
      "Train batch 485: loss: 43.52 gradNorm: 33.80 time 0.09\n",
      "Train batch 486: loss: 97.36 gradNorm: 80.09 time 0.13\n",
      "Train batch 487: loss: 39.91 gradNorm: 29.23 time 0.08\n",
      "Train batch 488: loss: 37.05 gradNorm: 28.28 time 0.07\n",
      "Train batch 489: loss: 40.86 gradNorm: 31.78 time 0.08\n",
      "Train batch 490: loss: 53.30 gradNorm: 46.61 time 0.09\n",
      "Train batch 491: loss: 45.06 gradNorm: 37.20 time 0.08\n",
      "Train batch 492: loss: 37.39 gradNorm: 31.07 time 0.07\n",
      "Train batch 493: loss: 40.49 gradNorm: 33.89 time 0.07\n",
      "Train batch 494: loss: 46.35 gradNorm: 36.36 time 0.08\n",
      "Train batch 495: loss: 40.75 gradNorm: 32.53 time 0.07\n",
      "Train batch 496: loss: 39.98 gradNorm: 28.94 time 0.08\n",
      "Train batch 497: loss: 45.58 gradNorm: 34.09 time 0.09\n",
      "Train batch 498: loss: 49.96 gradNorm: 35.08 time 0.10\n",
      "Train batch 499: loss: 46.63 gradNorm: 32.50 time 0.09\n",
      "Train batch 500: loss: 51.60 gradNorm: 35.65 time 0.08\n",
      "Val batch 500: CER: 0.41 time 2.46\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-500\n",
      "Train batch 501: loss: 38.51 gradNorm: 30.73 time 0.08\n",
      "Train batch 502: loss: 42.82 gradNorm: 35.25 time 0.09\n",
      "Train batch 503: loss: 38.77 gradNorm: 29.70 time 0.08\n",
      "Train batch 504: loss: 40.87 gradNorm: 33.71 time 0.08\n",
      "Train batch 505: loss: 40.43 gradNorm: 34.05 time 0.08\n",
      "Train batch 506: loss: 37.01 gradNorm: 30.15 time 0.07\n",
      "Train batch 507: loss: 41.22 gradNorm: 37.11 time 0.09\n",
      "Train batch 508: loss: 45.61 gradNorm: 35.72 time 0.10\n",
      "Train batch 509: loss: 41.28 gradNorm: 32.68 time 0.07\n",
      "Train batch 510: loss: 97.45 gradNorm: 58.99 time 0.13\n",
      "Train batch 511: loss: 40.66 gradNorm: 33.06 time 0.09\n",
      "Train batch 512: loss: 37.02 gradNorm: 34.41 time 0.09\n",
      "Train batch 513: loss: 100.18 gradNorm: 61.02 time 0.13\n",
      "Train batch 514: loss: 40.95 gradNorm: 33.55 time 0.07\n",
      "Train batch 515: loss: 45.77 gradNorm: 35.22 time 0.11\n",
      "Train batch 516: loss: 44.46 gradNorm: 35.14 time 0.08\n",
      "Train batch 517: loss: 35.53 gradNorm: 30.32 time 0.07\n",
      "Train batch 518: loss: 39.78 gradNorm: 35.46 time 0.08\n",
      "Train batch 519: loss: 41.88 gradNorm: 33.07 time 0.08\n",
      "Train batch 520: loss: 40.68 gradNorm: 31.38 time 0.07\n",
      "Train batch 521: loss: 39.96 gradNorm: 37.96 time 0.08\n",
      "Train batch 522: loss: 42.29 gradNorm: 32.90 time 0.08\n",
      "Train batch 523: loss: 53.35 gradNorm: 39.20 time 0.07\n",
      "Train batch 524: loss: 42.16 gradNorm: 31.73 time 0.10\n",
      "Train batch 525: loss: 44.95 gradNorm: 34.23 time 0.08\n",
      "Train batch 526: loss: 58.63 gradNorm: 38.98 time 0.08\n",
      "Train batch 527: loss: 39.74 gradNorm: 36.73 time 0.08\n",
      "Train batch 528: loss: 41.06 gradNorm: 37.82 time 0.09\n",
      "Train batch 529: loss: 44.28 gradNorm: 34.31 time 0.08\n",
      "Train batch 530: loss: 41.26 gradNorm: 35.44 time 0.07\n",
      "Train batch 531: loss: 39.37 gradNorm: 33.95 time 0.08\n",
      "Train batch 532: loss: 39.93 gradNorm: 33.19 time 0.07\n",
      "Train batch 533: loss: 41.45 gradNorm: 37.38 time 0.08\n",
      "Train batch 534: loss: 39.65 gradNorm: 33.06 time 0.08\n",
      "Train batch 535: loss: 37.88 gradNorm: 29.40 time 0.07\n",
      "Train batch 536: loss: 41.65 gradNorm: 32.88 time 0.08\n",
      "Train batch 537: loss: 48.08 gradNorm: 36.88 time 0.09\n",
      "Train batch 538: loss: 51.66 gradNorm: 35.86 time 0.08\n",
      "Train batch 539: loss: 35.80 gradNorm: 28.28 time 0.08\n",
      "Train batch 540: loss: 43.94 gradNorm: 34.21 time 0.08\n",
      "Train batch 541: loss: 42.91 gradNorm: 33.93 time 0.08\n",
      "Train batch 542: loss: 41.08 gradNorm: 35.62 time 0.08\n",
      "Train batch 543: loss: 93.80 gradNorm: 66.23 time 0.14\n",
      "Train batch 544: loss: 40.00 gradNorm: 33.75 time 0.07\n",
      "Train batch 545: loss: 34.58 gradNorm: 31.14 time 0.07\n",
      "Train batch 546: loss: 51.80 gradNorm: 42.88 time 0.11\n",
      "Train batch 547: loss: 41.88 gradNorm: 34.16 time 0.09\n",
      "Train batch 548: loss: 52.78 gradNorm: 51.68 time 0.07\n",
      "Train batch 549: loss: 39.66 gradNorm: 31.06 time 0.07\n",
      "Train batch 550: loss: 39.46 gradNorm: 30.02 time 0.08\n",
      "Val batch 550: CER: 0.38 time 2.50\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-550\n",
      "Train batch 551: loss: 53.05 gradNorm: 37.37 time 0.09\n",
      "Train batch 552: loss: 40.96 gradNorm: 39.51 time 0.10\n",
      "Train batch 553: loss: 53.83 gradNorm: 42.79 time 0.08\n",
      "Train batch 554: loss: 40.97 gradNorm: 38.34 time 0.07\n",
      "Train batch 555: loss: 94.28 gradNorm: 62.29 time 0.12\n",
      "Train batch 556: loss: 36.45 gradNorm: 31.32 time 0.08\n",
      "Train batch 557: loss: 93.63 gradNorm: 79.65 time 0.13\n",
      "Train batch 558: loss: 42.81 gradNorm: 35.00 time 0.08\n",
      "Train batch 559: loss: 99.28 gradNorm: 70.18 time 0.14\n",
      "Train batch 560: loss: 39.36 gradNorm: 37.25 time 0.07\n",
      "Train batch 561: loss: 38.76 gradNorm: 32.78 time 0.08\n",
      "Train batch 562: loss: 40.03 gradNorm: 32.87 time 0.07\n",
      "Train batch 563: loss: 37.56 gradNorm: 32.62 time 0.09\n",
      "Train batch 564: loss: 53.01 gradNorm: 39.66 time 0.08\n",
      "Train batch 565: loss: 43.97 gradNorm: 36.36 time 0.08\n",
      "Train batch 566: loss: 39.85 gradNorm: 32.77 time 0.07\n",
      "Train batch 567: loss: 37.19 gradNorm: 30.42 time 0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 568: loss: 40.36 gradNorm: 32.05 time 0.08\n",
      "Train batch 569: loss: 43.41 gradNorm: 37.03 time 0.09\n",
      "Train batch 570: loss: 41.58 gradNorm: 34.14 time 0.08\n",
      "Train batch 571: loss: 38.68 gradNorm: 31.64 time 0.08\n",
      "Train batch 572: loss: 37.48 gradNorm: 30.30 time 0.07\n",
      "Train batch 573: loss: 51.67 gradNorm: 37.64 time 0.08\n",
      "Train batch 574: loss: 37.73 gradNorm: 33.52 time 0.08\n",
      "Train batch 575: loss: 42.09 gradNorm: 33.38 time 0.07\n",
      "Train batch 576: loss: 38.47 gradNorm: 37.76 time 0.07\n",
      "Train batch 577: loss: 39.74 gradNorm: 34.36 time 0.07\n",
      "Train batch 578: loss: 40.46 gradNorm: 35.07 time 0.43\n",
      "Train batch 579: loss: 36.70 gradNorm: 32.78 time 0.08\n",
      "Train batch 580: loss: 45.14 gradNorm: 36.77 time 0.11\n",
      "Train batch 581: loss: 50.87 gradNorm: 40.64 time 0.08\n",
      "Train batch 582: loss: 55.49 gradNorm: 47.91 time 0.08\n",
      "Train batch 583: loss: 40.36 gradNorm: 36.09 time 0.09\n",
      "Train batch 584: loss: 53.77 gradNorm: 42.03 time 0.08\n",
      "Train batch 585: loss: 43.44 gradNorm: 35.26 time 0.10\n",
      "Train batch 586: loss: 34.12 gradNorm: 31.98 time 0.07\n",
      "Train batch 587: loss: 46.29 gradNorm: 40.62 time 0.08\n",
      "Train batch 588: loss: 36.62 gradNorm: 31.08 time 0.07\n",
      "Train batch 589: loss: 45.95 gradNorm: 35.32 time 0.09\n",
      "Train batch 590: loss: 36.32 gradNorm: 35.20 time 0.08\n",
      "Train batch 591: loss: 40.36 gradNorm: 38.68 time 0.08\n",
      "Train batch 592: loss: 40.56 gradNorm: 33.49 time 0.32\n",
      "Train batch 593: loss: 38.92 gradNorm: 31.17 time 0.09\n",
      "Train batch 594: loss: 38.26 gradNorm: 34.52 time 0.08\n",
      "Train batch 595: loss: 54.19 gradNorm: 43.37 time 0.08\n",
      "Train batch 596: loss: 37.58 gradNorm: 34.54 time 0.09\n",
      "Train batch 597: loss: 90.30 gradNorm: 61.83 time 0.15\n",
      "Train batch 598: loss: 38.81 gradNorm: 36.11 time 0.07\n",
      "Train batch 599: loss: 44.31 gradNorm: 38.98 time 0.08\n",
      "Train batch 600: loss: 39.59 gradNorm: 34.51 time 0.09\n",
      "Val batch 600: CER: 0.38 time 2.47\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-600\n",
      "Train batch 601: loss: 36.25 gradNorm: 29.94 time 0.09\n",
      "Train batch 602: loss: 39.73 gradNorm: 37.30 time 0.09\n",
      "Train batch 603: loss: 91.16 gradNorm: 62.57 time 0.13\n",
      "Train batch 604: loss: 49.77 gradNorm: 40.78 time 0.08\n",
      "Train batch 605: loss: 39.66 gradNorm: 32.52 time 0.08\n",
      "Train batch 606: loss: 35.06 gradNorm: 33.65 time 0.08\n",
      "Train batch 607: loss: 40.95 gradNorm: 36.55 time 0.08\n",
      "Train batch 608: loss: 34.99 gradNorm: 33.06 time 0.07\n",
      "Train batch 609: loss: 38.06 gradNorm: 30.49 time 0.55\n",
      "Train batch 610: loss: 38.62 gradNorm: 33.76 time 0.08\n",
      "Train batch 611: loss: 33.73 gradNorm: 31.97 time 0.07\n",
      "Train batch 612: loss: 36.04 gradNorm: 32.58 time 0.07\n",
      "Train batch 613: loss: 33.40 gradNorm: 32.06 time 0.07\n",
      "Train batch 614: loss: 89.93 gradNorm: 67.26 time 0.13\n",
      "Train batch 615: loss: 32.05 gradNorm: 30.56 time 0.07\n",
      "Train batch 616: loss: 32.50 gradNorm: 31.24 time 0.09\n",
      "Train batch 617: loss: 92.51 gradNorm: 70.27 time 0.13\n",
      "Train batch 618: loss: 38.32 gradNorm: 36.25 time 0.08\n",
      "Train batch 619: loss: 42.52 gradNorm: 35.05 time 0.09\n",
      "Train batch 620: loss: 38.53 gradNorm: 34.85 time 0.10\n",
      "Train batch 621: loss: 38.81 gradNorm: 40.94 time 0.09\n",
      "Train batch 622: loss: 36.58 gradNorm: 33.15 time 0.08\n",
      "Train batch 623: loss: 34.18 gradNorm: 32.74 time 0.09\n",
      "Train batch 624: loss: 49.60 gradNorm: 41.26 time 0.07\n",
      "Train batch 625: loss: 34.59 gradNorm: 33.30 time 0.08\n",
      "Train batch 626: loss: 92.34 gradNorm: 64.40 time 0.14\n",
      "Train batch 627: loss: 92.42 gradNorm: 64.91 time 0.14\n",
      "Train batch 628: loss: 38.03 gradNorm: 36.46 time 0.07\n",
      "Train batch 629: loss: 42.92 gradNorm: 45.47 time 0.08\n",
      "Train batch 630: loss: 44.61 gradNorm: 37.67 time 0.11\n",
      "Train batch 631: loss: 38.85 gradNorm: 34.79 time 0.10\n",
      "Train batch 632: loss: 38.24 gradNorm: 35.65 time 0.08\n",
      "Train batch 633: loss: 49.80 gradNorm: 43.10 time 0.11\n",
      "Train batch 634: loss: 35.76 gradNorm: 32.83 time 0.08\n",
      "Train batch 635: loss: 53.45 gradNorm: 47.35 time 0.08\n",
      "Train batch 636: loss: 33.41 gradNorm: 31.31 time 0.07\n",
      "Train batch 637: loss: 37.82 gradNorm: 35.73 time 0.07\n",
      "Train batch 638: loss: 35.56 gradNorm: 30.27 time 0.07\n",
      "Train batch 639: loss: 30.36 gradNorm: 28.14 time 0.07\n",
      "Train batch 640: loss: 35.71 gradNorm: 35.90 time 0.07\n",
      "Train batch 641: loss: 37.87 gradNorm: 38.87 time 0.08\n",
      "Train batch 642: loss: 42.78 gradNorm: 42.20 time 0.07\n",
      "Train batch 643: loss: 39.92 gradNorm: 39.89 time 1.02\n",
      "Train batch 644: loss: 38.46 gradNorm: 36.02 time 0.08\n",
      "Train batch 645: loss: 99.22 gradNorm: 99.81 time 0.14\n",
      "Train batch 646: loss: 95.27 gradNorm: 59.17 time 0.13\n",
      "Train batch 647: loss: 38.60 gradNorm: 38.51 time 0.08\n",
      "Train batch 648: loss: 34.38 gradNorm: 35.40 time 0.07\n",
      "Train batch 649: loss: 36.18 gradNorm: 34.05 time 0.08\n",
      "Train batch 650: loss: 40.86 gradNorm: 37.38 time 0.09\n",
      "Val batch 650: CER: 0.37 time 2.47\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-650\n",
      "Train batch 651: loss: 39.57 gradNorm: 38.34 time 0.09\n",
      "Train batch 652: loss: 38.64 gradNorm: 33.78 time 0.11\n",
      "Train batch 653: loss: 37.34 gradNorm: 32.22 time 0.11\n",
      "Train batch 654: loss: 97.73 gradNorm: 74.68 time 0.15\n",
      "Train batch 655: loss: 35.90 gradNorm: 33.30 time 0.08\n",
      "Train batch 656: loss: 34.48 gradNorm: 35.64 time 0.08\n",
      "Train batch 657: loss: 56.18 gradNorm: 63.62 time 0.09\n",
      "Train batch 658: loss: 34.01 gradNorm: 35.04 time 0.07\n",
      "Train batch 659: loss: 49.78 gradNorm: 45.20 time 0.08\n",
      "Train batch 660: loss: 36.32 gradNorm: 35.36 time 0.08\n",
      "Train batch 661: loss: 38.12 gradNorm: 35.80 time 0.08\n",
      "Train batch 662: loss: 38.42 gradNorm: 33.59 time 0.07\n",
      "Train batch 663: loss: 31.47 gradNorm: 28.74 time 0.07\n",
      "Train batch 664: loss: 36.68 gradNorm: 34.95 time 0.08\n",
      "Train batch 665: loss: 46.19 gradNorm: 38.98 time 0.07\n",
      "Train batch 666: loss: 36.14 gradNorm: 34.06 time 0.07\n",
      "Train batch 667: loss: 83.93 gradNorm: 60.73 time 0.14\n",
      "Train batch 668: loss: 40.66 gradNorm: 37.27 time 0.09\n",
      "Train batch 669: loss: 36.48 gradNorm: 36.05 time 0.09\n",
      "Train batch 670: loss: 32.03 gradNorm: 31.99 time 0.08\n",
      "Train batch 671: loss: 37.56 gradNorm: 32.42 time 0.09\n",
      "Train batch 672: loss: 42.16 gradNorm: 36.01 time 0.10\n",
      "Train batch 673: loss: 37.47 gradNorm: 31.41 time 0.09\n",
      "Train batch 674: loss: 30.23 gradNorm: 29.98 time 0.09\n",
      "Train batch 675: loss: 36.49 gradNorm: 33.32 time 0.09\n",
      "Train batch 676: loss: 37.23 gradNorm: 31.76 time 0.08\n",
      "Train batch 677: loss: 33.13 gradNorm: 29.16 time 0.07\n",
      "Train batch 678: loss: 35.09 gradNorm: 32.37 time 0.10\n",
      "Train batch 679: loss: 43.96 gradNorm: 36.84 time 0.10\n",
      "Train batch 680: loss: 32.03 gradNorm: 27.90 time 0.08\n",
      "Train batch 681: loss: 34.74 gradNorm: 33.25 time 0.08\n",
      "Train batch 682: loss: 35.30 gradNorm: 31.77 time 0.08\n",
      "Train batch 683: loss: 44.51 gradNorm: 40.49 time 0.08\n",
      "Train batch 684: loss: 38.69 gradNorm: 32.06 time 0.08\n",
      "Train batch 685: loss: 32.46 gradNorm: 29.37 time 0.08\n",
      "Train batch 686: loss: 33.10 gradNorm: 32.78 time 0.07\n",
      "Train batch 687: loss: 50.02 gradNorm: 44.23 time 0.08\n",
      "Train batch 688: loss: 35.25 gradNorm: 35.87 time 0.08\n",
      "Train batch 689: loss: 33.17 gradNorm: 32.52 time 0.08\n",
      "Train batch 690: loss: 35.04 gradNorm: 33.09 time 0.08\n",
      "Train batch 691: loss: 35.54 gradNorm: 31.88 time 0.08\n",
      "Train batch 692: loss: 82.18 gradNorm: 56.92 time 0.14\n",
      "Train batch 693: loss: 32.25 gradNorm: 35.41 time 0.08\n",
      "Train batch 694: loss: 42.31 gradNorm: 36.77 time 0.09\n",
      "Train batch 695: loss: 43.89 gradNorm: 41.04 time 0.10\n",
      "Train batch 696: loss: 32.01 gradNorm: 31.90 time 0.08\n",
      "Train batch 697: loss: 51.86 gradNorm: 47.45 time 0.08\n",
      "Train batch 698: loss: 37.99 gradNorm: 39.93 time 0.08\n",
      "Train batch 699: loss: 37.09 gradNorm: 36.85 time 0.07\n",
      "Train batch 700: loss: 35.57 gradNorm: 32.44 time 0.08\n",
      "Val batch 700: CER: 0.36 time 2.59\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-700\n",
      "Train batch 701: loss: 92.25 gradNorm: 75.37 time 0.15\n",
      "Train batch 702: loss: 46.42 gradNorm: 41.25 time 0.09\n",
      "Train batch 703: loss: 36.35 gradNorm: 37.84 time 0.09\n",
      "Train batch 704: loss: 36.62 gradNorm: 37.73 time 0.09\n",
      "Train batch 705: loss: 33.75 gradNorm: 33.24 time 0.09\n",
      "Train batch 706: loss: 37.36 gradNorm: 36.25 time 0.08\n",
      "Train batch 707: loss: 35.11 gradNorm: 38.20 time 0.09\n",
      "Train batch 708: loss: 89.09 gradNorm: 59.74 time 0.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 709: loss: 34.80 gradNorm: 35.72 time 0.09\n",
      "Train batch 710: loss: 35.59 gradNorm: 33.01 time 0.09\n",
      "Train batch 711: loss: 34.80 gradNorm: 30.58 time 0.08\n",
      "Train batch 712: loss: 35.11 gradNorm: 32.36 time 0.10\n",
      "Train batch 713: loss: 29.84 gradNorm: 29.20 time 0.07\n",
      "Train batch 714: loss: 35.71 gradNorm: 36.17 time 0.08\n",
      "Train batch 715: loss: 49.71 gradNorm: 39.80 time 0.09\n",
      "Train batch 716: loss: 47.34 gradNorm: 42.17 time 0.08\n",
      "Train batch 717: loss: 47.22 gradNorm: 45.02 time 0.09\n",
      "Train batch 718: loss: 32.17 gradNorm: 33.90 time 0.07\n",
      "Train batch 719: loss: 33.52 gradNorm: 33.35 time 0.08\n",
      "Train batch 720: loss: 45.56 gradNorm: 36.84 time 0.08\n",
      "Train batch 721: loss: 34.73 gradNorm: 33.86 time 0.08\n",
      "Train batch 722: loss: 45.46 gradNorm: 37.42 time 0.08\n",
      "Train batch 723: loss: 42.92 gradNorm: 35.33 time 0.08\n",
      "Train batch 724: loss: 45.75 gradNorm: 37.56 time 0.08\n",
      "Train batch 725: loss: 45.15 gradNorm: 36.66 time 0.08\n",
      "Train batch 726: loss: 34.67 gradNorm: 32.97 time 0.09\n",
      "Train batch 727: loss: 29.38 gradNorm: 29.23 time 0.08\n",
      "Train batch 728: loss: 51.72 gradNorm: 73.89 time 0.07\n",
      "Train batch 729: loss: 38.14 gradNorm: 34.57 time 0.09\n",
      "Train batch 730: loss: 38.36 gradNorm: 38.30 time 0.08\n",
      "Train batch 731: loss: 34.06 gradNorm: 34.79 time 0.07\n",
      "Train batch 732: loss: 47.05 gradNorm: 42.65 time 0.08\n",
      "Train batch 733: loss: 77.37 gradNorm: 57.99 time 0.13\n",
      "Train batch 734: loss: 37.39 gradNorm: 35.54 time 0.07\n",
      "Train batch 735: loss: 36.63 gradNorm: 33.33 time 0.09\n",
      "Train batch 736: loss: 32.89 gradNorm: 35.07 time 0.08\n",
      "Train batch 737: loss: 36.34 gradNorm: 35.88 time 0.08\n",
      "Train batch 738: loss: 45.63 gradNorm: 38.98 time 0.08\n",
      "Train batch 739: loss: 33.50 gradNorm: 31.84 time 0.08\n",
      "Train batch 740: loss: 31.91 gradNorm: 33.87 time 0.07\n",
      "Train batch 741: loss: 33.76 gradNorm: 34.23 time 0.34\n",
      "Train batch 742: loss: 35.79 gradNorm: 35.54 time 0.10\n",
      "Train batch 743: loss: 32.89 gradNorm: 32.47 time 0.09\n",
      "Train batch 744: loss: 47.10 gradNorm: 39.06 time 0.09\n",
      "Train batch 745: loss: 89.19 gradNorm: 63.82 time 0.14\n",
      "Train batch 746: loss: 35.27 gradNorm: 34.74 time 0.09\n",
      "Train batch 747: loss: 34.88 gradNorm: 33.49 time 0.09\n",
      "Train batch 748: loss: 46.29 gradNorm: 37.19 time 0.11\n",
      "Train batch 749: loss: 43.91 gradNorm: 40.51 time 0.09\n",
      "Train batch 750: loss: 33.62 gradNorm: 31.07 time 0.07\n",
      "Val batch 750: CER: 0.36 time 2.40\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-750\n",
      "Train batch 751: loss: 36.07 gradNorm: 33.07 time 0.10\n",
      "Train batch 752: loss: 33.26 gradNorm: 33.66 time 0.08\n",
      "Train batch 753: loss: 33.29 gradNorm: 31.81 time 0.08\n",
      "Train batch 754: loss: 33.94 gradNorm: 33.24 time 0.10\n",
      "Train batch 755: loss: 43.90 gradNorm: 38.20 time 0.09\n",
      "Train batch 756: loss: 34.00 gradNorm: 31.30 time 0.08\n",
      "Train batch 757: loss: 78.79 gradNorm: 65.71 time 0.14\n",
      "Train batch 758: loss: 35.35 gradNorm: 35.50 time 0.08\n",
      "Train batch 759: loss: 30.35 gradNorm: 31.54 time 0.08\n",
      "Train batch 760: loss: 33.47 gradNorm: 34.36 time 0.08\n",
      "Train batch 761: loss: 80.54 gradNorm: 56.39 time 0.13\n",
      "Train batch 762: loss: 32.19 gradNorm: 29.18 time 0.09\n",
      "Train batch 763: loss: 91.47 gradNorm: 63.19 time 0.13\n",
      "Train batch 764: loss: 34.25 gradNorm: 32.07 time 0.07\n",
      "Train batch 765: loss: 83.49 gradNorm: 66.92 time 0.13\n",
      "Train batch 766: loss: 31.76 gradNorm: 27.77 time 0.08\n",
      "Train batch 767: loss: 48.27 gradNorm: 43.98 time 0.08\n",
      "Train batch 768: loss: 75.09 gradNorm: 51.69 time 0.11\n",
      "Train batch 769: loss: 31.26 gradNorm: 31.05 time 0.09\n",
      "Train batch 770: loss: 82.77 gradNorm: 78.75 time 0.14\n",
      "Train batch 771: loss: 85.14 gradNorm: 63.67 time 0.13\n",
      "Train batch 772: loss: 80.36 gradNorm: 58.02 time 0.29\n",
      "Train batch 773: loss: 37.48 gradNorm: 38.07 time 0.08\n",
      "Train batch 774: loss: 34.79 gradNorm: 33.57 time 0.10\n",
      "Train batch 775: loss: 35.23 gradNorm: 33.69 time 0.07\n",
      "Train batch 776: loss: 34.65 gradNorm: 35.65 time 0.08\n",
      "Train batch 777: loss: 32.55 gradNorm: 32.86 time 0.08\n",
      "Train batch 778: loss: 31.72 gradNorm: 36.67 time 0.08\n",
      "Train batch 779: loss: 35.77 gradNorm: 42.34 time 0.09\n",
      "Train batch 780: loss: 43.23 gradNorm: 39.08 time 0.08\n",
      "Train batch 781: loss: 30.51 gradNorm: 30.85 time 0.07\n",
      "Train batch 782: loss: 49.35 gradNorm: 54.10 time 0.11\n",
      "Train batch 783: loss: 33.49 gradNorm: 39.00 time 0.08\n",
      "Train batch 784: loss: 35.31 gradNorm: 34.13 time 0.09\n",
      "Train batch 785: loss: 33.73 gradNorm: 41.16 time 0.07\n",
      "Train batch 786: loss: 50.16 gradNorm: 58.40 time 0.07\n",
      "Train batch 787: loss: 36.28 gradNorm: 38.91 time 0.08\n",
      "Train batch 788: loss: 36.93 gradNorm: 37.17 time 0.08\n",
      "Train batch 789: loss: 31.74 gradNorm: 31.98 time 0.08\n",
      "Train batch 790: loss: 41.78 gradNorm: 39.83 time 0.08\n",
      "Train batch 791: loss: 28.72 gradNorm: 32.86 time 0.08\n",
      "Train batch 792: loss: 33.71 gradNorm: 36.60 time 0.10\n",
      "Train batch 793: loss: 34.86 gradNorm: 36.03 time 0.10\n",
      "Train batch 794: loss: 32.70 gradNorm: 36.10 time 0.08\n",
      "Train batch 795: loss: 35.20 gradNorm: 38.41 time 0.07\n",
      "Train batch 796: loss: 34.59 gradNorm: 42.31 time 0.07\n",
      "Train batch 797: loss: 31.68 gradNorm: 31.67 time 0.07\n",
      "Train batch 798: loss: 38.19 gradNorm: 37.73 time 0.09\n",
      "Train batch 799: loss: 45.40 gradNorm: 45.96 time 0.08\n",
      "Train batch 800: loss: 41.03 gradNorm: 38.08 time 0.07\n",
      "Val batch 800: CER: 0.34 time 2.47\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-800\n",
      "Train batch 801: loss: 87.50 gradNorm: 59.77 time 0.15\n",
      "Train batch 802: loss: 35.35 gradNorm: 36.85 time 0.08\n",
      "Train batch 803: loss: 74.59 gradNorm: 64.07 time 0.14\n",
      "Train batch 804: loss: 40.97 gradNorm: 36.71 time 0.08\n",
      "Train batch 805: loss: 32.57 gradNorm: 33.39 time 0.08\n",
      "Train batch 806: loss: 32.94 gradNorm: 38.65 time 0.08\n",
      "Train batch 807: loss: 33.07 gradNorm: 35.04 time 0.08\n",
      "Train batch 808: loss: 32.16 gradNorm: 35.77 time 0.08\n",
      "Train batch 809: loss: 29.89 gradNorm: 32.25 time 0.09\n",
      "Train batch 810: loss: 30.61 gradNorm: 28.97 time 0.09\n",
      "Train batch 811: loss: 45.24 gradNorm: 43.17 time 0.09\n",
      "Train batch 812: loss: 44.36 gradNorm: 37.87 time 0.08\n",
      "Train batch 813: loss: 34.40 gradNorm: 40.61 time 0.08\n",
      "Train batch 814: loss: 35.62 gradNorm: 37.07 time 0.08\n",
      "Train batch 815: loss: 47.94 gradNorm: 49.24 time 0.09\n",
      "Train batch 816: loss: 31.02 gradNorm: 31.46 time 0.07\n",
      "Train batch 817: loss: 32.20 gradNorm: 36.65 time 0.07\n",
      "Train batch 818: loss: 33.41 gradNorm: 40.42 time 0.07\n",
      "Train batch 819: loss: 85.47 gradNorm: 69.85 time 0.13\n",
      "Train batch 820: loss: 34.00 gradNorm: 34.26 time 0.07\n",
      "Train batch 821: loss: 33.43 gradNorm: 33.50 time 0.09\n",
      "Train batch 822: loss: 35.17 gradNorm: 42.78 time 0.08\n",
      "Train batch 823: loss: 87.41 gradNorm: 102.38 time 0.13\n",
      "Train batch 824: loss: 44.88 gradNorm: 39.84 time 0.08\n",
      "Train batch 825: loss: 47.51 gradNorm: 47.76 time 0.08\n",
      "Train batch 826: loss: 33.39 gradNorm: 40.72 time 0.08\n",
      "Train batch 827: loss: 34.95 gradNorm: 38.38 time 0.07\n",
      "Train batch 828: loss: 33.13 gradNorm: 36.08 time 0.09\n",
      "Train batch 829: loss: 28.39 gradNorm: 31.99 time 0.08\n",
      "Train batch 830: loss: 45.06 gradNorm: 44.06 time 0.07\n",
      "Train batch 831: loss: 30.47 gradNorm: 37.67 time 0.08\n",
      "Train batch 832: loss: 32.36 gradNorm: 34.68 time 0.08\n",
      "Train batch 833: loss: 39.69 gradNorm: 38.43 time 0.09\n",
      "Train batch 834: loss: 27.27 gradNorm: 28.98 time 0.07\n",
      "Train batch 835: loss: 32.15 gradNorm: 35.91 time 0.08\n",
      "Train batch 836: loss: 31.69 gradNorm: 35.76 time 0.13\n",
      "Train batch 837: loss: 40.51 gradNorm: 37.28 time 0.10\n",
      "Train batch 838: loss: 41.64 gradNorm: 39.81 time 0.08\n",
      "Train batch 839: loss: 29.87 gradNorm: 32.58 time 0.08\n",
      "Train batch 840: loss: 34.94 gradNorm: 33.09 time 0.10\n",
      "Train batch 841: loss: 83.20 gradNorm: 69.75 time 0.13\n",
      "Train batch 842: loss: 33.54 gradNorm: 37.27 time 0.08\n",
      "Train batch 843: loss: 34.22 gradNorm: 37.45 time 0.07\n",
      "Train batch 844: loss: 33.36 gradNorm: 36.69 time 0.08\n",
      "Train batch 845: loss: 32.85 gradNorm: 34.79 time 0.13\n",
      "Train batch 846: loss: 32.46 gradNorm: 31.32 time 0.10\n",
      "Train batch 847: loss: 32.21 gradNorm: 31.04 time 0.09\n",
      "Train batch 848: loss: 30.90 gradNorm: 31.81 time 0.07\n",
      "Train batch 849: loss: 41.17 gradNorm: 37.87 time 0.09\n",
      "Train batch 850: loss: 32.64 gradNorm: 38.45 time 0.08\n",
      "Val batch 850: CER: 0.33 time 2.73\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 851: loss: 79.16 gradNorm: 64.82 time 0.13\n",
      "Train batch 852: loss: 29.44 gradNorm: 32.39 time 0.07\n",
      "Train batch 853: loss: 30.63 gradNorm: 33.98 time 0.08\n",
      "Train batch 854: loss: 25.05 gradNorm: 27.22 time 0.07\n",
      "Train batch 855: loss: 29.91 gradNorm: 29.54 time 0.08\n",
      "Train batch 856: loss: 30.55 gradNorm: 35.56 time 0.08\n",
      "Train batch 857: loss: 36.97 gradNorm: 36.28 time 0.11\n",
      "Train batch 858: loss: 33.56 gradNorm: 34.17 time 0.10\n",
      "Train batch 859: loss: 30.80 gradNorm: 32.11 time 0.07\n",
      "Train batch 860: loss: 29.72 gradNorm: 31.08 time 0.08\n",
      "Train batch 861: loss: 42.00 gradNorm: 45.15 time 0.10\n",
      "Train batch 862: loss: 83.43 gradNorm: 66.51 time 0.13\n",
      "Train batch 863: loss: 39.31 gradNorm: 36.51 time 0.08\n",
      "Train batch 864: loss: 31.47 gradNorm: 36.39 time 0.08\n",
      "Train batch 865: loss: 39.22 gradNorm: 41.24 time 0.09\n",
      "Train batch 866: loss: 43.91 gradNorm: 39.59 time 0.08\n",
      "Train batch 867: loss: 35.40 gradNorm: 35.19 time 0.09\n",
      "Train batch 868: loss: 43.63 gradNorm: 40.92 time 0.08\n",
      "Train batch 869: loss: 77.72 gradNorm: 73.02 time 0.14\n",
      "Train batch 870: loss: 31.55 gradNorm: 35.45 time 0.07\n",
      "Train batch 871: loss: 41.65 gradNorm: 44.76 time 0.08\n",
      "Train batch 872: loss: 30.49 gradNorm: 31.74 time 0.08\n",
      "Train batch 873: loss: 32.26 gradNorm: 35.38 time 0.07\n",
      "Train batch 874: loss: 30.42 gradNorm: 31.20 time 0.08\n",
      "Train batch 875: loss: 73.11 gradNorm: 59.65 time 0.14\n",
      "Train batch 876: loss: 74.10 gradNorm: 51.58 time 0.13\n",
      "Train batch 877: loss: 31.58 gradNorm: 33.41 time 0.07\n",
      "Train batch 878: loss: 34.82 gradNorm: 37.45 time 0.08\n",
      "Train batch 879: loss: 33.05 gradNorm: 32.89 time 0.08\n",
      "Train batch 880: loss: 47.40 gradNorm: 51.92 time 0.07\n",
      "Train batch 881: loss: 27.99 gradNorm: 32.03 time 0.08\n",
      "Train batch 882: loss: 31.91 gradNorm: 36.53 time 0.07\n",
      "Train batch 883: loss: 31.77 gradNorm: 35.33 time 0.08\n",
      "Train batch 884: loss: 47.78 gradNorm: 46.79 time 0.11\n",
      "Train batch 885: loss: 45.67 gradNorm: 46.84 time 0.08\n",
      "Train batch 886: loss: 46.47 gradNorm: 54.41 time 0.10\n",
      "Train batch 887: loss: 82.70 gradNorm: 59.40 time 0.15\n",
      "Train batch 888: loss: 27.45 gradNorm: 33.61 time 0.08\n",
      "Train batch 889: loss: 43.14 gradNorm: 38.14 time 0.08\n",
      "Train batch 890: loss: 76.31 gradNorm: 60.92 time 0.14\n",
      "Train batch 891: loss: 30.55 gradNorm: 29.28 time 0.09\n",
      "Train batch 892: loss: 34.36 gradNorm: 32.17 time 0.09\n",
      "Train batch 893: loss: 34.51 gradNorm: 33.51 time 0.08\n",
      "Train batch 894: loss: 31.16 gradNorm: 30.00 time 0.09\n",
      "Train batch 895: loss: 33.77 gradNorm: 37.93 time 0.09\n",
      "Train batch 896: loss: 28.01 gradNorm: 32.24 time 0.07\n",
      "Train batch 897: loss: 33.18 gradNorm: 38.73 time 0.08\n",
      "Train batch 898: loss: 34.78 gradNorm: 40.31 time 0.12\n",
      "Train batch 899: loss: 30.01 gradNorm: 30.67 time 0.09\n",
      "Train batch 900: loss: 28.97 gradNorm: 34.17 time 0.07\n",
      "Val batch 900: CER: 0.33 time 2.83\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-900\n",
      "Train batch 901: loss: 30.80 gradNorm: 35.01 time 0.08\n",
      "Train batch 902: loss: 41.11 gradNorm: 39.49 time 0.08\n",
      "Train batch 903: loss: 37.29 gradNorm: 35.98 time 0.07\n",
      "Train batch 904: loss: 31.38 gradNorm: 33.89 time 0.07\n",
      "Train batch 905: loss: 76.39 gradNorm: 61.07 time 0.12\n",
      "Train batch 906: loss: 27.32 gradNorm: 29.44 time 0.08\n",
      "Train batch 907: loss: 31.92 gradNorm: 36.20 time 0.07\n",
      "Train batch 908: loss: 34.19 gradNorm: 32.52 time 0.08\n",
      "Train batch 909: loss: 34.30 gradNorm: 32.81 time 0.08\n",
      "Train batch 910: loss: 78.40 gradNorm: 62.83 time 0.13\n",
      "Train batch 911: loss: 31.68 gradNorm: 35.38 time 0.09\n",
      "Train batch 912: loss: 28.74 gradNorm: 32.42 time 0.09\n",
      "Train batch 913: loss: 45.13 gradNorm: 50.01 time 0.07\n",
      "Train batch 914: loss: 29.10 gradNorm: 32.86 time 0.08\n",
      "Train batch 915: loss: 29.07 gradNorm: 33.09 time 0.09\n",
      "Train batch 916: loss: 29.59 gradNorm: 31.28 time 0.08\n",
      "Train batch 917: loss: 76.82 gradNorm: 64.04 time 0.14\n",
      "Train batch 918: loss: 33.61 gradNorm: 34.91 time 0.08\n",
      "Train batch 919: loss: 31.61 gradNorm: 34.50 time 0.08\n",
      "Train batch 920: loss: 46.41 gradNorm: 50.65 time 0.08\n",
      "Train batch 921: loss: 28.39 gradNorm: 34.82 time 0.10\n",
      "Train batch 922: loss: 29.55 gradNorm: 36.27 time 0.08\n",
      "Train batch 923: loss: 32.79 gradNorm: 33.81 time 0.09\n",
      "Train batch 924: loss: 77.56 gradNorm: 62.38 time 0.12\n",
      "Train batch 925: loss: 29.05 gradNorm: 34.55 time 0.07\n",
      "Train batch 926: loss: 30.12 gradNorm: 32.73 time 0.08\n",
      "Train batch 927: loss: 40.87 gradNorm: 45.15 time 0.08\n",
      "Train batch 928: loss: 29.18 gradNorm: 30.61 time 0.09\n",
      "Train batch 929: loss: 31.95 gradNorm: 34.70 time 0.08\n",
      "Train batch 930: loss: 24.90 gradNorm: 30.73 time 0.08\n",
      "Train batch 931: loss: 28.05 gradNorm: 31.41 time 0.07\n",
      "Train batch 932: loss: 30.42 gradNorm: 32.59 time 0.08\n",
      "Train batch 933: loss: 29.75 gradNorm: 35.46 time 0.08\n",
      "Train batch 934: loss: 32.95 gradNorm: 36.72 time 0.08\n",
      "Train batch 935: loss: 72.84 gradNorm: 53.99 time 0.12\n",
      "Train batch 936: loss: 78.60 gradNorm: 56.39 time 0.13\n",
      "Train batch 937: loss: 32.19 gradNorm: 35.32 time 0.09\n",
      "Train batch 938: loss: 32.68 gradNorm: 33.98 time 0.08\n",
      "Train batch 939: loss: 42.34 gradNorm: 39.41 time 0.07\n",
      "Train batch 940: loss: 33.70 gradNorm: 34.49 time 0.08\n",
      "Train batch 941: loss: 78.51 gradNorm: 59.44 time 0.14\n",
      "Train batch 942: loss: 26.85 gradNorm: 28.28 time 0.07\n",
      "Train batch 943: loss: 31.03 gradNorm: 31.31 time 0.08\n",
      "Train batch 944: loss: 29.00 gradNorm: 32.34 time 0.08\n",
      "Train batch 945: loss: 33.44 gradNorm: 33.40 time 0.08\n",
      "Train batch 946: loss: 28.00 gradNorm: 31.20 time 0.07\n",
      "Train batch 947: loss: 26.63 gradNorm: 30.13 time 0.07\n",
      "Train batch 948: loss: 31.36 gradNorm: 34.23 time 0.08\n",
      "Train batch 949: loss: 27.00 gradNorm: 31.73 time 0.08\n",
      "Train batch 950: loss: 28.37 gradNorm: 32.31 time 0.08\n",
      "Val batch 950: CER: 0.32 time 2.44\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-950\n",
      "Train batch 951: loss: 30.76 gradNorm: 35.14 time 0.09\n",
      "Train batch 952: loss: 27.45 gradNorm: 31.59 time 0.09\n",
      "Train batch 953: loss: 26.41 gradNorm: 30.64 time 0.09\n",
      "Train batch 954: loss: 77.80 gradNorm: 63.52 time 0.15\n",
      "Train batch 955: loss: 32.24 gradNorm: 34.08 time 0.08\n",
      "Train batch 956: loss: 39.94 gradNorm: 39.50 time 0.09\n",
      "Train batch 957: loss: 27.09 gradNorm: 30.63 time 0.08\n",
      "Train batch 958: loss: 40.67 gradNorm: 39.12 time 0.07\n",
      "Train batch 959: loss: 38.69 gradNorm: 36.94 time 0.09\n",
      "Train batch 960: loss: 29.23 gradNorm: 31.54 time 0.08\n",
      "Train batch 961: loss: 28.68 gradNorm: 31.85 time 0.07\n",
      "Train batch 962: loss: 70.79 gradNorm: 61.71 time 0.14\n",
      "Train batch 963: loss: 27.80 gradNorm: 34.12 time 0.09\n",
      "Train batch 964: loss: 31.65 gradNorm: 37.03 time 0.08\n",
      "Train batch 965: loss: 33.19 gradNorm: 34.51 time 0.09\n",
      "Train batch 966: loss: 32.50 gradNorm: 37.00 time 0.08\n",
      "Train batch 967: loss: 25.14 gradNorm: 29.35 time 0.09\n",
      "Train batch 968: loss: 36.24 gradNorm: 35.85 time 0.09\n",
      "Train batch 969: loss: 31.36 gradNorm: 36.35 time 0.08\n",
      "Train batch 970: loss: 28.65 gradNorm: 34.01 time 0.10\n",
      "Train batch 971: loss: 43.92 gradNorm: 40.68 time 0.08\n",
      "Train batch 972: loss: 41.74 gradNorm: 41.71 time 0.07\n",
      "Train batch 973: loss: 29.13 gradNorm: 32.49 time 0.08\n",
      "Train batch 974: loss: 28.94 gradNorm: 32.92 time 0.08\n",
      "Train batch 975: loss: 30.34 gradNorm: 31.99 time 0.08\n",
      "Train batch 976: loss: 29.02 gradNorm: 30.46 time 0.07\n",
      "Train batch 977: loss: 77.00 gradNorm: 58.80 time 0.13\n",
      "Train batch 978: loss: 28.08 gradNorm: 33.31 time 0.07\n",
      "Train batch 979: loss: 30.80 gradNorm: 36.53 time 0.08\n",
      "Train batch 980: loss: 29.02 gradNorm: 35.02 time 0.08\n",
      "Train batch 981: loss: 39.90 gradNorm: 40.74 time 0.09\n",
      "Train batch 982: loss: 25.12 gradNorm: 29.25 time 0.09\n",
      "Train batch 983: loss: 30.16 gradNorm: 34.86 time 0.08\n",
      "Train batch 984: loss: 29.90 gradNorm: 34.97 time 0.09\n",
      "Train batch 985: loss: 32.73 gradNorm: 41.44 time 0.09\n",
      "Train batch 986: loss: 27.21 gradNorm: 31.50 time 0.08\n",
      "Train batch 987: loss: 31.81 gradNorm: 32.87 time 0.08\n",
      "Train batch 988: loss: 41.22 gradNorm: 40.67 time 0.11\n",
      "Train batch 989: loss: 28.21 gradNorm: 30.90 time 0.07\n",
      "Train batch 990: loss: 32.48 gradNorm: 33.90 time 0.10\n",
      "Train batch 991: loss: 76.21 gradNorm: 67.73 time 0.13\n",
      "Train batch 992: loss: 33.00 gradNorm: 38.84 time 0.08\n",
      "Train batch 993: loss: 27.11 gradNorm: 31.70 time 0.08\n",
      "Train batch 994: loss: 30.21 gradNorm: 36.75 time 0.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 995: loss: 33.30 gradNorm: 36.32 time 0.09\n",
      "Train batch 996: loss: 28.48 gradNorm: 34.10 time 0.08\n",
      "Train batch 997: loss: 28.95 gradNorm: 39.33 time 0.09\n",
      "Train batch 998: loss: 26.70 gradNorm: 28.90 time 0.07\n",
      "Train batch 999: loss: 26.16 gradNorm: 30.06 time 0.09\n",
      "Train batch 1000: loss: 28.25 gradNorm: 33.16 time 0.08\n",
      "Val batch 1000: CER: 0.32 time 2.41\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1000\n",
      "Train batch 1001: loss: 27.28 gradNorm: 31.96 time 0.07\n",
      "Train batch 1002: loss: 35.75 gradNorm: 41.67 time 0.12\n",
      "Train batch 1003: loss: 27.15 gradNorm: 32.48 time 0.08\n",
      "Train batch 1004: loss: 28.03 gradNorm: 30.30 time 0.09\n",
      "Train batch 1005: loss: 29.05 gradNorm: 32.97 time 0.07\n",
      "Train batch 1006: loss: 40.38 gradNorm: 40.64 time 0.09\n",
      "Train batch 1007: loss: 27.78 gradNorm: 33.13 time 0.08\n",
      "Train batch 1008: loss: 28.92 gradNorm: 30.48 time 0.09\n",
      "Train batch 1009: loss: 74.60 gradNorm: 60.86 time 0.15\n",
      "Train batch 1010: loss: 30.30 gradNorm: 33.44 time 0.09\n",
      "Train batch 1011: loss: 69.45 gradNorm: 54.98 time 0.14\n",
      "Train batch 1012: loss: 26.41 gradNorm: 30.33 time 0.10\n",
      "Train batch 1013: loss: 42.79 gradNorm: 45.15 time 0.08\n",
      "Train batch 1014: loss: 31.40 gradNorm: 32.86 time 0.08\n",
      "Train batch 1015: loss: 30.10 gradNorm: 36.78 time 0.10\n",
      "Train batch 1016: loss: 74.66 gradNorm: 60.11 time 0.13\n",
      "Train batch 1017: loss: 73.99 gradNorm: 52.61 time 0.14\n",
      "Train batch 1018: loss: 31.22 gradNorm: 37.29 time 0.08\n",
      "Train batch 1019: loss: 26.88 gradNorm: 35.40 time 0.08\n",
      "Train batch 1020: loss: 25.79 gradNorm: 31.08 time 0.08\n",
      "Train batch 1021: loss: 27.61 gradNorm: 31.67 time 0.08\n",
      "Train batch 1022: loss: 34.80 gradNorm: 38.16 time 0.09\n",
      "Train batch 1023: loss: 42.63 gradNorm: 49.63 time 0.08\n",
      "Train batch 1024: loss: 27.37 gradNorm: 33.35 time 0.07\n",
      "Train batch 1025: loss: 30.35 gradNorm: 35.62 time 0.08\n",
      "Train batch 1026: loss: 29.26 gradNorm: 33.68 time 0.08\n",
      "Train batch 1027: loss: 32.07 gradNorm: 34.26 time 0.10\n",
      "Train batch 1028: loss: 28.05 gradNorm: 33.61 time 0.08\n",
      "Train batch 1029: loss: 79.19 gradNorm: 72.59 time 0.13\n",
      "Train batch 1030: loss: 27.56 gradNorm: 32.44 time 0.09\n",
      "Train batch 1031: loss: 29.37 gradNorm: 33.35 time 0.09\n",
      "Train batch 1032: loss: 42.47 gradNorm: 41.35 time 0.08\n",
      "Train batch 1033: loss: 41.74 gradNorm: 38.35 time 0.10\n",
      "Train batch 1034: loss: 35.11 gradNorm: 37.02 time 0.08\n",
      "Train batch 1035: loss: 31.04 gradNorm: 34.93 time 0.11\n",
      "Train batch 1036: loss: 29.19 gradNorm: 31.14 time 0.07\n",
      "Train batch 1037: loss: 21.99 gradNorm: 30.77 time 0.07\n",
      "Train batch 1038: loss: 29.04 gradNorm: 33.28 time 0.07\n",
      "Train batch 1039: loss: 30.44 gradNorm: 35.48 time 0.07\n",
      "Train batch 1040: loss: 41.26 gradNorm: 44.15 time 0.08\n",
      "Train batch 1041: loss: 26.78 gradNorm: 32.24 time 0.07\n",
      "Train batch 1042: loss: 23.07 gradNorm: 28.30 time 0.08\n",
      "Train batch 1043: loss: 27.26 gradNorm: 30.81 time 0.07\n",
      "Train batch 1044: loss: 36.63 gradNorm: 37.37 time 0.08\n",
      "Train batch 1045: loss: 39.92 gradNorm: 37.39 time 0.08\n",
      "Train batch 1046: loss: 31.98 gradNorm: 36.28 time 0.08\n",
      "Train batch 1047: loss: 26.19 gradNorm: 33.61 time 0.08\n",
      "Train batch 1048: loss: 29.40 gradNorm: 32.02 time 0.07\n",
      "Train batch 1049: loss: 26.97 gradNorm: 30.47 time 0.07\n",
      "Train batch 1050: loss: 27.04 gradNorm: 34.69 time 0.08\n",
      "Val batch 1050: CER: 0.31 time 2.48\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1050\n",
      "Train batch 1051: loss: 35.75 gradNorm: 37.63 time 0.08\n",
      "Train batch 1052: loss: 26.73 gradNorm: 34.74 time 0.08\n",
      "Train batch 1053: loss: 32.23 gradNorm: 38.72 time 0.08\n",
      "Train batch 1054: loss: 33.54 gradNorm: 33.00 time 0.09\n",
      "Train batch 1055: loss: 29.16 gradNorm: 32.95 time 0.09\n",
      "Train batch 1056: loss: 25.13 gradNorm: 30.71 time 0.09\n",
      "Train batch 1057: loss: 24.15 gradNorm: 27.40 time 0.09\n",
      "Train batch 1058: loss: 32.35 gradNorm: 34.56 time 0.08\n",
      "Train batch 1059: loss: 28.20 gradNorm: 30.89 time 0.09\n",
      "Train batch 1060: loss: 23.25 gradNorm: 29.59 time 0.08\n",
      "Train batch 1061: loss: 72.31 gradNorm: 55.95 time 0.13\n",
      "Train batch 1062: loss: 41.60 gradNorm: 40.60 time 0.09\n",
      "Train batch 1063: loss: 37.69 gradNorm: 35.29 time 0.08\n",
      "Train batch 1064: loss: 26.86 gradNorm: 30.16 time 0.08\n",
      "Train batch 1065: loss: 28.13 gradNorm: 32.12 time 0.09\n",
      "Train batch 1066: loss: 32.13 gradNorm: 32.47 time 0.09\n",
      "Train batch 1067: loss: 29.23 gradNorm: 38.05 time 0.08\n",
      "Train batch 1068: loss: 29.55 gradNorm: 33.93 time 0.09\n",
      "Train batch 1069: loss: 37.16 gradNorm: 36.95 time 0.09\n",
      "Train batch 1070: loss: 27.71 gradNorm: 31.41 time 0.08\n",
      "Train batch 1071: loss: 26.19 gradNorm: 31.54 time 0.09\n",
      "Train batch 1072: loss: 25.46 gradNorm: 27.89 time 0.08\n",
      "Train batch 1073: loss: 74.35 gradNorm: 60.80 time 0.13\n",
      "Train batch 1074: loss: 27.46 gradNorm: 34.61 time 0.07\n",
      "Train batch 1075: loss: 24.74 gradNorm: 32.79 time 0.07\n",
      "Train batch 1076: loss: 25.68 gradNorm: 32.38 time 0.07\n",
      "Train batch 1077: loss: 76.98 gradNorm: 69.90 time 0.14\n",
      "Train batch 1078: loss: 29.96 gradNorm: 34.77 time 0.10\n",
      "Train batch 1079: loss: 29.05 gradNorm: 32.17 time 0.07\n",
      "Train batch 1080: loss: 25.90 gradNorm: 30.78 time 0.08\n",
      "Train batch 1081: loss: 40.38 gradNorm: 42.31 time 0.07\n",
      "Train batch 1082: loss: 39.53 gradNorm: 36.94 time 0.07\n",
      "Train batch 1083: loss: 30.20 gradNorm: 35.32 time 0.08\n",
      "Train batch 1084: loss: 72.40 gradNorm: 65.02 time 0.12\n",
      "Train batch 1085: loss: 33.69 gradNorm: 34.06 time 0.09\n",
      "Train batch 1086: loss: 32.35 gradNorm: 33.93 time 0.09\n",
      "Train batch 1087: loss: 29.25 gradNorm: 33.03 time 0.08\n",
      "Train batch 1088: loss: 25.77 gradNorm: 31.48 time 0.08\n",
      "Train batch 1089: loss: 25.35 gradNorm: 30.58 time 0.10\n",
      "Train batch 1090: loss: 26.60 gradNorm: 29.90 time 0.11\n",
      "Train batch 1091: loss: 28.96 gradNorm: 33.24 time 0.08\n",
      "Train batch 1092: loss: 28.74 gradNorm: 32.36 time 0.08\n",
      "Train batch 1093: loss: 43.25 gradNorm: 46.02 time 0.08\n",
      "Train batch 1094: loss: 28.14 gradNorm: 33.77 time 0.08\n",
      "Train batch 1095: loss: 26.41 gradNorm: 33.84 time 0.08\n",
      "Train batch 1096: loss: 28.07 gradNorm: 36.93 time 0.07\n",
      "Train batch 1097: loss: 72.47 gradNorm: 60.74 time 0.12\n",
      "Train batch 1098: loss: 27.80 gradNorm: 33.92 time 0.07\n",
      "Train batch 1099: loss: 24.71 gradNorm: 30.52 time 0.08\n",
      "Train batch 1100: loss: 27.63 gradNorm: 32.62 time 0.07\n",
      "Val batch 1100: CER: 0.31 time 2.61\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1100\n",
      "Train batch 1101: loss: 34.20 gradNorm: 36.28 time 0.10\n",
      "Train batch 1102: loss: 24.71 gradNorm: 28.07 time 0.09\n",
      "Train batch 1103: loss: 74.34 gradNorm: 60.14 time 0.12\n",
      "Train batch 1104: loss: 38.06 gradNorm: 36.07 time 0.08\n",
      "Train batch 1105: loss: 25.90 gradNorm: 32.87 time 0.10\n",
      "Train batch 1106: loss: 31.50 gradNorm: 34.86 time 0.10\n",
      "Train batch 1107: loss: 28.21 gradNorm: 32.84 time 0.07\n",
      "Train batch 1108: loss: 30.11 gradNorm: 33.40 time 0.10\n",
      "Train batch 1109: loss: 24.28 gradNorm: 29.71 time 0.08\n",
      "Train batch 1110: loss: 76.91 gradNorm: 66.52 time 0.14\n",
      "Train batch 1111: loss: 24.75 gradNorm: 32.47 time 0.08\n",
      "Train batch 1112: loss: 26.80 gradNorm: 34.06 time 0.08\n",
      "Train batch 1113: loss: 23.65 gradNorm: 31.46 time 0.08\n",
      "Train batch 1114: loss: 27.81 gradNorm: 33.82 time 0.10\n",
      "Train batch 1115: loss: 26.32 gradNorm: 31.96 time 0.07\n",
      "Train batch 1116: loss: 26.75 gradNorm: 31.58 time 0.08\n",
      "Train batch 1117: loss: 31.65 gradNorm: 41.03 time 0.08\n",
      "Train batch 1118: loss: 27.43 gradNorm: 32.45 time 0.08\n",
      "Train batch 1119: loss: 29.71 gradNorm: 35.40 time 0.08\n",
      "Train batch 1120: loss: 46.70 gradNorm: 66.25 time 0.07\n",
      "Train batch 1121: loss: 29.34 gradNorm: 32.62 time 0.08\n",
      "Train batch 1122: loss: 26.64 gradNorm: 29.53 time 0.10\n",
      "Train batch 1123: loss: 27.57 gradNorm: 29.40 time 0.09\n",
      "Train batch 1124: loss: 26.78 gradNorm: 37.45 time 0.08\n",
      "Train batch 1125: loss: 26.80 gradNorm: 33.96 time 0.07\n",
      "Train batch 1126: loss: 65.78 gradNorm: 56.80 time 0.11\n",
      "Train batch 1127: loss: 25.38 gradNorm: 32.72 time 0.07\n",
      "Train batch 1128: loss: 29.01 gradNorm: 32.00 time 0.07\n",
      "Train batch 1129: loss: 26.21 gradNorm: 32.58 time 0.08\n",
      "Train batch 1130: loss: 72.22 gradNorm: 65.67 time 0.13\n",
      "Train batch 1131: loss: 26.79 gradNorm: 34.58 time 0.07\n",
      "Train batch 1132: loss: 24.06 gradNorm: 30.51 time 0.10\n",
      "Train batch 1133: loss: 25.30 gradNorm: 30.04 time 0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 1134: loss: 27.64 gradNorm: 34.12 time 0.08\n",
      "Train batch 1135: loss: 34.98 gradNorm: 39.71 time 0.09\n",
      "Train batch 1136: loss: 29.21 gradNorm: 35.22 time 0.08\n",
      "Train batch 1137: loss: 33.96 gradNorm: 35.83 time 0.09\n",
      "Train batch 1138: loss: 66.94 gradNorm: 52.85 time 0.13\n",
      "Train batch 1139: loss: 26.87 gradNorm: 32.65 time 0.08\n",
      "Train batch 1140: loss: 27.79 gradNorm: 33.23 time 0.07\n",
      "Train batch 1141: loss: 43.98 gradNorm: 45.41 time 0.09\n",
      "Train batch 1142: loss: 24.56 gradNorm: 31.68 time 0.08\n",
      "Train batch 1143: loss: 39.36 gradNorm: 36.97 time 0.08\n",
      "Train batch 1144: loss: 27.53 gradNorm: 32.02 time 0.09\n",
      "Train batch 1145: loss: 29.88 gradNorm: 35.07 time 0.08\n",
      "Train batch 1146: loss: 29.25 gradNorm: 35.34 time 0.07\n",
      "Train batch 1147: loss: 71.69 gradNorm: 60.37 time 0.13\n",
      "Train batch 1148: loss: 27.52 gradNorm: 32.43 time 0.07\n",
      "Train batch 1149: loss: 27.85 gradNorm: 34.33 time 0.08\n",
      "Train batch 1150: loss: 27.76 gradNorm: 31.33 time 0.08\n",
      "Val batch 1150: CER: 0.30 time 2.41\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1150\n",
      "Train batch 1151: loss: 32.49 gradNorm: 38.54 time 0.11\n",
      "Train batch 1152: loss: 68.53 gradNorm: 58.65 time 0.15\n",
      "Train batch 1153: loss: 24.17 gradNorm: 28.50 time 0.08\n",
      "Train batch 1154: loss: 42.36 gradNorm: 62.92 time 0.08\n",
      "Train batch 1155: loss: 75.65 gradNorm: 56.94 time 0.13\n",
      "Train batch 1156: loss: 34.66 gradNorm: 38.31 time 0.07\n",
      "Train batch 1157: loss: 26.78 gradNorm: 35.49 time 0.07\n",
      "Train batch 1158: loss: 29.52 gradNorm: 32.26 time 0.11\n",
      "Train batch 1159: loss: 24.28 gradNorm: 30.24 time 0.08\n",
      "Train batch 1160: loss: 23.57 gradNorm: 31.02 time 0.07\n",
      "Train batch 1161: loss: 25.67 gradNorm: 31.97 time 0.08\n",
      "Train batch 1162: loss: 26.39 gradNorm: 30.44 time 0.09\n",
      "Train batch 1163: loss: 33.53 gradNorm: 33.89 time 0.09\n",
      "Train batch 1164: loss: 24.93 gradNorm: 31.14 time 0.07\n",
      "Train batch 1165: loss: 70.44 gradNorm: 57.61 time 0.13\n",
      "Train batch 1166: loss: 24.75 gradNorm: 33.74 time 0.08\n",
      "Train batch 1167: loss: 28.37 gradNorm: 32.17 time 0.08\n",
      "Train batch 1168: loss: 25.30 gradNorm: 33.67 time 0.07\n",
      "Train batch 1169: loss: 68.36 gradNorm: 54.81 time 0.13\n",
      "Train batch 1170: loss: 71.62 gradNorm: 63.02 time 0.15\n",
      "Train batch 1171: loss: 25.85 gradNorm: 31.98 time 0.08\n",
      "Train batch 1172: loss: 25.54 gradNorm: 28.93 time 0.07\n",
      "Train batch 1173: loss: 23.47 gradNorm: 28.46 time 0.07\n",
      "Train batch 1174: loss: 29.12 gradNorm: 31.38 time 0.07\n",
      "Train batch 1175: loss: 37.97 gradNorm: 40.32 time 0.07\n",
      "Train batch 1176: loss: 23.43 gradNorm: 28.77 time 0.08\n",
      "Train batch 1177: loss: 26.68 gradNorm: 32.36 time 0.08\n",
      "Train batch 1178: loss: 26.45 gradNorm: 31.95 time 0.08\n",
      "Train batch 1179: loss: 26.77 gradNorm: 31.25 time 0.09\n",
      "Train batch 1180: loss: 40.78 gradNorm: 42.57 time 0.08\n",
      "Train batch 1181: loss: 27.95 gradNorm: 34.02 time 0.09\n",
      "Train batch 1182: loss: 25.09 gradNorm: 34.02 time 0.08\n",
      "Train batch 1183: loss: 26.55 gradNorm: 33.48 time 0.09\n",
      "Train batch 1184: loss: 21.48 gradNorm: 27.67 time 0.08\n",
      "Train batch 1185: loss: 26.13 gradNorm: 31.88 time 0.07\n",
      "Train batch 1186: loss: 23.17 gradNorm: 32.73 time 0.07\n",
      "Train batch 1187: loss: 25.03 gradNorm: 32.96 time 0.09\n",
      "Train batch 1188: loss: 23.76 gradNorm: 29.98 time 0.10\n",
      "Train batch 1189: loss: 42.13 gradNorm: 42.79 time 0.09\n",
      "Train batch 1190: loss: 31.67 gradNorm: 35.57 time 0.08\n",
      "Train batch 1191: loss: 37.43 gradNorm: 39.80 time 0.11\n",
      "Train batch 1192: loss: 75.16 gradNorm: 62.21 time 0.12\n",
      "Train batch 1193: loss: 72.58 gradNorm: 54.62 time 0.14\n",
      "Train batch 1194: loss: 38.51 gradNorm: 39.24 time 0.08\n",
      "Train batch 1195: loss: 30.26 gradNorm: 36.42 time 0.07\n",
      "Train batch 1196: loss: 27.31 gradNorm: 32.35 time 0.07\n",
      "Train batch 1197: loss: 26.09 gradNorm: 31.34 time 0.08\n",
      "Train batch 1198: loss: 26.75 gradNorm: 34.77 time 0.08\n",
      "Train batch 1199: loss: 41.49 gradNorm: 43.81 time 0.09\n",
      "Train batch 1200: loss: 36.84 gradNorm: 39.26 time 0.08\n",
      "Val batch 1200: CER: 0.30 time 2.44\n",
      "Train batch 1201: loss: 29.80 gradNorm: 38.71 time 0.08\n",
      "Train batch 1202: loss: 23.93 gradNorm: 29.94 time 0.08\n",
      "Train batch 1203: loss: 24.33 gradNorm: 33.93 time 0.08\n",
      "Train batch 1204: loss: 29.61 gradNorm: 36.43 time 0.07\n",
      "Train batch 1205: loss: 38.15 gradNorm: 40.07 time 0.08\n",
      "Train batch 1206: loss: 25.14 gradNorm: 31.13 time 0.09\n",
      "Train batch 1207: loss: 25.50 gradNorm: 37.58 time 0.07\n",
      "Train batch 1208: loss: 66.58 gradNorm: 57.11 time 0.11\n",
      "Train batch 1209: loss: 19.39 gradNorm: 27.53 time 0.10\n",
      "Train batch 1210: loss: 29.29 gradNorm: 34.48 time 0.09\n",
      "Train batch 1211: loss: 31.45 gradNorm: 41.72 time 0.08\n",
      "Train batch 1212: loss: 24.83 gradNorm: 34.20 time 0.08\n",
      "Train batch 1213: loss: 31.21 gradNorm: 35.85 time 0.08\n",
      "Train batch 1214: loss: 30.78 gradNorm: 34.37 time 0.11\n",
      "Train batch 1215: loss: 25.83 gradNorm: 32.22 time 0.08\n",
      "Train batch 1216: loss: 71.63 gradNorm: 73.64 time 0.15\n",
      "Train batch 1217: loss: 25.43 gradNorm: 31.97 time 0.09\n",
      "Train batch 1218: loss: 25.66 gradNorm: 33.04 time 0.39\n",
      "Train batch 1219: loss: 26.27 gradNorm: 34.88 time 0.08\n",
      "Train batch 1220: loss: 26.57 gradNorm: 36.12 time 0.07\n",
      "Train batch 1221: loss: 29.99 gradNorm: 33.78 time 0.08\n",
      "Train batch 1222: loss: 25.45 gradNorm: 34.36 time 0.08\n",
      "Train batch 1223: loss: 26.65 gradNorm: 34.28 time 0.08\n",
      "Train batch 1224: loss: 27.53 gradNorm: 36.03 time 0.09\n",
      "Train batch 1225: loss: 72.84 gradNorm: 67.24 time 0.13\n",
      "Train batch 1226: loss: 21.25 gradNorm: 29.26 time 0.09\n",
      "Train batch 1227: loss: 26.21 gradNorm: 36.14 time 0.09\n",
      "Train batch 1228: loss: 38.54 gradNorm: 41.58 time 0.08\n",
      "Train batch 1229: loss: 24.92 gradNorm: 31.70 time 0.07\n",
      "Train batch 1230: loss: 70.21 gradNorm: 71.75 time 0.14\n",
      "Train batch 1231: loss: 61.26 gradNorm: 50.90 time 0.13\n",
      "Train batch 1232: loss: 24.78 gradNorm: 36.73 time 0.08\n",
      "Train batch 1233: loss: 25.84 gradNorm: 33.57 time 0.07\n",
      "Train batch 1234: loss: 40.78 gradNorm: 46.39 time 0.08\n",
      "Train batch 1235: loss: 37.44 gradNorm: 41.47 time 0.08\n",
      "Train batch 1236: loss: 24.27 gradNorm: 32.16 time 0.07\n",
      "Train batch 1237: loss: 24.91 gradNorm: 34.41 time 0.08\n",
      "Train batch 1238: loss: 25.73 gradNorm: 30.98 time 0.08\n",
      "Train batch 1239: loss: 24.34 gradNorm: 30.85 time 0.08\n",
      "Train batch 1240: loss: 27.32 gradNorm: 38.94 time 0.07\n",
      "Train batch 1241: loss: 40.50 gradNorm: 39.62 time 0.10\n",
      "Train batch 1242: loss: 32.07 gradNorm: 41.02 time 0.08\n",
      "Train batch 1243: loss: 36.49 gradNorm: 42.79 time 0.08\n",
      "Train batch 1244: loss: 25.63 gradNorm: 33.48 time 0.07\n",
      "Train batch 1245: loss: 26.17 gradNorm: 34.18 time 0.08\n",
      "Train batch 1246: loss: 32.42 gradNorm: 41.77 time 0.09\n",
      "Train batch 1247: loss: 26.42 gradNorm: 32.49 time 0.07\n",
      "Train batch 1248: loss: 25.50 gradNorm: 33.27 time 0.07\n",
      "Train batch 1249: loss: 26.16 gradNorm: 33.39 time 0.07\n",
      "Train batch 1250: loss: 29.59 gradNorm: 38.02 time 0.08\n",
      "Val batch 1250: CER: 0.29 time 2.45\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1250\n",
      "Train batch 1251: loss: 26.77 gradNorm: 37.01 time 0.09\n",
      "Train batch 1252: loss: 35.36 gradNorm: 40.43 time 0.08\n",
      "Train batch 1253: loss: 24.02 gradNorm: 33.35 time 0.09\n",
      "Train batch 1254: loss: 27.10 gradNorm: 36.44 time 0.09\n",
      "Train batch 1255: loss: 22.51 gradNorm: 28.83 time 0.08\n",
      "Train batch 1256: loss: 25.27 gradNorm: 30.35 time 0.09\n",
      "Train batch 1257: loss: 23.31 gradNorm: 29.06 time 0.08\n",
      "Train batch 1258: loss: 24.40 gradNorm: 29.83 time 0.10\n",
      "Train batch 1259: loss: 35.02 gradNorm: 36.97 time 0.08\n",
      "Train batch 1260: loss: 27.93 gradNorm: 33.00 time 0.08\n",
      "Train batch 1261: loss: 27.10 gradNorm: 33.28 time 0.09\n",
      "Train batch 1262: loss: 26.52 gradNorm: 33.08 time 0.08\n",
      "Train batch 1263: loss: 28.14 gradNorm: 36.68 time 0.08\n",
      "Train batch 1264: loss: 24.27 gradNorm: 30.46 time 0.07\n",
      "Train batch 1265: loss: 26.42 gradNorm: 33.20 time 0.08\n",
      "Train batch 1266: loss: 26.35 gradNorm: 34.46 time 0.08\n",
      "Train batch 1267: loss: 24.57 gradNorm: 33.80 time 0.07\n",
      "Train batch 1268: loss: 24.73 gradNorm: 30.86 time 0.10\n",
      "Train batch 1269: loss: 24.50 gradNorm: 30.20 time 0.08\n",
      "Train batch 1270: loss: 35.28 gradNorm: 41.20 time 0.08\n",
      "Train batch 1271: loss: 32.12 gradNorm: 36.94 time 0.08\n",
      "Train batch 1272: loss: 26.12 gradNorm: 34.05 time 0.08\n",
      "Train batch 1273: loss: 23.41 gradNorm: 31.24 time 0.07\n",
      "Train batch 1274: loss: 29.08 gradNorm: 35.29 time 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 1275: loss: 23.40 gradNorm: 27.37 time 0.07\n",
      "Train batch 1276: loss: 25.36 gradNorm: 36.05 time 0.08\n",
      "Train batch 1277: loss: 77.04 gradNorm: 71.89 time 0.15\n",
      "Train batch 1278: loss: 26.66 gradNorm: 32.47 time 0.07\n",
      "Train batch 1279: loss: 23.40 gradNorm: 31.31 time 0.08\n",
      "Train batch 1280: loss: 29.14 gradNorm: 34.48 time 0.08\n",
      "Train batch 1281: loss: 24.06 gradNorm: 30.43 time 0.08\n",
      "Train batch 1282: loss: 26.82 gradNorm: 34.35 time 0.07\n",
      "Train batch 1283: loss: 26.58 gradNorm: 32.32 time 0.09\n",
      "Train batch 1284: loss: 22.67 gradNorm: 29.98 time 0.07\n",
      "Train batch 1285: loss: 21.94 gradNorm: 30.20 time 0.08\n",
      "Train batch 1286: loss: 27.08 gradNorm: 32.77 time 0.08\n",
      "Train batch 1287: loss: 27.31 gradNorm: 36.32 time 0.08\n",
      "Train batch 1288: loss: 66.97 gradNorm: 55.30 time 0.13\n",
      "Train batch 1289: loss: 27.31 gradNorm: 36.10 time 0.07\n",
      "Train batch 1290: loss: 38.21 gradNorm: 44.08 time 0.11\n",
      "Train batch 1291: loss: 27.48 gradNorm: 34.68 time 0.08\n",
      "Train batch 1292: loss: 65.11 gradNorm: 57.47 time 0.12\n",
      "Train batch 1293: loss: 31.75 gradNorm: 42.03 time 0.10\n",
      "Train batch 1294: loss: 29.98 gradNorm: 34.28 time 0.11\n",
      "Train batch 1295: loss: 23.21 gradNorm: 32.95 time 0.07\n",
      "Train batch 1296: loss: 31.56 gradNorm: 36.84 time 0.08\n",
      "Train batch 1297: loss: 24.06 gradNorm: 31.71 time 0.08\n",
      "Train batch 1298: loss: 23.88 gradNorm: 30.63 time 0.07\n",
      "Train batch 1299: loss: 42.79 gradNorm: 45.34 time 0.08\n",
      "Train batch 1300: loss: 38.71 gradNorm: 41.17 time 0.09\n",
      "Val batch 1300: CER: 0.29 time 2.88\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1300\n",
      "Train batch 1301: loss: 24.98 gradNorm: 32.95 time 0.08\n",
      "Train batch 1302: loss: 29.39 gradNorm: 37.68 time 0.08\n",
      "Train batch 1303: loss: 33.23 gradNorm: 40.53 time 0.08\n",
      "Train batch 1304: loss: 32.35 gradNorm: 37.25 time 0.08\n",
      "Train batch 1305: loss: 29.59 gradNorm: 34.35 time 0.09\n",
      "Train batch 1306: loss: 26.17 gradNorm: 31.74 time 0.07\n",
      "Train batch 1307: loss: 24.44 gradNorm: 34.22 time 0.08\n",
      "Train batch 1308: loss: 24.17 gradNorm: 33.88 time 0.08\n",
      "Train batch 1309: loss: 22.96 gradNorm: 30.94 time 0.08\n",
      "Train batch 1310: loss: 22.18 gradNorm: 33.06 time 0.07\n",
      "Train batch 1311: loss: 32.51 gradNorm: 37.21 time 0.08\n",
      "Train batch 1312: loss: 25.82 gradNorm: 33.40 time 0.09\n",
      "Train batch 1313: loss: 23.01 gradNorm: 35.93 time 0.08\n",
      "Train batch 1314: loss: 25.45 gradNorm: 34.13 time 0.07\n",
      "Train batch 1315: loss: 24.06 gradNorm: 35.70 time 0.07\n",
      "Train batch 1316: loss: 26.50 gradNorm: 36.96 time 0.07\n",
      "Train batch 1317: loss: 33.47 gradNorm: 36.32 time 0.08\n",
      "Train batch 1318: loss: 23.01 gradNorm: 29.03 time 0.07\n",
      "Train batch 1319: loss: 30.93 gradNorm: 34.45 time 0.11\n",
      "Train batch 1320: loss: 22.87 gradNorm: 30.06 time 0.10\n",
      "Train batch 1321: loss: 37.13 gradNorm: 40.68 time 0.09\n",
      "Train batch 1322: loss: 67.98 gradNorm: 57.82 time 0.14\n",
      "Train batch 1323: loss: 26.03 gradNorm: 34.75 time 0.08\n",
      "Train batch 1324: loss: 67.77 gradNorm: 60.62 time 0.13\n",
      "Train batch 1325: loss: 36.46 gradNorm: 40.01 time 0.07\n",
      "Train batch 1326: loss: 64.49 gradNorm: 55.16 time 0.13\n",
      "Train batch 1327: loss: 32.40 gradNorm: 35.55 time 0.08\n",
      "Train batch 1328: loss: 27.76 gradNorm: 35.47 time 0.09\n",
      "Train batch 1329: loss: 25.70 gradNorm: 35.77 time 0.08\n",
      "Train batch 1330: loss: 23.02 gradNorm: 32.53 time 0.08\n",
      "Train batch 1331: loss: 22.92 gradNorm: 32.33 time 0.08\n",
      "Train batch 1332: loss: 36.67 gradNorm: 39.62 time 0.11\n",
      "Train batch 1333: loss: 67.65 gradNorm: 56.21 time 0.12\n",
      "Train batch 1334: loss: 36.85 gradNorm: 41.40 time 0.08\n",
      "Train batch 1335: loss: 24.66 gradNorm: 33.95 time 0.07\n",
      "Train batch 1336: loss: 22.33 gradNorm: 29.87 time 0.07\n",
      "Train batch 1337: loss: 70.20 gradNorm: 64.72 time 0.14\n",
      "Train batch 1338: loss: 26.47 gradNorm: 30.94 time 0.07\n",
      "Train batch 1339: loss: 23.76 gradNorm: 31.51 time 0.08\n",
      "Train batch 1340: loss: 25.62 gradNorm: 31.45 time 0.07\n",
      "Train batch 1341: loss: 20.03 gradNorm: 25.47 time 0.08\n",
      "Train batch 1342: loss: 23.62 gradNorm: 30.46 time 0.09\n",
      "Train batch 1343: loss: 24.48 gradNorm: 30.51 time 0.09\n",
      "Train batch 1344: loss: 23.81 gradNorm: 31.81 time 0.08\n",
      "Train batch 1345: loss: 71.42 gradNorm: 60.21 time 0.13\n",
      "Train batch 1346: loss: 26.82 gradNorm: 30.87 time 0.08\n",
      "Train batch 1347: loss: 26.37 gradNorm: 33.06 time 0.07\n",
      "Train batch 1348: loss: 27.27 gradNorm: 32.49 time 0.09\n",
      "Train batch 1349: loss: 60.71 gradNorm: 51.96 time 0.14\n",
      "Train batch 1350: loss: 26.07 gradNorm: 32.00 time 0.48\n",
      "Val batch 1350: CER: 0.29 time 2.36\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1350\n",
      "Train batch 1351: loss: 33.20 gradNorm: 38.74 time 0.08\n",
      "Train batch 1352: loss: 21.37 gradNorm: 30.93 time 0.09\n",
      "Train batch 1353: loss: 24.87 gradNorm: 34.08 time 0.08\n",
      "Train batch 1354: loss: 31.41 gradNorm: 35.65 time 0.10\n",
      "Train batch 1355: loss: 23.90 gradNorm: 31.02 time 0.08\n",
      "Train batch 1356: loss: 26.92 gradNorm: 37.91 time 0.09\n",
      "Train batch 1357: loss: 22.07 gradNorm: 31.28 time 0.08\n",
      "Train batch 1358: loss: 22.46 gradNorm: 31.63 time 0.08\n",
      "Train batch 1359: loss: 25.93 gradNorm: 34.22 time 0.07\n",
      "Train batch 1360: loss: 31.15 gradNorm: 37.18 time 0.08\n",
      "Train batch 1361: loss: 27.13 gradNorm: 32.93 time 0.07\n",
      "Train batch 1362: loss: 74.03 gradNorm: 61.16 time 0.13\n",
      "Train batch 1363: loss: 24.74 gradNorm: 32.77 time 0.08\n",
      "Train batch 1364: loss: 20.03 gradNorm: 29.72 time 0.09\n",
      "Train batch 1365: loss: 22.63 gradNorm: 30.91 time 0.08\n",
      "Train batch 1366: loss: 24.60 gradNorm: 30.99 time 0.08\n",
      "Train batch 1367: loss: 22.17 gradNorm: 32.50 time 0.08\n",
      "Train batch 1368: loss: 22.66 gradNorm: 31.80 time 0.07\n",
      "Train batch 1369: loss: 25.31 gradNorm: 30.84 time 0.08\n",
      "Train batch 1370: loss: 28.90 gradNorm: 35.37 time 0.08\n",
      "Train batch 1371: loss: 26.26 gradNorm: 33.20 time 0.08\n",
      "Train batch 1372: loss: 23.62 gradNorm: 31.33 time 0.07\n",
      "Train batch 1373: loss: 23.76 gradNorm: 34.42 time 0.07\n",
      "Train batch 1374: loss: 22.09 gradNorm: 30.29 time 0.08\n",
      "Train batch 1375: loss: 34.46 gradNorm: 37.77 time 0.08\n",
      "Train batch 1376: loss: 29.17 gradNorm: 36.71 time 0.10\n",
      "Train batch 1377: loss: 40.30 gradNorm: 46.42 time 0.07\n",
      "Train batch 1378: loss: 22.89 gradNorm: 33.91 time 0.09\n",
      "Train batch 1379: loss: 73.16 gradNorm: 74.26 time 0.14\n",
      "Train batch 1380: loss: 27.66 gradNorm: 36.67 time 0.08\n",
      "Train batch 1381: loss: 24.18 gradNorm: 32.74 time 0.08\n",
      "Train batch 1382: loss: 38.25 gradNorm: 40.59 time 0.09\n",
      "Train batch 1383: loss: 23.35 gradNorm: 31.72 time 0.09\n",
      "Train batch 1384: loss: 24.62 gradNorm: 34.76 time 0.08\n",
      "Train batch 1385: loss: 22.89 gradNorm: 34.25 time 0.09\n",
      "Train batch 1386: loss: 23.26 gradNorm: 31.01 time 0.07\n",
      "Train batch 1387: loss: 23.26 gradNorm: 32.81 time 0.07\n",
      "Train batch 1388: loss: 25.91 gradNorm: 33.24 time 0.07\n",
      "Train batch 1389: loss: 26.50 gradNorm: 34.79 time 0.07\n",
      "Train batch 1390: loss: 26.66 gradNorm: 36.63 time 0.10\n",
      "Train batch 1391: loss: 24.81 gradNorm: 34.89 time 0.10\n",
      "Train batch 1392: loss: 27.08 gradNorm: 33.97 time 0.08\n",
      "Train batch 1393: loss: 22.18 gradNorm: 30.82 time 0.07\n",
      "Train batch 1394: loss: 21.04 gradNorm: 29.62 time 0.07\n",
      "Train batch 1395: loss: 39.07 gradNorm: 54.68 time 0.08\n",
      "Train batch 1396: loss: 25.31 gradNorm: 34.06 time 0.08\n",
      "Train batch 1397: loss: 21.85 gradNorm: 30.10 time 0.08\n",
      "Train batch 1398: loss: 32.42 gradNorm: 42.89 time 0.08\n",
      "Train batch 1399: loss: 20.01 gradNorm: 29.87 time 0.07\n",
      "Train batch 1400: loss: 64.15 gradNorm: 62.58 time 0.13\n",
      "Val batch 1400: CER: 0.29 time 2.41\n",
      "Train batch 1401: loss: 25.43 gradNorm: 36.01 time 0.09\n",
      "Train batch 1402: loss: 36.95 gradNorm: 41.12 time 0.08\n",
      "Train batch 1403: loss: 26.07 gradNorm: 33.47 time 0.09\n",
      "Train batch 1404: loss: 41.11 gradNorm: 47.64 time 0.07\n",
      "Train batch 1405: loss: 32.80 gradNorm: 36.75 time 0.10\n",
      "Train batch 1406: loss: 24.36 gradNorm: 32.85 time 0.08\n",
      "Train batch 1407: loss: 22.37 gradNorm: 32.62 time 0.08\n",
      "Train batch 1408: loss: 26.13 gradNorm: 37.55 time 0.09\n",
      "Train batch 1409: loss: 73.33 gradNorm: 66.74 time 0.14\n",
      "Train batch 1410: loss: 65.07 gradNorm: 66.51 time 0.13\n",
      "Train batch 1411: loss: 25.07 gradNorm: 35.66 time 0.07\n",
      "Train batch 1412: loss: 26.07 gradNorm: 34.01 time 0.08\n",
      "Train batch 1413: loss: 26.62 gradNorm: 37.26 time 0.07\n",
      "Train batch 1414: loss: 25.73 gradNorm: 33.72 time 0.07\n",
      "Train batch 1415: loss: 20.18 gradNorm: 33.26 time 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 1416: loss: 25.10 gradNorm: 38.08 time 0.08\n",
      "Train batch 1417: loss: 25.53 gradNorm: 38.09 time 0.08\n",
      "Train batch 1418: loss: 64.19 gradNorm: 58.05 time 0.12\n",
      "Train batch 1419: loss: 25.02 gradNorm: 35.17 time 0.08\n",
      "Train batch 1420: loss: 37.31 gradNorm: 37.22 time 0.11\n",
      "Train batch 1421: loss: 40.54 gradNorm: 47.58 time 0.09\n",
      "Train batch 1422: loss: 23.08 gradNorm: 31.46 time 0.07\n",
      "Train batch 1423: loss: 23.58 gradNorm: 29.90 time 0.08\n",
      "Train batch 1424: loss: 23.81 gradNorm: 31.47 time 0.07\n",
      "Train batch 1425: loss: 71.85 gradNorm: 61.71 time 0.13\n",
      "Train batch 1426: loss: 34.06 gradNorm: 40.78 time 0.08\n",
      "Train batch 1427: loss: 25.26 gradNorm: 34.23 time 0.07\n",
      "Train batch 1428: loss: 23.55 gradNorm: 32.30 time 0.09\n",
      "Train batch 1429: loss: 28.12 gradNorm: 33.91 time 0.08\n",
      "Train batch 1430: loss: 33.60 gradNorm: 38.79 time 0.08\n",
      "Train batch 1431: loss: 37.61 gradNorm: 40.12 time 0.09\n",
      "Train batch 1432: loss: 19.58 gradNorm: 27.47 time 0.08\n",
      "Train batch 1433: loss: 24.36 gradNorm: 32.62 time 0.07\n",
      "Train batch 1434: loss: 25.35 gradNorm: 34.13 time 0.08\n",
      "Train batch 1435: loss: 19.00 gradNorm: 28.50 time 0.08\n",
      "Train batch 1436: loss: 21.64 gradNorm: 30.70 time 0.08\n",
      "Train batch 1437: loss: 23.14 gradNorm: 29.39 time 0.07\n",
      "Train batch 1438: loss: 66.31 gradNorm: 66.65 time 0.14\n",
      "Train batch 1439: loss: 25.83 gradNorm: 34.21 time 0.08\n",
      "Train batch 1440: loss: 32.49 gradNorm: 40.78 time 0.08\n",
      "Train batch 1441: loss: 23.17 gradNorm: 32.58 time 0.07\n",
      "Train batch 1442: loss: 21.65 gradNorm: 29.32 time 0.08\n",
      "Train batch 1443: loss: 20.27 gradNorm: 27.68 time 0.08\n",
      "Train batch 1444: loss: 28.50 gradNorm: 33.98 time 0.09\n",
      "Train batch 1445: loss: 25.75 gradNorm: 37.86 time 0.08\n",
      "Train batch 1446: loss: 24.96 gradNorm: 31.87 time 0.09\n",
      "Train batch 1447: loss: 21.02 gradNorm: 29.95 time 0.08\n",
      "Train batch 1448: loss: 70.53 gradNorm: 60.58 time 0.14\n",
      "Train batch 1449: loss: 25.47 gradNorm: 34.23 time 0.08\n",
      "Train batch 1450: loss: 35.54 gradNorm: 41.69 time 0.09\n",
      "Val batch 1450: CER: 0.27 time 2.56\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1450\n",
      "Train batch 1451: loss: 23.42 gradNorm: 30.48 time 0.09\n",
      "Train batch 1452: loss: 68.82 gradNorm: 55.10 time 0.13\n",
      "Train batch 1453: loss: 63.07 gradNorm: 50.47 time 0.13\n",
      "Train batch 1454: loss: 36.26 gradNorm: 48.15 time 0.08\n",
      "Train batch 1455: loss: 24.98 gradNorm: 33.20 time 0.07\n",
      "Train batch 1456: loss: 22.48 gradNorm: 30.21 time 0.08\n",
      "Train batch 1457: loss: 23.24 gradNorm: 34.91 time 0.09\n",
      "Train batch 1458: loss: 21.93 gradNorm: 31.02 time 0.07\n",
      "Train batch 1459: loss: 22.19 gradNorm: 35.21 time 0.09\n",
      "Train batch 1460: loss: 30.99 gradNorm: 37.16 time 0.08\n",
      "Train batch 1461: loss: 29.02 gradNorm: 36.65 time 0.08\n",
      "Train batch 1462: loss: 21.59 gradNorm: 30.98 time 0.08\n",
      "Train batch 1463: loss: 22.72 gradNorm: 31.01 time 0.10\n",
      "Train batch 1464: loss: 26.77 gradNorm: 35.84 time 0.08\n",
      "Train batch 1465: loss: 23.40 gradNorm: 31.23 time 0.08\n",
      "Train batch 1466: loss: 21.74 gradNorm: 31.46 time 0.09\n",
      "Train batch 1467: loss: 19.38 gradNorm: 30.34 time 0.08\n",
      "Train batch 1468: loss: 21.26 gradNorm: 29.63 time 0.07\n",
      "Train batch 1469: loss: 34.64 gradNorm: 45.14 time 0.09\n",
      "Train batch 1470: loss: 22.53 gradNorm: 33.29 time 0.08\n",
      "Train batch 1471: loss: 24.03 gradNorm: 35.29 time 0.08\n",
      "Train batch 1472: loss: 24.77 gradNorm: 33.87 time 0.09\n",
      "Train batch 1473: loss: 22.03 gradNorm: 32.79 time 0.09\n",
      "Train batch 1474: loss: 22.14 gradNorm: 32.51 time 0.09\n",
      "Train batch 1475: loss: 23.30 gradNorm: 32.26 time 0.08\n",
      "Train batch 1476: loss: 22.62 gradNorm: 32.70 time 0.08\n",
      "Train batch 1477: loss: 23.44 gradNorm: 34.18 time 0.07\n",
      "Train batch 1478: loss: 26.02 gradNorm: 32.53 time 0.08\n",
      "Train batch 1479: loss: 18.59 gradNorm: 30.46 time 0.07\n",
      "Train batch 1480: loss: 26.38 gradNorm: 32.53 time 0.08\n",
      "Train batch 1481: loss: 25.41 gradNorm: 36.44 time 0.08\n",
      "Train batch 1482: loss: 18.82 gradNorm: 25.66 time 0.07\n",
      "Train batch 1483: loss: 64.45 gradNorm: 51.75 time 0.13\n",
      "Train batch 1484: loss: 25.10 gradNorm: 31.57 time 0.09\n",
      "Train batch 1485: loss: 24.69 gradNorm: 34.34 time 0.10\n",
      "Train batch 1486: loss: 22.67 gradNorm: 29.44 time 0.07\n",
      "Train batch 1487: loss: 31.61 gradNorm: 41.97 time 0.08\n",
      "Train batch 1488: loss: 20.17 gradNorm: 28.68 time 0.08\n",
      "Train batch 1489: loss: 34.12 gradNorm: 43.03 time 0.10\n",
      "Train batch 1490: loss: 23.17 gradNorm: 33.91 time 0.08\n",
      "Train batch 1491: loss: 23.67 gradNorm: 35.86 time 0.07\n",
      "Train batch 1492: loss: 24.34 gradNorm: 34.97 time 0.07\n",
      "Train batch 1493: loss: 27.02 gradNorm: 35.99 time 0.08\n",
      "Train batch 1494: loss: 35.37 gradNorm: 40.30 time 0.08\n",
      "Train batch 1495: loss: 31.70 gradNorm: 38.22 time 0.08\n",
      "Train batch 1496: loss: 22.99 gradNorm: 34.10 time 0.07\n",
      "Train batch 1497: loss: 19.09 gradNorm: 30.92 time 0.09\n",
      "Train batch 1498: loss: 21.55 gradNorm: 35.28 time 0.08\n",
      "Train batch 1499: loss: 19.73 gradNorm: 29.36 time 0.08\n",
      "Train batch 1500: loss: 25.12 gradNorm: 31.94 time 0.08\n",
      "Val batch 1500: CER: 0.27 time 2.46\n",
      "Train batch 1501: loss: 63.64 gradNorm: 52.53 time 0.14\n",
      "Train batch 1502: loss: 22.15 gradNorm: 34.76 time 0.08\n",
      "Train batch 1503: loss: 25.00 gradNorm: 33.60 time 0.08\n",
      "Train batch 1504: loss: 20.30 gradNorm: 29.57 time 0.08\n",
      "Train batch 1505: loss: 22.09 gradNorm: 31.08 time 0.08\n",
      "Train batch 1506: loss: 19.82 gradNorm: 30.74 time 0.08\n",
      "Train batch 1507: loss: 17.75 gradNorm: 27.66 time 0.35\n",
      "Train batch 1508: loss: 23.34 gradNorm: 32.41 time 0.09\n",
      "Train batch 1509: loss: 18.10 gradNorm: 27.95 time 0.17\n",
      "Train batch 1510: loss: 31.50 gradNorm: 36.20 time 0.08\n",
      "Train batch 1511: loss: 35.84 gradNorm: 40.68 time 0.08\n",
      "Train batch 1512: loss: 33.11 gradNorm: 39.30 time 0.08\n",
      "Train batch 1513: loss: 25.37 gradNorm: 33.88 time 0.09\n",
      "Train batch 1514: loss: 24.99 gradNorm: 34.41 time 0.08\n",
      "Train batch 1515: loss: 23.07 gradNorm: 31.80 time 0.08\n",
      "Train batch 1516: loss: 25.18 gradNorm: 33.58 time 0.08\n",
      "Train batch 1517: loss: 21.80 gradNorm: 30.09 time 0.07\n",
      "Train batch 1518: loss: 31.39 gradNorm: 37.99 time 0.08\n",
      "Train batch 1519: loss: 23.43 gradNorm: 32.56 time 0.07\n",
      "Train batch 1520: loss: 22.40 gradNorm: 31.32 time 0.07\n",
      "Train batch 1521: loss: 72.19 gradNorm: 62.78 time 0.15\n",
      "Train batch 1522: loss: 22.62 gradNorm: 29.11 time 0.08\n",
      "Train batch 1523: loss: 65.16 gradNorm: 53.47 time 0.14\n",
      "Train batch 1524: loss: 23.60 gradNorm: 30.28 time 0.08\n",
      "Train batch 1525: loss: 21.91 gradNorm: 29.75 time 0.08\n",
      "Train batch 1526: loss: 20.76 gradNorm: 33.58 time 0.09\n",
      "Train batch 1527: loss: 24.77 gradNorm: 36.91 time 0.08\n",
      "Train batch 1528: loss: 23.41 gradNorm: 35.12 time 0.07\n",
      "Train batch 1529: loss: 21.61 gradNorm: 30.96 time 0.08\n",
      "Train batch 1530: loss: 22.96 gradNorm: 33.03 time 0.07\n",
      "Train batch 1531: loss: 22.40 gradNorm: 30.94 time 0.08\n",
      "Train batch 1532: loss: 22.28 gradNorm: 29.90 time 0.08\n",
      "Train batch 1533: loss: 21.64 gradNorm: 33.44 time 0.10\n",
      "Train batch 1534: loss: 20.62 gradNorm: 32.33 time 0.09\n",
      "Train batch 1535: loss: 19.62 gradNorm: 28.13 time 0.07\n",
      "Train batch 1536: loss: 23.26 gradNorm: 34.40 time 0.08\n",
      "Train batch 1537: loss: 21.76 gradNorm: 33.57 time 0.08\n",
      "Train batch 1538: loss: 65.27 gradNorm: 59.38 time 0.13\n",
      "Train batch 1539: loss: 23.70 gradNorm: 32.92 time 0.10\n",
      "Train batch 1540: loss: 25.96 gradNorm: 33.72 time 0.13\n",
      "Train batch 1541: loss: 67.32 gradNorm: 57.31 time 0.15\n",
      "Train batch 1542: loss: 23.85 gradNorm: 34.90 time 0.08\n",
      "Train batch 1543: loss: 40.31 gradNorm: 41.58 time 0.08\n",
      "Train batch 1544: loss: 20.06 gradNorm: 28.05 time 0.07\n",
      "Train batch 1545: loss: 25.61 gradNorm: 32.89 time 0.08\n",
      "Train batch 1546: loss: 61.90 gradNorm: 61.89 time 0.14\n",
      "Train batch 1547: loss: 20.84 gradNorm: 30.54 time 0.07\n",
      "Train batch 1548: loss: 36.62 gradNorm: 38.32 time 0.07\n",
      "Train batch 1549: loss: 23.61 gradNorm: 34.03 time 0.07\n",
      "Train batch 1550: loss: 23.84 gradNorm: 36.28 time 0.08\n",
      "Val batch 1550: CER: 0.27 time 2.77\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1550\n",
      "Train batch 1551: loss: 30.76 gradNorm: 41.96 time 0.09\n",
      "Train batch 1552: loss: 21.59 gradNorm: 30.65 time 0.10\n",
      "Train batch 1553: loss: 21.53 gradNorm: 28.64 time 0.08\n",
      "Train batch 1554: loss: 24.89 gradNorm: 36.83 time 0.09\n",
      "Train batch 1555: loss: 19.34 gradNorm: 27.89 time 0.07\n",
      "Train batch 1556: loss: 21.47 gradNorm: 30.56 time 0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 1557: loss: 24.72 gradNorm: 32.14 time 0.07\n",
      "Train batch 1558: loss: 20.73 gradNorm: 29.56 time 0.07\n",
      "Train batch 1559: loss: 22.02 gradNorm: 31.56 time 0.07\n",
      "Train batch 1560: loss: 26.57 gradNorm: 33.37 time 0.10\n",
      "Train batch 1561: loss: 19.01 gradNorm: 27.08 time 0.07\n",
      "Train batch 1562: loss: 30.43 gradNorm: 37.03 time 0.09\n",
      "Train batch 1563: loss: 28.60 gradNorm: 34.08 time 0.10\n",
      "Train batch 1564: loss: 20.62 gradNorm: 30.35 time 0.07\n",
      "Train batch 1565: loss: 23.04 gradNorm: 32.65 time 0.08\n",
      "Train batch 1566: loss: 24.22 gradNorm: 33.41 time 0.08\n",
      "Train batch 1567: loss: 16.97 gradNorm: 23.30 time 0.07\n",
      "Train batch 1568: loss: 23.69 gradNorm: 35.15 time 0.09\n",
      "Train batch 1569: loss: 22.26 gradNorm: 34.52 time 0.08\n",
      "Train batch 1570: loss: 25.23 gradNorm: 32.41 time 0.09\n",
      "Train batch 1571: loss: 21.16 gradNorm: 30.86 time 0.08\n",
      "Train batch 1572: loss: 20.91 gradNorm: 32.84 time 0.08\n",
      "Train batch 1573: loss: 21.96 gradNorm: 38.09 time 0.08\n",
      "Train batch 1574: loss: 23.16 gradNorm: 32.00 time 0.10\n",
      "Train batch 1575: loss: 29.58 gradNorm: 36.93 time 0.08\n",
      "Train batch 1576: loss: 22.78 gradNorm: 30.49 time 0.09\n",
      "Train batch 1577: loss: 22.52 gradNorm: 35.06 time 0.08\n",
      "Train batch 1578: loss: 68.62 gradNorm: 64.64 time 0.13\n",
      "Train batch 1579: loss: 24.54 gradNorm: 34.36 time 0.08\n",
      "Train batch 1580: loss: 20.98 gradNorm: 30.24 time 0.08\n",
      "Train batch 1581: loss: 18.73 gradNorm: 30.65 time 0.07\n",
      "Train batch 1582: loss: 19.09 gradNorm: 31.16 time 0.08\n",
      "Train batch 1583: loss: 20.79 gradNorm: 33.41 time 0.07\n",
      "Train batch 1584: loss: 28.04 gradNorm: 35.65 time 0.08\n",
      "Train batch 1585: loss: 22.87 gradNorm: 31.57 time 0.09\n",
      "Train batch 1586: loss: 23.82 gradNorm: 31.01 time 0.08\n",
      "Train batch 1587: loss: 33.27 gradNorm: 41.32 time 0.08\n",
      "Train batch 1588: loss: 25.71 gradNorm: 38.75 time 0.08\n",
      "Train batch 1589: loss: 21.52 gradNorm: 31.18 time 0.11\n",
      "Train batch 1590: loss: 24.59 gradNorm: 34.37 time 0.07\n",
      "Train batch 1591: loss: 20.16 gradNorm: 31.53 time 0.08\n",
      "Train batch 1592: loss: 21.33 gradNorm: 31.00 time 0.07\n",
      "Train batch 1593: loss: 22.13 gradNorm: 30.58 time 0.07\n",
      "Train batch 1594: loss: 23.27 gradNorm: 37.16 time 0.10\n",
      "Train batch 1595: loss: 23.48 gradNorm: 32.15 time 0.08\n",
      "Train batch 1596: loss: 20.74 gradNorm: 29.44 time 0.08\n",
      "Train batch 1597: loss: 26.97 gradNorm: 34.11 time 0.10\n",
      "Train batch 1598: loss: 22.09 gradNorm: 28.84 time 0.10\n",
      "Train batch 1599: loss: 31.52 gradNorm: 37.52 time 0.12\n",
      "Train batch 1600: loss: 21.76 gradNorm: 33.29 time 0.09\n",
      "Val batch 1600: CER: 0.27 time 2.46\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1600\n",
      "Train batch 1601: loss: 60.51 gradNorm: 52.43 time 0.15\n",
      "Train batch 1602: loss: 22.01 gradNorm: 34.67 time 0.08\n",
      "Train batch 1603: loss: 23.92 gradNorm: 34.96 time 0.09\n",
      "Train batch 1604: loss: 67.03 gradNorm: 57.16 time 0.13\n",
      "Train batch 1605: loss: 20.15 gradNorm: 29.68 time 0.08\n",
      "Train batch 1606: loss: 21.74 gradNorm: 29.61 time 0.09\n",
      "Train batch 1607: loss: 20.86 gradNorm: 34.40 time 0.07\n",
      "Train batch 1608: loss: 29.42 gradNorm: 37.27 time 0.08\n",
      "Train batch 1609: loss: 24.02 gradNorm: 33.00 time 0.09\n",
      "Train batch 1610: loss: 21.57 gradNorm: 28.62 time 0.08\n",
      "Train batch 1611: loss: 57.83 gradNorm: 56.45 time 0.13\n",
      "Train batch 1612: loss: 20.55 gradNorm: 28.23 time 0.07\n",
      "Train batch 1613: loss: 26.01 gradNorm: 32.36 time 0.10\n",
      "Train batch 1614: loss: 63.20 gradNorm: 61.65 time 0.15\n",
      "Train batch 1615: loss: 23.59 gradNorm: 32.00 time 0.08\n",
      "Train batch 1616: loss: 36.23 gradNorm: 41.82 time 0.09\n",
      "Train batch 1617: loss: 24.19 gradNorm: 31.50 time 0.11\n",
      "Train batch 1618: loss: 27.99 gradNorm: 35.38 time 0.08\n",
      "Train batch 1619: loss: 28.21 gradNorm: 35.55 time 0.11\n",
      "Train batch 1620: loss: 25.11 gradNorm: 34.79 time 0.08\n",
      "Train batch 1621: loss: 18.57 gradNorm: 29.84 time 0.09\n",
      "Train batch 1622: loss: 28.75 gradNorm: 34.24 time 0.08\n",
      "Train batch 1623: loss: 21.59 gradNorm: 33.59 time 0.08\n",
      "Train batch 1624: loss: 57.26 gradNorm: 53.55 time 0.12\n",
      "Train batch 1625: loss: 66.91 gradNorm: 53.71 time 0.13\n",
      "Train batch 1626: loss: 20.53 gradNorm: 30.88 time 0.07\n",
      "Train batch 1627: loss: 21.79 gradNorm: 31.50 time 0.08\n",
      "Train batch 1628: loss: 66.86 gradNorm: 56.38 time 0.14\n",
      "Train batch 1629: loss: 21.52 gradNorm: 32.31 time 0.10\n",
      "Train batch 1630: loss: 57.36 gradNorm: 52.46 time 0.12\n",
      "Train batch 1631: loss: 23.50 gradNorm: 37.42 time 0.07\n",
      "Train batch 1632: loss: 22.19 gradNorm: 30.76 time 0.07\n",
      "Train batch 1633: loss: 23.33 gradNorm: 32.14 time 0.08\n",
      "Train batch 1634: loss: 20.82 gradNorm: 28.01 time 0.07\n",
      "Train batch 1635: loss: 36.37 gradNorm: 43.18 time 0.09\n",
      "Train batch 1636: loss: 23.93 gradNorm: 33.98 time 0.07\n",
      "Train batch 1637: loss: 21.00 gradNorm: 32.65 time 0.07\n",
      "Train batch 1638: loss: 22.35 gradNorm: 31.34 time 0.08\n",
      "Train batch 1639: loss: 75.76 gradNorm: 66.20 time 0.15\n",
      "Train batch 1640: loss: 21.70 gradNorm: 31.24 time 0.07\n",
      "Train batch 1641: loss: 18.91 gradNorm: 28.98 time 0.08\n",
      "Train batch 1642: loss: 20.86 gradNorm: 33.35 time 0.08\n",
      "Train batch 1643: loss: 33.52 gradNorm: 44.97 time 0.08\n",
      "Train batch 1644: loss: 26.41 gradNorm: 35.96 time 0.08\n",
      "Train batch 1645: loss: 32.29 gradNorm: 39.58 time 0.09\n",
      "Train batch 1646: loss: 21.62 gradNorm: 31.08 time 0.09\n",
      "Train batch 1647: loss: 62.97 gradNorm: 58.10 time 0.14\n",
      "Train batch 1648: loss: 18.58 gradNorm: 28.79 time 0.08\n",
      "Train batch 1649: loss: 28.25 gradNorm: 37.04 time 0.08\n",
      "Train batch 1650: loss: 35.40 gradNorm: 46.52 time 0.08\n",
      "Val batch 1650: CER: 0.27 time 2.50\n",
      "Train batch 1651: loss: 66.87 gradNorm: 70.15 time 0.13\n",
      "Train batch 1652: loss: 33.90 gradNorm: 46.96 time 0.08\n",
      "Train batch 1653: loss: 22.29 gradNorm: 32.57 time 0.08\n",
      "Train batch 1654: loss: 31.15 gradNorm: 36.62 time 0.09\n",
      "Train batch 1655: loss: 20.46 gradNorm: 27.80 time 0.08\n",
      "Train batch 1656: loss: 20.59 gradNorm: 35.65 time 0.10\n",
      "Train batch 1657: loss: 21.98 gradNorm: 32.14 time 0.08\n",
      "Train batch 1658: loss: 22.22 gradNorm: 31.01 time 0.07\n",
      "Train batch 1659: loss: 35.74 gradNorm: 43.73 time 0.08\n",
      "Train batch 1660: loss: 64.10 gradNorm: 67.53 time 0.14\n",
      "Train batch 1661: loss: 32.31 gradNorm: 40.69 time 1.12\n",
      "Train batch 1662: loss: 21.96 gradNorm: 28.58 time 0.10\n",
      "Train batch 1663: loss: 20.92 gradNorm: 32.68 time 0.08\n",
      "Train batch 1664: loss: 31.69 gradNorm: 36.89 time 0.11\n",
      "Train batch 1665: loss: 20.68 gradNorm: 29.15 time 0.08\n",
      "Train batch 1666: loss: 21.72 gradNorm: 31.41 time 0.08\n",
      "Train batch 1667: loss: 21.53 gradNorm: 30.27 time 0.08\n",
      "Train batch 1668: loss: 32.28 gradNorm: 41.98 time 0.06\n",
      "Train batch 1669: loss: 28.67 gradNorm: 36.14 time 0.08\n",
      "Train batch 1670: loss: 21.18 gradNorm: 32.72 time 0.08\n",
      "Train batch 1671: loss: 24.54 gradNorm: 35.20 time 0.08\n",
      "Train batch 1672: loss: 21.03 gradNorm: 34.20 time 0.09\n",
      "Train batch 1673: loss: 18.79 gradNorm: 31.06 time 0.08\n",
      "Train batch 1674: loss: 23.88 gradNorm: 36.91 time 0.07\n",
      "Train batch 1675: loss: 34.05 gradNorm: 47.69 time 0.08\n",
      "Train batch 1676: loss: 22.18 gradNorm: 33.54 time 0.07\n",
      "Train batch 1677: loss: 18.58 gradNorm: 28.23 time 0.08\n",
      "Train batch 1678: loss: 22.62 gradNorm: 32.71 time 0.07\n",
      "Train batch 1679: loss: 25.92 gradNorm: 33.05 time 0.09\n",
      "Train batch 1680: loss: 22.71 gradNorm: 29.59 time 0.08\n",
      "Train batch 1681: loss: 17.66 gradNorm: 28.28 time 0.07\n",
      "Train batch 1682: loss: 20.46 gradNorm: 34.94 time 0.08\n",
      "Train batch 1683: loss: 18.90 gradNorm: 31.01 time 0.07\n",
      "Train batch 1684: loss: 20.52 gradNorm: 29.72 time 0.07\n",
      "Train batch 1685: loss: 29.36 gradNorm: 32.89 time 0.10\n",
      "Train batch 1686: loss: 20.88 gradNorm: 35.24 time 0.08\n",
      "Train batch 1687: loss: 20.91 gradNorm: 31.70 time 0.09\n",
      "Train batch 1688: loss: 36.47 gradNorm: 39.96 time 0.09\n",
      "Train batch 1689: loss: 19.76 gradNorm: 29.68 time 0.08\n",
      "Train batch 1690: loss: 32.18 gradNorm: 38.16 time 0.08\n",
      "Train batch 1691: loss: 62.45 gradNorm: 54.72 time 0.12\n",
      "Train batch 1692: loss: 20.04 gradNorm: 31.58 time 0.08\n",
      "Train batch 1693: loss: 19.76 gradNorm: 30.93 time 0.10\n",
      "Train batch 1694: loss: 23.06 gradNorm: 33.99 time 0.08\n",
      "Train batch 1695: loss: 19.52 gradNorm: 30.94 time 0.07\n",
      "Train batch 1696: loss: 64.83 gradNorm: 56.30 time 0.13\n",
      "Train batch 1697: loss: 32.54 gradNorm: 35.85 time 0.08\n",
      "Train batch 1698: loss: 20.69 gradNorm: 30.63 time 0.08\n",
      "Train batch 1699: loss: 55.99 gradNorm: 56.02 time 0.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 1700: loss: 22.79 gradNorm: 34.87 time 0.08\n",
      "Val batch 1700: CER: 0.26 time 2.48\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1700\n",
      "Train batch 1701: loss: 20.00 gradNorm: 30.97 time 0.08\n",
      "Train batch 1702: loss: 20.38 gradNorm: 28.89 time 0.08\n",
      "Train batch 1703: loss: 19.08 gradNorm: 28.03 time 0.07\n",
      "Train batch 1704: loss: 20.06 gradNorm: 28.85 time 0.08\n",
      "Train batch 1705: loss: 22.16 gradNorm: 35.41 time 0.08\n",
      "Train batch 1706: loss: 23.01 gradNorm: 32.15 time 0.08\n",
      "Train batch 1707: loss: 59.71 gradNorm: 51.82 time 0.13\n",
      "Train batch 1708: loss: 19.77 gradNorm: 32.41 time 0.07\n",
      "Train batch 1709: loss: 67.10 gradNorm: 63.70 time 0.15\n",
      "Train batch 1710: loss: 23.20 gradNorm: 33.93 time 0.08\n",
      "Train batch 1711: loss: 21.71 gradNorm: 31.29 time 0.08\n",
      "Train batch 1712: loss: 20.73 gradNorm: 33.69 time 0.09\n",
      "Train batch 1713: loss: 22.15 gradNorm: 30.38 time 0.09\n",
      "Train batch 1714: loss: 62.17 gradNorm: 57.60 time 0.14\n",
      "Train batch 1715: loss: 20.92 gradNorm: 30.45 time 0.08\n",
      "Train batch 1716: loss: 20.78 gradNorm: 31.66 time 0.09\n",
      "Train batch 1717: loss: 22.80 gradNorm: 32.01 time 0.07\n",
      "Train batch 1718: loss: 21.67 gradNorm: 35.35 time 0.07\n",
      "Train batch 1719: loss: 19.22 gradNorm: 33.71 time 0.08\n",
      "Train batch 1720: loss: 22.11 gradNorm: 32.17 time 0.08\n",
      "Train batch 1721: loss: 59.61 gradNorm: 62.53 time 0.14\n",
      "Train batch 1722: loss: 57.46 gradNorm: 56.94 time 0.14\n",
      "Train batch 1723: loss: 24.15 gradNorm: 36.44 time 0.11\n",
      "Train batch 1724: loss: 20.12 gradNorm: 29.67 time 0.08\n",
      "Train batch 1725: loss: 32.16 gradNorm: 40.19 time 0.11\n",
      "Train batch 1726: loss: 37.76 gradNorm: 42.58 time 0.07\n",
      "Train batch 1727: loss: 18.70 gradNorm: 34.24 time 0.08\n",
      "Train batch 1728: loss: 64.29 gradNorm: 58.23 time 0.13\n",
      "Train batch 1729: loss: 19.95 gradNorm: 27.23 time 0.07\n",
      "Train batch 1730: loss: 20.53 gradNorm: 31.05 time 0.08\n",
      "Train batch 1731: loss: 24.30 gradNorm: 36.17 time 0.08\n",
      "Train batch 1732: loss: 23.32 gradNorm: 32.31 time 0.07\n",
      "Train batch 1733: loss: 15.81 gradNorm: 25.40 time 0.08\n",
      "Train batch 1734: loss: 19.96 gradNorm: 31.50 time 0.08\n",
      "Train batch 1735: loss: 18.28 gradNorm: 30.27 time 0.07\n",
      "Train batch 1736: loss: 23.40 gradNorm: 33.49 time 0.08\n",
      "Train batch 1737: loss: 19.33 gradNorm: 31.36 time 0.08\n",
      "Train batch 1738: loss: 24.27 gradNorm: 33.47 time 0.07\n",
      "Train batch 1739: loss: 19.18 gradNorm: 31.05 time 0.07\n",
      "Train batch 1740: loss: 35.24 gradNorm: 39.65 time 0.08\n",
      "Train batch 1741: loss: 62.75 gradNorm: 56.78 time 0.13\n",
      "Train batch 1742: loss: 29.16 gradNorm: 38.67 time 0.08\n",
      "Train batch 1743: loss: 17.75 gradNorm: 30.32 time 0.07\n",
      "Train batch 1744: loss: 20.58 gradNorm: 29.79 time 0.09\n",
      "Train batch 1745: loss: 20.15 gradNorm: 28.63 time 0.07\n",
      "Train batch 1746: loss: 30.21 gradNorm: 37.56 time 0.09\n",
      "Train batch 1747: loss: 18.58 gradNorm: 28.90 time 0.08\n",
      "Train batch 1748: loss: 56.44 gradNorm: 52.62 time 0.14\n",
      "Train batch 1749: loss: 24.39 gradNorm: 35.17 time 0.08\n",
      "Train batch 1750: loss: 21.39 gradNorm: 33.60 time 0.08\n",
      "Val batch 1750: CER: 0.26 time 2.44\n",
      "Train batch 1751: loss: 21.19 gradNorm: 36.90 time 0.09\n",
      "Train batch 1752: loss: 24.71 gradNorm: 37.67 time 0.10\n",
      "Train batch 1753: loss: 20.53 gradNorm: 30.06 time 0.07\n",
      "Train batch 1754: loss: 27.05 gradNorm: 36.95 time 0.09\n",
      "Train batch 1755: loss: 20.33 gradNorm: 29.01 time 0.09\n",
      "Train batch 1756: loss: 20.03 gradNorm: 28.69 time 0.09\n",
      "Train batch 1757: loss: 20.51 gradNorm: 30.77 time 0.09\n",
      "Train batch 1758: loss: 23.47 gradNorm: 32.24 time 0.08\n",
      "Train batch 1759: loss: 21.69 gradNorm: 31.33 time 0.09\n",
      "Train batch 1760: loss: 57.33 gradNorm: 54.96 time 0.12\n",
      "Train batch 1761: loss: 24.27 gradNorm: 33.64 time 1.23\n",
      "Train batch 1762: loss: 18.70 gradNorm: 29.82 time 0.08\n",
      "Train batch 1763: loss: 56.76 gradNorm: 56.55 time 0.13\n",
      "Train batch 1764: loss: 60.65 gradNorm: 54.62 time 0.13\n",
      "Train batch 1765: loss: 58.31 gradNorm: 51.11 time 0.13\n",
      "Train batch 1766: loss: 19.95 gradNorm: 35.58 time 0.07\n",
      "Train batch 1767: loss: 18.09 gradNorm: 30.71 time 0.08\n",
      "Train batch 1768: loss: 23.38 gradNorm: 34.95 time 0.10\n",
      "Train batch 1769: loss: 16.82 gradNorm: 31.20 time 0.07\n",
      "Train batch 1770: loss: 57.28 gradNorm: 57.17 time 0.46\n",
      "Train batch 1771: loss: 32.54 gradNorm: 36.61 time 0.09\n",
      "Train batch 1772: loss: 19.71 gradNorm: 28.21 time 0.08\n",
      "Train batch 1773: loss: 19.73 gradNorm: 32.24 time 0.08\n",
      "Train batch 1774: loss: 33.47 gradNorm: 40.30 time 0.08\n",
      "Train batch 1775: loss: 28.18 gradNorm: 38.95 time 0.09\n",
      "Train batch 1776: loss: 22.07 gradNorm: 33.10 time 0.09\n",
      "Train batch 1777: loss: 32.92 gradNorm: 40.85 time 0.09\n",
      "Train batch 1778: loss: 18.70 gradNorm: 32.22 time 0.08\n",
      "Train batch 1779: loss: 23.99 gradNorm: 34.06 time 0.09\n",
      "Train batch 1780: loss: 18.88 gradNorm: 30.17 time 0.10\n",
      "Train batch 1781: loss: 21.50 gradNorm: 32.54 time 0.07\n",
      "Train batch 1782: loss: 18.88 gradNorm: 30.20 time 0.10\n",
      "Train batch 1783: loss: 26.60 gradNorm: 40.25 time 0.09\n",
      "Train batch 1784: loss: 19.13 gradNorm: 31.56 time 0.09\n",
      "Train batch 1785: loss: 22.45 gradNorm: 35.55 time 0.07\n",
      "Train batch 1786: loss: 23.35 gradNorm: 36.42 time 0.09\n",
      "Train batch 1787: loss: 20.38 gradNorm: 34.41 time 0.09\n",
      "Train batch 1788: loss: 21.06 gradNorm: 30.68 time 0.07\n",
      "Train batch 1789: loss: 58.71 gradNorm: 58.01 time 0.13\n",
      "Train batch 1790: loss: 20.50 gradNorm: 35.29 time 0.08\n",
      "Train batch 1791: loss: 20.62 gradNorm: 29.74 time 0.08\n",
      "Train batch 1792: loss: 20.59 gradNorm: 34.39 time 0.08\n",
      "Train batch 1793: loss: 21.49 gradNorm: 32.46 time 0.09\n",
      "Train batch 1794: loss: 19.18 gradNorm: 33.73 time 0.07\n",
      "Train batch 1795: loss: 17.52 gradNorm: 28.36 time 0.08\n",
      "Train batch 1796: loss: 66.57 gradNorm: 59.26 time 0.14\n",
      "Train batch 1797: loss: 20.12 gradNorm: 31.30 time 0.09\n",
      "Train batch 1798: loss: 15.20 gradNorm: 24.41 time 0.08\n",
      "Train batch 1799: loss: 21.75 gradNorm: 30.99 time 0.08\n",
      "Train batch 1800: loss: 17.72 gradNorm: 28.14 time 0.07\n",
      "Val batch 1800: CER: 0.26 time 2.64\n",
      "Train batch 1801: loss: 24.13 gradNorm: 36.64 time 0.09\n",
      "Train batch 1802: loss: 34.99 gradNorm: 43.81 time 0.08\n",
      "Train batch 1803: loss: 17.12 gradNorm: 32.12 time 0.08\n",
      "Train batch 1804: loss: 15.87 gradNorm: 24.65 time 0.10\n",
      "Train batch 1805: loss: 17.32 gradNorm: 28.21 time 0.07\n",
      "Train batch 1806: loss: 22.85 gradNorm: 32.28 time 0.10\n",
      "Train batch 1807: loss: 19.97 gradNorm: 31.69 time 0.08\n",
      "Train batch 1808: loss: 19.20 gradNorm: 31.97 time 0.08\n",
      "Train batch 1809: loss: 19.88 gradNorm: 29.29 time 0.07\n",
      "Train batch 1810: loss: 18.16 gradNorm: 30.18 time 0.07\n",
      "Train batch 1811: loss: 21.48 gradNorm: 33.40 time 0.09\n",
      "Train batch 1812: loss: 32.19 gradNorm: 38.38 time 0.08\n",
      "Train batch 1813: loss: 21.14 gradNorm: 31.09 time 0.08\n",
      "Train batch 1814: loss: 22.25 gradNorm: 39.14 time 0.08\n",
      "Train batch 1815: loss: 33.25 gradNorm: 46.02 time 0.07\n",
      "Train batch 1816: loss: 19.18 gradNorm: 33.36 time 0.09\n",
      "Train batch 1817: loss: 17.71 gradNorm: 28.37 time 0.08\n",
      "Train batch 1818: loss: 19.75 gradNorm: 30.40 time 0.07\n",
      "Train batch 1819: loss: 17.69 gradNorm: 29.91 time 0.08\n",
      "Train batch 1820: loss: 19.12 gradNorm: 30.13 time 0.07\n",
      "Train batch 1821: loss: 21.74 gradNorm: 32.17 time 0.09\n",
      "Train batch 1822: loss: 23.33 gradNorm: 33.77 time 0.08\n",
      "Train batch 1823: loss: 21.56 gradNorm: 34.79 time 0.08\n",
      "Train batch 1824: loss: 21.17 gradNorm: 30.18 time 0.08\n",
      "Train batch 1825: loss: 22.48 gradNorm: 34.95 time 0.08\n",
      "Train batch 1826: loss: 21.67 gradNorm: 32.64 time 0.08\n",
      "Train batch 1827: loss: 18.66 gradNorm: 28.42 time 0.08\n",
      "Train batch 1828: loss: 19.76 gradNorm: 33.58 time 0.07\n",
      "Train batch 1829: loss: 20.56 gradNorm: 34.13 time 0.08\n",
      "Train batch 1830: loss: 17.23 gradNorm: 32.08 time 0.08\n",
      "Train batch 1831: loss: 21.96 gradNorm: 33.63 time 0.08\n",
      "Train batch 1832: loss: 22.41 gradNorm: 35.84 time 0.08\n",
      "Train batch 1833: loss: 20.29 gradNorm: 31.56 time 0.07\n",
      "Train batch 1834: loss: 15.81 gradNorm: 27.63 time 0.07\n",
      "Train batch 1835: loss: 18.26 gradNorm: 28.63 time 0.07\n",
      "Train batch 1836: loss: 21.85 gradNorm: 31.90 time 0.08\n",
      "Train batch 1837: loss: 17.16 gradNorm: 29.24 time 0.08\n",
      "Train batch 1838: loss: 30.50 gradNorm: 43.65 time 0.08\n",
      "Train batch 1839: loss: 16.51 gradNorm: 26.79 time 0.07\n",
      "Train batch 1840: loss: 17.79 gradNorm: 31.58 time 0.07\n",
      "Train batch 1841: loss: 18.45 gradNorm: 27.14 time 0.09\n",
      "Train batch 1842: loss: 30.70 gradNorm: 40.61 time 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 1843: loss: 20.10 gradNorm: 30.04 time 0.07\n",
      "Train batch 1844: loss: 18.79 gradNorm: 31.48 time 0.08\n",
      "Train batch 1845: loss: 23.09 gradNorm: 32.35 time 0.08\n",
      "Train batch 1846: loss: 18.00 gradNorm: 33.52 time 0.08\n",
      "Train batch 1847: loss: 20.19 gradNorm: 35.41 time 0.07\n",
      "Train batch 1848: loss: 39.77 gradNorm: 60.00 time 0.10\n",
      "Train batch 1849: loss: 20.08 gradNorm: 30.39 time 0.08\n",
      "Train batch 1850: loss: 70.63 gradNorm: 73.95 time 0.14\n",
      "Val batch 1850: CER: 0.26 time 2.49\n",
      "Train batch 1851: loss: 63.30 gradNorm: 65.38 time 0.15\n",
      "Train batch 1852: loss: 22.05 gradNorm: 33.33 time 0.09\n",
      "Train batch 1853: loss: 59.21 gradNorm: 55.89 time 0.13\n",
      "Train batch 1854: loss: 25.69 gradNorm: 35.90 time 0.07\n",
      "Train batch 1855: loss: 18.87 gradNorm: 33.89 time 0.08\n",
      "Train batch 1856: loss: 21.01 gradNorm: 31.76 time 0.08\n",
      "Train batch 1857: loss: 17.84 gradNorm: 27.37 time 0.08\n",
      "Train batch 1858: loss: 19.60 gradNorm: 33.01 time 0.08\n",
      "Train batch 1859: loss: 18.51 gradNorm: 33.78 time 0.08\n",
      "Train batch 1860: loss: 17.42 gradNorm: 28.80 time 0.08\n",
      "Train batch 1861: loss: 18.49 gradNorm: 28.15 time 0.09\n",
      "Train batch 1862: loss: 21.48 gradNorm: 30.00 time 0.08\n",
      "Train batch 1863: loss: 63.54 gradNorm: 59.82 time 0.12\n",
      "Train batch 1864: loss: 21.09 gradNorm: 32.03 time 0.08\n",
      "Train batch 1865: loss: 19.14 gradNorm: 30.42 time 0.08\n",
      "Train batch 1866: loss: 39.62 gradNorm: 61.60 time 0.07\n",
      "Train batch 1867: loss: 17.68 gradNorm: 29.84 time 0.08\n",
      "Train batch 1868: loss: 18.20 gradNorm: 29.39 time 0.08\n",
      "Train batch 1869: loss: 16.60 gradNorm: 29.14 time 0.07\n",
      "Train batch 1870: loss: 23.02 gradNorm: 32.39 time 0.09\n",
      "Train batch 1871: loss: 19.15 gradNorm: 31.92 time 0.07\n",
      "Train batch 1872: loss: 18.57 gradNorm: 30.09 time 0.09\n",
      "Train batch 1873: loss: 36.09 gradNorm: 43.60 time 0.11\n",
      "Train batch 1874: loss: 22.03 gradNorm: 28.74 time 0.08\n",
      "Train batch 1875: loss: 20.30 gradNorm: 34.99 time 0.08\n",
      "Train batch 1876: loss: 21.32 gradNorm: 37.43 time 0.07\n",
      "Train batch 1877: loss: 20.08 gradNorm: 36.82 time 0.09\n",
      "Train batch 1878: loss: 21.61 gradNorm: 34.39 time 0.08\n",
      "Train batch 1879: loss: 60.31 gradNorm: 54.61 time 0.12\n",
      "Train batch 1880: loss: 18.98 gradNorm: 33.63 time 0.08\n",
      "Train batch 1881: loss: 17.19 gradNorm: 33.81 time 0.08\n",
      "Train batch 1882: loss: 18.67 gradNorm: 30.59 time 0.10\n",
      "Train batch 1883: loss: 18.67 gradNorm: 35.44 time 0.10\n",
      "Train batch 1884: loss: 18.21 gradNorm: 30.35 time 0.07\n",
      "Train batch 1885: loss: 15.66 gradNorm: 26.48 time 0.07\n",
      "Train batch 1886: loss: 24.46 gradNorm: 37.37 time 0.08\n",
      "Train batch 1887: loss: 15.15 gradNorm: 31.92 time 0.07\n",
      "Train batch 1888: loss: 17.55 gradNorm: 30.11 time 0.09\n",
      "Train batch 1889: loss: 20.98 gradNorm: 34.60 time 0.07\n",
      "Train batch 1890: loss: 27.75 gradNorm: 37.69 time 0.08\n",
      "Train batch 1891: loss: 20.54 gradNorm: 33.79 time 0.09\n",
      "Train batch 1892: loss: 16.83 gradNorm: 30.26 time 0.08\n",
      "Train batch 1893: loss: 34.05 gradNorm: 39.90 time 0.09\n",
      "Train batch 1894: loss: 31.35 gradNorm: 41.09 time 0.08\n",
      "Train batch 1895: loss: 57.84 gradNorm: 55.59 time 0.13\n",
      "Train batch 1896: loss: 27.53 gradNorm: 35.28 time 0.11\n",
      "Train batch 1897: loss: 54.23 gradNorm: 54.93 time 0.12\n",
      "Train batch 1898: loss: 20.78 gradNorm: 33.70 time 0.11\n",
      "Train batch 1899: loss: 22.91 gradNorm: 34.69 time 0.07\n",
      "Train batch 1900: loss: 61.51 gradNorm: 67.92 time 0.13\n",
      "Val batch 1900: CER: 0.26 time 2.43\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1900\n",
      "Train batch 1901: loss: 21.37 gradNorm: 34.37 time 0.10\n",
      "Train batch 1902: loss: 62.07 gradNorm: 59.13 time 0.14\n",
      "Train batch 1903: loss: 21.41 gradNorm: 35.76 time 0.07\n",
      "Train batch 1904: loss: 18.29 gradNorm: 32.12 time 0.08\n",
      "Train batch 1905: loss: 60.55 gradNorm: 60.26 time 0.13\n",
      "Train batch 1906: loss: 20.55 gradNorm: 31.86 time 0.08\n",
      "Train batch 1907: loss: 62.14 gradNorm: 63.96 time 0.14\n",
      "Train batch 1908: loss: 16.72 gradNorm: 28.16 time 0.08\n",
      "Train batch 1909: loss: 20.78 gradNorm: 31.44 time 0.07\n",
      "Train batch 1910: loss: 20.87 gradNorm: 33.17 time 0.09\n",
      "Train batch 1911: loss: 18.08 gradNorm: 30.59 time 0.08\n",
      "Train batch 1912: loss: 20.03 gradNorm: 31.86 time 0.08\n",
      "Train batch 1913: loss: 18.31 gradNorm: 29.31 time 0.08\n",
      "Train batch 1914: loss: 17.45 gradNorm: 27.79 time 0.10\n",
      "Train batch 1915: loss: 18.71 gradNorm: 31.84 time 0.08\n",
      "Train batch 1916: loss: 28.70 gradNorm: 38.84 time 0.08\n",
      "Train batch 1917: loss: 19.51 gradNorm: 31.12 time 0.08\n",
      "Train batch 1918: loss: 19.78 gradNorm: 31.46 time 0.09\n",
      "Train batch 1919: loss: 21.09 gradNorm: 32.70 time 0.08\n",
      "Train batch 1920: loss: 18.83 gradNorm: 28.11 time 0.07\n",
      "Train batch 1921: loss: 20.56 gradNorm: 33.95 time 0.07\n",
      "Train batch 1922: loss: 15.68 gradNorm: 29.61 time 0.07\n",
      "Train batch 1923: loss: 20.69 gradNorm: 35.47 time 0.07\n",
      "Train batch 1924: loss: 15.66 gradNorm: 26.60 time 0.07\n",
      "Train batch 1925: loss: 25.47 gradNorm: 38.29 time 0.09\n",
      "Train batch 1926: loss: 19.41 gradNorm: 30.25 time 0.08\n",
      "Train batch 1927: loss: 30.21 gradNorm: 38.89 time 0.09\n",
      "Train batch 1928: loss: 20.16 gradNorm: 31.62 time 0.07\n",
      "Train batch 1929: loss: 18.37 gradNorm: 27.73 time 0.09\n",
      "Train batch 1930: loss: 19.93 gradNorm: 35.07 time 0.09\n",
      "Train batch 1931: loss: 19.06 gradNorm: 29.27 time 0.09\n",
      "Train batch 1932: loss: 15.40 gradNorm: 26.37 time 0.08\n",
      "Train batch 1933: loss: 19.98 gradNorm: 29.99 time 0.07\n",
      "Train batch 1934: loss: 15.13 gradNorm: 29.88 time 0.09\n",
      "Train batch 1935: loss: 19.40 gradNorm: 32.09 time 0.07\n",
      "Train batch 1936: loss: 29.49 gradNorm: 43.33 time 0.08\n",
      "Train batch 1937: loss: 17.18 gradNorm: 25.17 time 0.09\n",
      "Train batch 1938: loss: 56.01 gradNorm: 51.14 time 0.12\n",
      "Train batch 1939: loss: 24.26 gradNorm: 34.22 time 0.08\n",
      "Train batch 1940: loss: 27.11 gradNorm: 35.41 time 0.07\n",
      "Train batch 1941: loss: 25.33 gradNorm: 35.60 time 0.08\n",
      "Train batch 1942: loss: 56.85 gradNorm: 51.15 time 0.12\n",
      "Train batch 1943: loss: 16.88 gradNorm: 27.99 time 0.08\n",
      "Train batch 1944: loss: 18.05 gradNorm: 31.06 time 0.08\n",
      "Train batch 1945: loss: 18.04 gradNorm: 32.63 time 0.08\n",
      "Train batch 1946: loss: 19.63 gradNorm: 37.33 time 0.07\n",
      "Train batch 1947: loss: 20.41 gradNorm: 31.11 time 0.08\n",
      "Train batch 1948: loss: 22.15 gradNorm: 39.21 time 0.09\n",
      "Train batch 1949: loss: 18.55 gradNorm: 39.62 time 0.08\n",
      "Train batch 1950: loss: 68.50 gradNorm: 70.34 time 0.14\n",
      "Val batch 1950: CER: 0.25 time 2.47\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-1950\n",
      "Train batch 1951: loss: 31.98 gradNorm: 40.20 time 0.09\n",
      "Train batch 1952: loss: 37.51 gradNorm: 51.49 time 0.08\n",
      "Train batch 1953: loss: 18.49 gradNorm: 32.34 time 0.08\n",
      "Train batch 1954: loss: 21.80 gradNorm: 31.61 time 0.08\n",
      "Train batch 1955: loss: 39.22 gradNorm: 70.71 time 0.08\n",
      "Train batch 1956: loss: 14.85 gradNorm: 27.06 time 0.09\n",
      "Train batch 1957: loss: 18.18 gradNorm: 28.50 time 0.09\n",
      "Train batch 1958: loss: 30.44 gradNorm: 46.10 time 0.10\n",
      "Train batch 1959: loss: 55.92 gradNorm: 55.44 time 0.14\n",
      "Train batch 1960: loss: 17.83 gradNorm: 28.97 time 0.07\n",
      "Train batch 1961: loss: 36.51 gradNorm: 44.13 time 0.09\n",
      "Train batch 1962: loss: 16.58 gradNorm: 25.82 time 0.07\n",
      "Train batch 1963: loss: 20.61 gradNorm: 31.38 time 0.07\n",
      "Train batch 1964: loss: 19.77 gradNorm: 36.20 time 0.07\n",
      "Train batch 1965: loss: 18.85 gradNorm: 32.64 time 0.08\n",
      "Train batch 1966: loss: 18.41 gradNorm: 28.24 time 0.08\n",
      "Train batch 1967: loss: 60.93 gradNorm: 52.15 time 0.13\n",
      "Train batch 1968: loss: 19.52 gradNorm: 30.61 time 0.08\n",
      "Train batch 1969: loss: 18.38 gradNorm: 34.40 time 0.08\n",
      "Train batch 1970: loss: 17.89 gradNorm: 30.50 time 0.08\n",
      "Train batch 1971: loss: 20.66 gradNorm: 31.93 time 0.08\n",
      "Train batch 1972: loss: 18.27 gradNorm: 32.81 time 0.08\n",
      "Train batch 1973: loss: 27.47 gradNorm: 40.63 time 0.08\n",
      "Train batch 1974: loss: 18.97 gradNorm: 31.94 time 0.09\n",
      "Train batch 1975: loss: 19.18 gradNorm: 32.03 time 0.09\n",
      "Train batch 1976: loss: 16.17 gradNorm: 27.06 time 0.07\n",
      "Train batch 1977: loss: 20.66 gradNorm: 31.08 time 0.08\n",
      "Train batch 1978: loss: 27.06 gradNorm: 34.22 time 0.07\n",
      "Train batch 1979: loss: 21.98 gradNorm: 34.39 time 0.08\n",
      "Train batch 1980: loss: 56.18 gradNorm: 53.98 time 0.13\n",
      "Train batch 1981: loss: 18.83 gradNorm: 29.79 time 0.09\n",
      "Train batch 1982: loss: 14.98 gradNorm: 27.31 time 0.08\n",
      "Train batch 1983: loss: 31.73 gradNorm: 43.37 time 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 1984: loss: 20.75 gradNorm: 37.08 time 0.07\n",
      "Train batch 1985: loss: 16.33 gradNorm: 28.23 time 0.08\n",
      "Train batch 1986: loss: 19.87 gradNorm: 31.13 time 0.09\n",
      "Train batch 1987: loss: 21.33 gradNorm: 35.32 time 0.08\n",
      "Train batch 1988: loss: 19.42 gradNorm: 30.12 time 0.08\n",
      "Train batch 1989: loss: 17.35 gradNorm: 31.94 time 0.07\n",
      "Train batch 1990: loss: 53.84 gradNorm: 57.43 time 0.13\n",
      "Train batch 1991: loss: 21.15 gradNorm: 33.68 time 0.08\n",
      "Train batch 1992: loss: 33.20 gradNorm: 38.46 time 0.11\n",
      "Train batch 1993: loss: 13.99 gradNorm: 27.51 time 0.07\n",
      "Train batch 1994: loss: 18.83 gradNorm: 30.44 time 0.08\n",
      "Train batch 1995: loss: 22.21 gradNorm: 34.81 time 0.08\n",
      "Train batch 1996: loss: 21.56 gradNorm: 34.60 time 0.08\n",
      "Train batch 1997: loss: 20.20 gradNorm: 36.38 time 0.07\n",
      "Train batch 1998: loss: 14.22 gradNorm: 27.11 time 0.07\n",
      "Train batch 1999: loss: 19.77 gradNorm: 31.92 time 0.07\n",
      "Train batch 2000: loss: 54.47 gradNorm: 49.36 time 0.12\n",
      "Val batch 2000: CER: 0.25 time 2.47\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-2000\n",
      "Train batch 2001: loss: 21.31 gradNorm: 29.81 time 0.09\n",
      "Train batch 2002: loss: 17.56 gradNorm: 29.98 time 0.08\n",
      "Train batch 2003: loss: 17.65 gradNorm: 29.84 time 0.08\n",
      "Train batch 2004: loss: 18.16 gradNorm: 28.43 time 0.08\n",
      "Train batch 2005: loss: 19.68 gradNorm: 33.77 time 0.08\n",
      "Train batch 2006: loss: 19.71 gradNorm: 31.38 time 0.08\n",
      "Train batch 2007: loss: 16.80 gradNorm: 28.19 time 0.07\n",
      "Train batch 2008: loss: 34.47 gradNorm: 43.25 time 0.08\n",
      "Train batch 2009: loss: 17.24 gradNorm: 31.99 time 0.08\n",
      "Train batch 2010: loss: 33.34 gradNorm: 39.27 time 0.07\n",
      "Train batch 2011: loss: 19.50 gradNorm: 32.83 time 0.08\n",
      "Train batch 2012: loss: 18.53 gradNorm: 30.23 time 0.08\n",
      "Train batch 2013: loss: 23.67 gradNorm: 43.49 time 0.08\n",
      "Train batch 2014: loss: 19.21 gradNorm: 30.62 time 0.09\n",
      "Train batch 2015: loss: 63.84 gradNorm: 64.47 time 0.14\n",
      "Train batch 2016: loss: 21.24 gradNorm: 34.77 time 0.07\n",
      "Train batch 2017: loss: 27.53 gradNorm: 40.15 time 0.08\n",
      "Train batch 2018: loss: 17.66 gradNorm: 33.42 time 0.08\n",
      "Train batch 2019: loss: 23.76 gradNorm: 40.55 time 0.08\n",
      "Train batch 2020: loss: 21.77 gradNorm: 42.17 time 0.08\n",
      "Train batch 2021: loss: 19.61 gradNorm: 32.11 time 0.08\n",
      "Train batch 2022: loss: 20.79 gradNorm: 30.96 time 0.07\n",
      "Train batch 2023: loss: 60.70 gradNorm: 63.18 time 0.14\n",
      "Train batch 2024: loss: 19.72 gradNorm: 35.56 time 0.07\n",
      "Train batch 2025: loss: 31.77 gradNorm: 47.80 time 0.09\n",
      "Train batch 2026: loss: 17.10 gradNorm: 34.00 time 0.07\n",
      "Train batch 2027: loss: 17.43 gradNorm: 29.42 time 0.08\n",
      "Train batch 2028: loss: 16.37 gradNorm: 25.64 time 0.07\n",
      "Train batch 2029: loss: 19.52 gradNorm: 30.55 time 0.08\n",
      "Train batch 2030: loss: 21.58 gradNorm: 35.47 time 0.08\n",
      "Train batch 2031: loss: 19.73 gradNorm: 31.49 time 0.10\n",
      "Train batch 2032: loss: 17.76 gradNorm: 27.80 time 0.07\n",
      "Train batch 2033: loss: 17.36 gradNorm: 27.27 time 0.08\n",
      "Train batch 2034: loss: 18.39 gradNorm: 34.55 time 0.06\n",
      "Train batch 2035: loss: 32.09 gradNorm: 39.17 time 0.08\n",
      "Train batch 2036: loss: 17.08 gradNorm: 28.43 time 0.08\n",
      "Train batch 2037: loss: 58.28 gradNorm: 52.57 time 0.12\n",
      "Train batch 2038: loss: 17.72 gradNorm: 31.28 time 0.09\n",
      "Train batch 2039: loss: 21.09 gradNorm: 33.28 time 0.07\n",
      "Train batch 2040: loss: 16.48 gradNorm: 26.51 time 0.09\n",
      "Train batch 2041: loss: 18.46 gradNorm: 33.08 time 0.09\n",
      "Train batch 2042: loss: 18.07 gradNorm: 28.86 time 0.08\n",
      "Train batch 2043: loss: 27.74 gradNorm: 35.51 time 0.09\n",
      "Train batch 2044: loss: 20.56 gradNorm: 31.87 time 0.09\n",
      "Train batch 2045: loss: 30.92 gradNorm: 41.92 time 0.09\n",
      "Train batch 2046: loss: 18.08 gradNorm: 29.93 time 0.08\n",
      "Train batch 2047: loss: 14.53 gradNorm: 26.56 time 0.07\n",
      "Train batch 2048: loss: 26.80 gradNorm: 38.99 time 0.08\n",
      "Train batch 2049: loss: 20.44 gradNorm: 35.47 time 0.08\n",
      "Train batch 2050: loss: 16.88 gradNorm: 31.88 time 0.07\n",
      "Val batch 2050: CER: 0.26 time 7.62\n",
      "Train batch 2051: loss: 58.71 gradNorm: 60.48 time 0.14\n",
      "Train batch 2052: loss: 19.62 gradNorm: 30.09 time 0.08\n",
      "Train batch 2053: loss: 19.46 gradNorm: 34.40 time 0.08\n",
      "Train batch 2054: loss: 19.59 gradNorm: 30.04 time 0.07\n",
      "Train batch 2055: loss: 18.13 gradNorm: 30.66 time 0.09\n",
      "Train batch 2056: loss: 25.61 gradNorm: 41.37 time 0.07\n",
      "Train batch 2057: loss: 24.10 gradNorm: 43.35 time 0.09\n",
      "Train batch 2058: loss: 16.62 gradNorm: 30.12 time 0.07\n",
      "Train batch 2059: loss: 19.72 gradNorm: 34.85 time 0.08\n",
      "Train batch 2060: loss: 60.81 gradNorm: 58.63 time 0.14\n",
      "Train batch 2061: loss: 16.56 gradNorm: 26.85 time 0.07\n",
      "Train batch 2062: loss: 17.55 gradNorm: 29.24 time 0.07\n",
      "Train batch 2063: loss: 30.87 gradNorm: 39.93 time 0.08\n",
      "Train batch 2064: loss: 17.54 gradNorm: 33.32 time 0.08\n",
      "Train batch 2065: loss: 19.89 gradNorm: 34.26 time 0.09\n",
      "Train batch 2066: loss: 17.72 gradNorm: 32.27 time 0.09\n",
      "Train batch 2067: loss: 15.80 gradNorm: 26.94 time 0.10\n",
      "Train batch 2068: loss: 16.48 gradNorm: 30.54 time 0.08\n",
      "Train batch 2069: loss: 21.23 gradNorm: 34.73 time 0.08\n",
      "Train batch 2070: loss: 30.20 gradNorm: 38.21 time 0.11\n",
      "Train batch 2071: loss: 17.00 gradNorm: 29.49 time 0.08\n",
      "Train batch 2072: loss: 26.51 gradNorm: 38.40 time 0.09\n",
      "Train batch 2073: loss: 18.51 gradNorm: 32.09 time 0.08\n",
      "Train batch 2074: loss: 17.29 gradNorm: 30.91 time 0.10\n",
      "Train batch 2075: loss: 17.26 gradNorm: 30.06 time 0.08\n",
      "Train batch 2076: loss: 18.36 gradNorm: 31.33 time 0.08\n",
      "Train batch 2077: loss: 32.20 gradNorm: 42.88 time 0.08\n",
      "Train batch 2078: loss: 19.64 gradNorm: 31.33 time 0.07\n",
      "Train batch 2079: loss: 24.84 gradNorm: 35.26 time 0.09\n",
      "Train batch 2080: loss: 61.83 gradNorm: 59.38 time 0.13\n",
      "Train batch 2081: loss: 19.46 gradNorm: 31.42 time 0.08\n",
      "Train batch 2082: loss: 63.39 gradNorm: 61.16 time 0.14\n",
      "Train batch 2083: loss: 26.04 gradNorm: 34.53 time 0.08\n",
      "Train batch 2084: loss: 30.14 gradNorm: 42.52 time 0.09\n",
      "Train batch 2085: loss: 60.60 gradNorm: 56.64 time 0.14\n",
      "Train batch 2086: loss: 18.98 gradNorm: 27.13 time 0.08\n",
      "Train batch 2087: loss: 19.54 gradNorm: 31.74 time 0.08\n",
      "Train batch 2088: loss: 29.93 gradNorm: 42.44 time 0.08\n",
      "Train batch 2089: loss: 53.90 gradNorm: 54.80 time 0.12\n",
      "Train batch 2090: loss: 59.60 gradNorm: 59.43 time 0.14\n",
      "Train batch 2091: loss: 17.72 gradNorm: 32.58 time 0.09\n",
      "Train batch 2092: loss: 15.37 gradNorm: 32.06 time 0.07\n",
      "Train batch 2093: loss: 56.94 gradNorm: 61.11 time 0.13\n",
      "Train batch 2094: loss: 56.77 gradNorm: 67.21 time 0.12\n",
      "Train batch 2095: loss: 54.54 gradNorm: 56.21 time 0.13\n",
      "Train batch 2096: loss: 16.19 gradNorm: 26.87 time 0.08\n",
      "Train batch 2097: loss: 49.41 gradNorm: 62.80 time 0.13\n",
      "Train batch 2098: loss: 16.18 gradNorm: 29.39 time 0.07\n",
      "Train batch 2099: loss: 18.55 gradNorm: 29.89 time 0.09\n",
      "Train batch 2100: loss: 33.64 gradNorm: 40.99 time 0.07\n",
      "Val batch 2100: CER: 0.24 time 2.83\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-2100\n",
      "Train batch 2101: loss: 58.31 gradNorm: 60.16 time 0.15\n",
      "Train batch 2102: loss: 25.47 gradNorm: 38.34 time 0.08\n",
      "Train batch 2103: loss: 14.23 gradNorm: 28.56 time 0.08\n",
      "Train batch 2104: loss: 20.38 gradNorm: 31.11 time 0.09\n",
      "Train batch 2105: loss: 18.20 gradNorm: 27.06 time 0.07\n",
      "Train batch 2106: loss: 14.91 gradNorm: 27.59 time 0.07\n",
      "Train batch 2107: loss: 14.38 gradNorm: 27.32 time 0.07\n",
      "Train batch 2108: loss: 28.10 gradNorm: 38.18 time 0.09\n",
      "Train batch 2109: loss: 30.59 gradNorm: 40.97 time 0.08\n",
      "Train batch 2110: loss: 18.16 gradNorm: 30.83 time 0.08\n",
      "Train batch 2111: loss: 17.73 gradNorm: 31.97 time 0.10\n",
      "Train batch 2112: loss: 18.12 gradNorm: 34.30 time 0.08\n",
      "Train batch 2113: loss: 17.31 gradNorm: 34.16 time 0.07\n",
      "Train batch 2114: loss: 21.21 gradNorm: 35.41 time 0.08\n",
      "Train batch 2115: loss: 19.12 gradNorm: 36.19 time 0.07\n",
      "Train batch 2116: loss: 24.58 gradNorm: 36.64 time 0.08\n",
      "Train batch 2117: loss: 17.93 gradNorm: 30.87 time 0.10\n",
      "Train batch 2118: loss: 20.15 gradNorm: 34.69 time 0.07\n",
      "Train batch 2119: loss: 58.67 gradNorm: 57.88 time 0.15\n",
      "Train batch 2120: loss: 30.78 gradNorm: 41.60 time 0.08\n",
      "Train batch 2121: loss: 14.41 gradNorm: 27.35 time 0.07\n",
      "Train batch 2122: loss: 19.21 gradNorm: 34.74 time 0.07\n",
      "Train batch 2123: loss: 17.84 gradNorm: 36.11 time 0.09\n",
      "Train batch 2124: loss: 20.30 gradNorm: 36.08 time 0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 2125: loss: 21.24 gradNorm: 33.76 time 0.09\n",
      "Train batch 2126: loss: 16.43 gradNorm: 29.34 time 0.08\n",
      "Train batch 2127: loss: 25.57 gradNorm: 35.70 time 0.08\n",
      "Train batch 2128: loss: 18.86 gradNorm: 29.77 time 0.08\n",
      "Train batch 2129: loss: 17.82 gradNorm: 30.70 time 0.08\n",
      "Train batch 2130: loss: 16.85 gradNorm: 35.46 time 0.07\n",
      "Train batch 2131: loss: 56.02 gradNorm: 56.55 time 0.14\n",
      "Train batch 2132: loss: 16.32 gradNorm: 32.52 time 0.07\n",
      "Train batch 2133: loss: 21.90 gradNorm: 35.36 time 0.11\n",
      "Train batch 2134: loss: 35.34 gradNorm: 67.33 time 0.08\n",
      "Train batch 2135: loss: 17.64 gradNorm: 36.09 time 0.09\n",
      "Train batch 2136: loss: 17.61 gradNorm: 32.20 time 0.08\n",
      "Train batch 2137: loss: 60.26 gradNorm: 63.93 time 0.15\n",
      "Train batch 2138: loss: 20.08 gradNorm: 31.21 time 0.07\n",
      "Train batch 2139: loss: 17.84 gradNorm: 37.52 time 0.07\n",
      "Train batch 2140: loss: 21.90 gradNorm: 36.15 time 0.09\n",
      "Train batch 2141: loss: 16.88 gradNorm: 31.63 time 0.08\n",
      "Train batch 2142: loss: 17.29 gradNorm: 27.87 time 0.08\n",
      "Train batch 2143: loss: 25.49 gradNorm: 38.94 time 0.08\n",
      "Train batch 2144: loss: 30.56 gradNorm: 43.57 time 0.08\n",
      "Train batch 2145: loss: 18.74 gradNorm: 31.99 time 0.09\n",
      "Train batch 2146: loss: 17.45 gradNorm: 30.95 time 0.08\n",
      "Train batch 2147: loss: 18.11 gradNorm: 29.18 time 0.08\n",
      "Train batch 2148: loss: 31.77 gradNorm: 47.73 time 0.07\n",
      "Train batch 2149: loss: 16.28 gradNorm: 31.93 time 0.07\n",
      "Train batch 2150: loss: 25.80 gradNorm: 38.05 time 0.08\n",
      "Val batch 2150: CER: 0.25 time 2.43\n",
      "Train batch 2151: loss: 29.13 gradNorm: 35.33 time 0.09\n",
      "Train batch 2152: loss: 21.96 gradNorm: 53.71 time 0.12\n",
      "Train batch 2153: loss: 17.61 gradNorm: 30.14 time 0.09\n",
      "Train batch 2154: loss: 16.57 gradNorm: 29.40 time 0.08\n",
      "Train batch 2155: loss: 21.63 gradNorm: 36.73 time 0.09\n",
      "Train batch 2156: loss: 29.06 gradNorm: 37.99 time 0.08\n",
      "Train batch 2157: loss: 16.36 gradNorm: 33.75 time 0.07\n",
      "Train batch 2158: loss: 19.32 gradNorm: 40.40 time 0.08\n",
      "Train batch 2159: loss: 33.95 gradNorm: 60.38 time 0.07\n",
      "Train batch 2160: loss: 15.34 gradNorm: 28.25 time 0.08\n",
      "Train batch 2161: loss: 18.36 gradNorm: 29.93 time 0.08\n",
      "Train batch 2162: loss: 19.12 gradNorm: 31.85 time 0.08\n",
      "Train batch 2163: loss: 27.60 gradNorm: 47.23 time 0.09\n",
      "Train batch 2164: loss: 19.47 gradNorm: 33.37 time 0.09\n",
      "Train batch 2165: loss: 21.20 gradNorm: 31.52 time 0.07\n",
      "Train batch 2166: loss: 16.63 gradNorm: 34.08 time 0.09\n",
      "Train batch 2167: loss: 24.71 gradNorm: 36.72 time 0.07\n",
      "Train batch 2168: loss: 18.92 gradNorm: 32.55 time 0.09\n",
      "Train batch 2169: loss: 17.75 gradNorm: 29.77 time 0.09\n",
      "Train batch 2170: loss: 35.41 gradNorm: 48.65 time 0.09\n",
      "Train batch 2171: loss: 20.20 gradNorm: 35.91 time 0.07\n",
      "Train batch 2172: loss: 16.02 gradNorm: 29.66 time 0.07\n",
      "Train batch 2173: loss: 15.50 gradNorm: 28.64 time 0.07\n",
      "Train batch 2174: loss: 17.34 gradNorm: 31.06 time 0.09\n",
      "Train batch 2175: loss: 56.86 gradNorm: 60.22 time 0.13\n",
      "Train batch 2176: loss: 14.87 gradNorm: 29.78 time 0.08\n",
      "Train batch 2177: loss: 24.10 gradNorm: 43.32 time 0.12\n",
      "Train batch 2178: loss: 17.36 gradNorm: 28.80 time 0.09\n",
      "Train batch 2179: loss: 21.10 gradNorm: 34.19 time 0.08\n",
      "Train batch 2180: loss: 15.30 gradNorm: 30.10 time 0.07\n",
      "Train batch 2181: loss: 19.78 gradNorm: 31.44 time 0.07\n",
      "Train batch 2182: loss: 16.55 gradNorm: 31.63 time 0.08\n",
      "Train batch 2183: loss: 17.24 gradNorm: 33.63 time 0.07\n",
      "Train batch 2184: loss: 19.94 gradNorm: 31.01 time 0.07\n",
      "Train batch 2185: loss: 18.33 gradNorm: 33.69 time 0.09\n",
      "Train batch 2186: loss: 53.49 gradNorm: 52.06 time 0.14\n",
      "Train batch 2187: loss: 18.30 gradNorm: 36.44 time 0.08\n",
      "Train batch 2188: loss: 18.61 gradNorm: 30.82 time 0.08\n",
      "Train batch 2189: loss: 16.19 gradNorm: 28.98 time 0.09\n",
      "Train batch 2190: loss: 18.52 gradNorm: 32.01 time 0.09\n",
      "Train batch 2191: loss: 19.08 gradNorm: 33.00 time 0.08\n",
      "Train batch 2192: loss: 18.11 gradNorm: 31.94 time 0.07\n",
      "Train batch 2193: loss: 17.52 gradNorm: 31.53 time 0.07\n",
      "Train batch 2194: loss: 16.32 gradNorm: 32.91 time 0.08\n",
      "Train batch 2195: loss: 23.63 gradNorm: 35.04 time 0.08\n",
      "Train batch 2196: loss: 17.60 gradNorm: 34.05 time 0.08\n",
      "Train batch 2197: loss: 19.10 gradNorm: 35.01 time 0.09\n",
      "Train batch 2198: loss: 18.41 gradNorm: 31.11 time 0.09\n",
      "Train batch 2199: loss: 18.20 gradNorm: 30.27 time 0.07\n",
      "Train batch 2200: loss: 14.70 gradNorm: 28.50 time 0.07\n",
      "Val batch 2200: CER: 0.25 time 3.75\n",
      "Train batch 2201: loss: 18.01 gradNorm: 34.60 time 0.09\n",
      "Train batch 2202: loss: 20.63 gradNorm: 38.66 time 0.11\n",
      "Train batch 2203: loss: 22.29 gradNorm: 37.31 time 0.11\n",
      "Train batch 2204: loss: 13.76 gradNorm: 28.56 time 0.08\n",
      "Train batch 2205: loss: 23.20 gradNorm: 36.72 time 0.08\n",
      "Train batch 2206: loss: 17.02 gradNorm: 31.23 time 0.07\n",
      "Train batch 2207: loss: 19.08 gradNorm: 31.52 time 0.08\n",
      "Train batch 2208: loss: 18.52 gradNorm: 33.36 time 0.07\n",
      "Train batch 2209: loss: 16.49 gradNorm: 30.04 time 0.08\n",
      "Train batch 2210: loss: 22.36 gradNorm: 37.78 time 0.08\n",
      "Train batch 2211: loss: 16.86 gradNorm: 36.46 time 0.11\n",
      "Train batch 2212: loss: 20.25 gradNorm: 36.47 time 0.09\n",
      "Train batch 2213: loss: 18.89 gradNorm: 32.67 time 0.10\n",
      "Train batch 2214: loss: 13.49 gradNorm: 26.31 time 0.08\n",
      "Train batch 2215: loss: 14.50 gradNorm: 28.91 time 0.07\n",
      "Train batch 2216: loss: 16.70 gradNorm: 27.69 time 0.07\n",
      "Train batch 2217: loss: 29.20 gradNorm: 38.76 time 0.09\n",
      "Train batch 2218: loss: 20.25 gradNorm: 34.94 time 0.08\n",
      "Train batch 2219: loss: 27.26 gradNorm: 38.13 time 0.09\n",
      "Train batch 2220: loss: 57.97 gradNorm: 56.79 time 0.13\n",
      "Train batch 2221: loss: 16.29 gradNorm: 26.74 time 0.10\n",
      "Train batch 2222: loss: 15.86 gradNorm: 27.79 time 0.08\n",
      "Train batch 2223: loss: 21.94 gradNorm: 34.29 time 0.08\n",
      "Train batch 2224: loss: 15.52 gradNorm: 30.47 time 0.09\n",
      "Train batch 2225: loss: 16.59 gradNorm: 29.74 time 0.08\n",
      "Train batch 2226: loss: 14.85 gradNorm: 29.86 time 0.08\n",
      "Train batch 2227: loss: 16.26 gradNorm: 26.40 time 0.09\n",
      "Train batch 2228: loss: 52.98 gradNorm: 56.76 time 0.15\n",
      "Train batch 2229: loss: 27.49 gradNorm: 34.87 time 0.11\n",
      "Train batch 2230: loss: 28.23 gradNorm: 37.52 time 0.09\n",
      "Train batch 2231: loss: 17.22 gradNorm: 31.55 time 0.08\n",
      "Train batch 2232: loss: 19.12 gradNorm: 33.67 time 0.09\n",
      "Train batch 2233: loss: 17.96 gradNorm: 36.55 time 0.09\n",
      "Train batch 2234: loss: 19.30 gradNorm: 32.70 time 0.07\n",
      "Train batch 2235: loss: 16.57 gradNorm: 31.57 time 0.09\n",
      "Train batch 2236: loss: 16.90 gradNorm: 32.17 time 0.08\n",
      "Train batch 2237: loss: 18.38 gradNorm: 34.51 time 0.08\n",
      "Train batch 2238: loss: 17.22 gradNorm: 39.62 time 0.08\n",
      "Train batch 2239: loss: 14.25 gradNorm: 37.72 time 0.08\n",
      "Train batch 2240: loss: 19.62 gradNorm: 37.19 time 0.08\n",
      "Train batch 2241: loss: 20.45 gradNorm: 34.13 time 0.09\n",
      "Train batch 2242: loss: 30.93 gradNorm: 41.82 time 0.08\n",
      "Train batch 2243: loss: 17.71 gradNorm: 31.47 time 0.08\n",
      "Train batch 2244: loss: 17.88 gradNorm: 33.48 time 0.07\n",
      "Train batch 2245: loss: 17.90 gradNorm: 32.39 time 0.08\n",
      "Train batch 2246: loss: 54.96 gradNorm: 57.41 time 0.14\n",
      "Train batch 2247: loss: 22.80 gradNorm: 39.20 time 0.08\n",
      "Train batch 2248: loss: 15.32 gradNorm: 32.72 time 0.09\n",
      "Train batch 2249: loss: 20.95 gradNorm: 39.08 time 0.07\n",
      "Train batch 2250: loss: 14.07 gradNorm: 25.51 time 0.07\n",
      "Val batch 2250: CER: 0.25 time 2.72\n",
      "Train batch 2251: loss: 18.32 gradNorm: 29.11 time 0.08\n",
      "Train batch 2252: loss: 16.10 gradNorm: 31.29 time 0.08\n",
      "Train batch 2253: loss: 23.39 gradNorm: 35.61 time 0.07\n",
      "Train batch 2254: loss: 18.78 gradNorm: 30.44 time 0.08\n",
      "Train batch 2255: loss: 17.67 gradNorm: 28.46 time 0.08\n",
      "Train batch 2256: loss: 17.57 gradNorm: 29.74 time 0.08\n",
      "Train batch 2257: loss: 13.39 gradNorm: 26.56 time 0.07\n",
      "Train batch 2258: loss: 14.52 gradNorm: 29.03 time 0.08\n",
      "Train batch 2259: loss: 27.50 gradNorm: 38.34 time 0.08\n",
      "Train batch 2260: loss: 14.95 gradNorm: 32.55 time 0.08\n",
      "Train batch 2261: loss: 15.75 gradNorm: 28.67 time 0.08\n",
      "Train batch 2262: loss: 53.48 gradNorm: 58.94 time 0.14\n",
      "Train batch 2263: loss: 56.06 gradNorm: 62.79 time 0.13\n",
      "Train batch 2264: loss: 13.29 gradNorm: 26.03 time 0.07\n",
      "Train batch 2265: loss: 19.93 gradNorm: 34.16 time 0.09\n",
      "Train batch 2266: loss: 18.88 gradNorm: 32.11 time 0.07\n",
      "Train batch 2267: loss: 16.03 gradNorm: 32.82 time 0.08\n",
      "Train batch 2268: loss: 19.00 gradNorm: 31.52 time 0.07\n",
      "Train batch 2269: loss: 15.13 gradNorm: 30.98 time 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 2270: loss: 16.79 gradNorm: 28.78 time 0.09\n",
      "Train batch 2271: loss: 28.42 gradNorm: 37.33 time 0.10\n",
      "Train batch 2272: loss: 29.52 gradNorm: 37.05 time 0.35\n",
      "Train batch 2273: loss: 16.95 gradNorm: 30.71 time 0.07\n",
      "Train batch 2274: loss: 31.47 gradNorm: 38.99 time 0.11\n",
      "Train batch 2275: loss: 26.26 gradNorm: 36.86 time 0.08\n",
      "Train batch 2276: loss: 16.98 gradNorm: 26.90 time 0.08\n",
      "Train batch 2277: loss: 26.44 gradNorm: 37.86 time 0.08\n",
      "Train batch 2278: loss: 28.87 gradNorm: 37.09 time 0.08\n",
      "Train batch 2279: loss: 16.90 gradNorm: 29.24 time 0.08\n",
      "Train batch 2280: loss: 16.13 gradNorm: 30.22 time 0.07\n",
      "Train batch 2281: loss: 55.37 gradNorm: 58.76 time 0.13\n",
      "Train batch 2282: loss: 29.10 gradNorm: 43.13 time 0.07\n",
      "Train batch 2283: loss: 27.51 gradNorm: 33.39 time 0.06\n",
      "Train batch 2284: loss: 25.19 gradNorm: 39.32 time 0.09\n",
      "Train batch 2285: loss: 19.16 gradNorm: 35.07 time 0.07\n",
      "Train batch 2286: loss: 58.85 gradNorm: 72.09 time 0.14\n",
      "Train batch 2287: loss: 24.93 gradNorm: 38.43 time 0.08\n",
      "Train batch 2288: loss: 22.56 gradNorm: 35.43 time 0.09\n",
      "Train batch 2289: loss: 16.32 gradNorm: 32.30 time 0.08\n",
      "Train batch 2290: loss: 30.47 gradNorm: 39.66 time 0.08\n",
      "Train batch 2291: loss: 61.06 gradNorm: 61.08 time 0.14\n",
      "Train batch 2292: loss: 19.06 gradNorm: 31.60 time 0.09\n",
      "Train batch 2293: loss: 24.69 gradNorm: 37.33 time 0.08\n",
      "Train batch 2294: loss: 36.22 gradNorm: 51.50 time 0.08\n",
      "Train batch 2295: loss: 11.89 gradNorm: 24.97 time 0.07\n",
      "Train batch 2296: loss: 18.05 gradNorm: 30.14 time 0.07\n",
      "Train batch 2297: loss: 14.73 gradNorm: 27.79 time 0.07\n",
      "Train batch 2298: loss: 21.97 gradNorm: 33.42 time 0.08\n",
      "Train batch 2299: loss: 14.11 gradNorm: 28.37 time 0.09\n",
      "Train batch 2300: loss: 54.45 gradNorm: 57.19 time 0.14\n",
      "Val batch 2300: CER: 0.24 time 2.43\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-2300\n",
      "Train batch 2301: loss: 17.58 gradNorm: 31.04 time 0.08\n",
      "Train batch 2302: loss: 17.00 gradNorm: 30.96 time 0.08\n",
      "Train batch 2303: loss: 18.73 gradNorm: 30.45 time 0.08\n",
      "Train batch 2304: loss: 29.38 gradNorm: 42.77 time 0.07\n",
      "Train batch 2305: loss: 18.61 gradNorm: 30.28 time 0.08\n",
      "Train batch 2306: loss: 27.15 gradNorm: 37.33 time 0.08\n",
      "Train batch 2307: loss: 15.42 gradNorm: 27.19 time 0.07\n",
      "Train batch 2308: loss: 15.94 gradNorm: 31.09 time 0.08\n",
      "Train batch 2309: loss: 18.14 gradNorm: 32.46 time 0.09\n",
      "Train batch 2310: loss: 22.37 gradNorm: 34.25 time 0.08\n",
      "Train batch 2311: loss: 17.51 gradNorm: 29.26 time 0.08\n",
      "Train batch 2312: loss: 27.85 gradNorm: 38.88 time 0.08\n",
      "Train batch 2313: loss: 34.45 gradNorm: 52.76 time 0.10\n",
      "Train batch 2314: loss: 31.21 gradNorm: 37.49 time 0.11\n",
      "Train batch 2315: loss: 17.10 gradNorm: 33.45 time 0.08\n",
      "Train batch 2316: loss: 55.15 gradNorm: 58.19 time 0.14\n",
      "Train batch 2317: loss: 17.06 gradNorm: 28.38 time 0.08\n",
      "Train batch 2318: loss: 17.97 gradNorm: 32.46 time 0.08\n",
      "Train batch 2319: loss: 15.72 gradNorm: 31.07 time 0.08\n",
      "Train batch 2320: loss: 14.42 gradNorm: 29.17 time 0.08\n",
      "Train batch 2321: loss: 14.24 gradNorm: 27.43 time 0.07\n",
      "Train batch 2322: loss: 16.01 gradNorm: 28.58 time 0.10\n",
      "Train batch 2323: loss: 16.93 gradNorm: 30.76 time 0.08\n",
      "Train batch 2324: loss: 16.29 gradNorm: 31.72 time 0.08\n",
      "Train batch 2325: loss: 13.43 gradNorm: 35.83 time 0.08\n",
      "Train batch 2326: loss: 22.08 gradNorm: 34.29 time 0.07\n",
      "Train batch 2327: loss: 13.44 gradNorm: 27.49 time 0.07\n",
      "Train batch 2328: loss: 17.73 gradNorm: 33.38 time 0.07\n",
      "Train batch 2329: loss: 17.24 gradNorm: 31.78 time 0.08\n",
      "Train batch 2330: loss: 15.99 gradNorm: 27.91 time 0.07\n",
      "Train batch 2331: loss: 15.14 gradNorm: 30.12 time 0.07\n",
      "Train batch 2332: loss: 15.97 gradNorm: 28.50 time 0.10\n",
      "Train batch 2333: loss: 16.73 gradNorm: 28.55 time 0.08\n",
      "Train batch 2334: loss: 15.15 gradNorm: 29.77 time 0.08\n",
      "Train batch 2335: loss: 16.62 gradNorm: 32.10 time 0.08\n",
      "Train batch 2336: loss: 16.62 gradNorm: 35.25 time 0.08\n",
      "Train batch 2337: loss: 32.38 gradNorm: 50.85 time 0.07\n",
      "Train batch 2338: loss: 14.88 gradNorm: 27.43 time 0.08\n",
      "Train batch 2339: loss: 15.68 gradNorm: 28.12 time 0.08\n",
      "Train batch 2340: loss: 19.55 gradNorm: 33.27 time 0.08\n",
      "Train batch 2341: loss: 55.20 gradNorm: 57.40 time 0.14\n",
      "Train batch 2342: loss: 14.92 gradNorm: 28.21 time 0.08\n",
      "Train batch 2343: loss: 21.15 gradNorm: 38.36 time 0.08\n",
      "Train batch 2344: loss: 15.97 gradNorm: 31.65 time 0.07\n",
      "Train batch 2345: loss: 25.93 gradNorm: 39.87 time 0.09\n",
      "Train batch 2346: loss: 18.14 gradNorm: 33.77 time 0.08\n",
      "Train batch 2347: loss: 16.68 gradNorm: 31.28 time 0.07\n",
      "Train batch 2348: loss: 14.16 gradNorm: 29.04 time 0.08\n",
      "Train batch 2349: loss: 16.50 gradNorm: 31.54 time 0.08\n",
      "Train batch 2350: loss: 16.44 gradNorm: 33.18 time 0.08\n",
      "Val batch 2350: CER: 0.25 time 2.44\n",
      "Train batch 2351: loss: 19.08 gradNorm: 34.90 time 0.09\n",
      "Train batch 2352: loss: 17.18 gradNorm: 34.34 time 0.08\n",
      "Train batch 2353: loss: 23.13 gradNorm: 33.28 time 0.09\n",
      "Train batch 2354: loss: 16.58 gradNorm: 31.66 time 0.08\n",
      "Train batch 2355: loss: 16.54 gradNorm: 28.07 time 0.07\n",
      "Train batch 2356: loss: 18.44 gradNorm: 32.11 time 0.07\n",
      "Train batch 2357: loss: 19.21 gradNorm: 37.66 time 0.08\n",
      "Train batch 2358: loss: 17.03 gradNorm: 36.73 time 0.09\n",
      "Train batch 2359: loss: 14.50 gradNorm: 29.88 time 0.08\n",
      "Train batch 2360: loss: 58.90 gradNorm: 55.33 time 0.14\n",
      "Train batch 2361: loss: 13.95 gradNorm: 28.42 time 0.09\n",
      "Train batch 2362: loss: 16.26 gradNorm: 27.54 time 0.10\n",
      "Train batch 2363: loss: 14.56 gradNorm: 30.11 time 0.08\n",
      "Train batch 2364: loss: 16.44 gradNorm: 31.43 time 0.08\n",
      "Train batch 2365: loss: 24.04 gradNorm: 35.83 time 0.09\n",
      "Train batch 2366: loss: 22.19 gradNorm: 35.56 time 0.08\n",
      "Train batch 2367: loss: 20.66 gradNorm: 42.01 time 0.08\n",
      "Train batch 2368: loss: 25.95 gradNorm: 35.49 time 0.09\n",
      "Train batch 2369: loss: 13.86 gradNorm: 29.29 time 0.07\n",
      "Train batch 2370: loss: 17.04 gradNorm: 37.85 time 0.09\n",
      "Train batch 2371: loss: 21.75 gradNorm: 35.53 time 0.08\n",
      "Train batch 2372: loss: 13.32 gradNorm: 29.94 time 0.07\n",
      "Train batch 2373: loss: 18.88 gradNorm: 33.47 time 0.08\n",
      "Train batch 2374: loss: 13.79 gradNorm: 30.72 time 0.09\n",
      "Train batch 2375: loss: 16.53 gradNorm: 37.43 time 0.09\n",
      "Train batch 2376: loss: 14.45 gradNorm: 30.61 time 0.07\n",
      "Train batch 2377: loss: 56.56 gradNorm: 59.79 time 0.12\n",
      "Train batch 2378: loss: 17.08 gradNorm: 30.07 time 0.08\n",
      "Train batch 2379: loss: 20.88 gradNorm: 35.52 time 0.08\n",
      "Train batch 2380: loss: 19.03 gradNorm: 28.66 time 0.08\n",
      "Train batch 2381: loss: 16.75 gradNorm: 36.67 time 0.09\n",
      "Train batch 2382: loss: 14.18 gradNorm: 29.79 time 0.08\n",
      "Train batch 2383: loss: 22.03 gradNorm: 38.36 time 0.10\n",
      "Train batch 2384: loss: 17.41 gradNorm: 32.80 time 0.08\n",
      "Train batch 2385: loss: 13.34 gradNorm: 34.30 time 0.07\n",
      "Train batch 2386: loss: 16.19 gradNorm: 30.25 time 0.09\n",
      "Train batch 2387: loss: 17.30 gradNorm: 32.16 time 0.08\n",
      "Train batch 2388: loss: 12.82 gradNorm: 26.13 time 0.07\n",
      "Train batch 2389: loss: 14.60 gradNorm: 27.78 time 0.08\n",
      "Train batch 2390: loss: 28.22 gradNorm: 39.55 time 0.08\n",
      "Train batch 2391: loss: 18.38 gradNorm: 32.56 time 0.08\n",
      "Train batch 2392: loss: 20.71 gradNorm: 33.91 time 0.09\n",
      "Train batch 2393: loss: 18.76 gradNorm: 31.33 time 0.09\n",
      "Train batch 2394: loss: 14.08 gradNorm: 26.30 time 0.08\n",
      "Train batch 2395: loss: 12.86 gradNorm: 27.00 time 0.07\n",
      "Train batch 2396: loss: 13.99 gradNorm: 27.38 time 0.09\n",
      "Train batch 2397: loss: 13.48 gradNorm: 28.84 time 0.08\n",
      "Train batch 2398: loss: 15.14 gradNorm: 33.01 time 0.08\n",
      "Train batch 2399: loss: 14.30 gradNorm: 29.71 time 0.08\n",
      "Train batch 2400: loss: 16.31 gradNorm: 33.94 time 0.08\n",
      "Val batch 2400: CER: 0.24 time 2.68\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-2400\n",
      "Train batch 2401: loss: 16.53 gradNorm: 35.74 time 0.08\n",
      "Train batch 2402: loss: 16.93 gradNorm: 32.47 time 0.09\n",
      "Train batch 2403: loss: 51.51 gradNorm: 56.89 time 0.13\n",
      "Train batch 2404: loss: 56.04 gradNorm: 61.49 time 0.15\n",
      "Train batch 2405: loss: 55.30 gradNorm: 57.30 time 0.13\n",
      "Train batch 2406: loss: 17.37 gradNorm: 30.82 time 0.08\n",
      "Train batch 2407: loss: 16.63 gradNorm: 30.50 time 0.07\n",
      "Train batch 2408: loss: 13.56 gradNorm: 25.72 time 0.08\n",
      "Train batch 2409: loss: 13.93 gradNorm: 27.43 time 0.10\n",
      "Train batch 2410: loss: 18.32 gradNorm: 33.31 time 0.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch 2411: loss: 18.21 gradNorm: 42.86 time 0.08\n",
      "Train batch 2412: loss: 26.58 gradNorm: 39.43 time 0.08\n",
      "Train batch 2413: loss: 16.36 gradNorm: 30.60 time 0.07\n",
      "Train batch 2414: loss: 56.54 gradNorm: 63.73 time 0.14\n",
      "Train batch 2415: loss: 21.62 gradNorm: 37.38 time 0.08\n",
      "Train batch 2416: loss: 14.55 gradNorm: 30.99 time 0.08\n",
      "Train batch 2417: loss: 16.39 gradNorm: 31.71 time 0.08\n",
      "Train batch 2418: loss: 13.01 gradNorm: 29.95 time 0.08\n",
      "Train batch 2419: loss: 20.37 gradNorm: 39.49 time 0.08\n",
      "Train batch 2420: loss: 17.38 gradNorm: 36.17 time 0.09\n",
      "Train batch 2421: loss: 14.83 gradNorm: 31.57 time 0.09\n",
      "Train batch 2422: loss: 18.73 gradNorm: 34.60 time 0.08\n",
      "Train batch 2423: loss: 15.09 gradNorm: 30.52 time 0.07\n",
      "Train batch 2424: loss: 16.87 gradNorm: 39.59 time 0.07\n",
      "Train batch 2425: loss: 14.85 gradNorm: 30.51 time 0.09\n",
      "Train batch 2426: loss: 18.92 gradNorm: 35.52 time 0.08\n",
      "Train batch 2427: loss: 14.28 gradNorm: 29.52 time 0.07\n",
      "Train batch 2428: loss: 47.89 gradNorm: 60.43 time 0.14\n",
      "Train batch 2429: loss: 15.30 gradNorm: 31.74 time 0.07\n",
      "Train batch 2430: loss: 19.18 gradNorm: 29.67 time 0.07\n",
      "Train batch 2431: loss: 13.91 gradNorm: 32.24 time 0.09\n",
      "Train batch 2432: loss: 14.77 gradNorm: 27.68 time 0.07\n",
      "Train batch 2433: loss: 25.91 gradNorm: 36.01 time 0.08\n",
      "Train batch 2434: loss: 57.77 gradNorm: 60.43 time 0.14\n",
      "Train batch 2435: loss: 13.83 gradNorm: 28.44 time 0.07\n",
      "Train batch 2436: loss: 16.89 gradNorm: 29.02 time 0.09\n",
      "Train batch 2437: loss: 18.38 gradNorm: 33.75 time 0.07\n",
      "Train batch 2438: loss: 14.92 gradNorm: 32.72 time 0.08\n",
      "Train batch 2439: loss: 56.58 gradNorm: 56.99 time 0.12\n",
      "Train batch 2440: loss: 12.99 gradNorm: 26.73 time 0.07\n",
      "Train batch 2441: loss: 13.30 gradNorm: 28.29 time 0.08\n",
      "Train batch 2442: loss: 16.38 gradNorm: 31.25 time 0.08\n",
      "Train batch 2443: loss: 11.49 gradNorm: 23.82 time 0.07\n",
      "Train batch 2444: loss: 17.41 gradNorm: 28.74 time 0.09\n",
      "Train batch 2445: loss: 27.68 gradNorm: 41.73 time 0.09\n",
      "Train batch 2446: loss: 15.72 gradNorm: 35.66 time 0.07\n",
      "Train batch 2447: loss: 18.43 gradNorm: 39.77 time 0.07\n",
      "Train batch 2448: loss: 22.15 gradNorm: 39.46 time 0.11\n",
      "Train batch 2449: loss: 14.05 gradNorm: 26.65 time 0.09\n",
      "Train batch 2450: loss: 55.93 gradNorm: 62.33 time 0.13\n",
      "Val batch 2450: CER: 0.24 time 2.48\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-2450\n",
      "Train batch 2451: loss: 16.76 gradNorm: 29.66 time 0.08\n",
      "Train batch 2452: loss: 15.11 gradNorm: 27.86 time 0.08\n",
      "Train batch 2453: loss: 12.43 gradNorm: 25.30 time 0.09\n",
      "Train batch 2454: loss: 16.61 gradNorm: 31.29 time 0.08\n",
      "Train batch 2455: loss: 11.50 gradNorm: 24.18 time 0.10\n",
      "Train batch 2456: loss: 18.03 gradNorm: 29.48 time 0.08\n",
      "Train batch 2457: loss: 16.84 gradNorm: 33.52 time 0.07\n",
      "Train batch 2458: loss: 16.55 gradNorm: 31.81 time 0.08\n",
      "Train batch 2459: loss: 18.48 gradNorm: 35.06 time 0.07\n",
      "Train batch 2460: loss: 54.55 gradNorm: 56.95 time 0.13\n",
      "Train batch 2461: loss: 14.29 gradNorm: 40.30 time 0.08\n",
      "Train batch 2462: loss: 17.76 gradNorm: 31.55 time 0.07\n",
      "Train batch 2463: loss: 14.46 gradNorm: 26.45 time 0.06\n",
      "Train batch 2464: loss: 16.22 gradNorm: 34.47 time 0.08\n",
      "Train batch 2465: loss: 17.62 gradNorm: 32.32 time 0.08\n",
      "Train batch 2466: loss: 17.66 gradNorm: 37.68 time 0.09\n",
      "Train batch 2467: loss: 19.03 gradNorm: 43.27 time 0.08\n",
      "Train batch 2468: loss: 52.25 gradNorm: 55.69 time 0.13\n",
      "Train batch 2469: loss: 20.58 gradNorm: 32.80 time 0.09\n",
      "Train batch 2470: loss: 16.54 gradNorm: 37.92 time 0.08\n",
      "Train batch 2471: loss: 34.36 gradNorm: 46.21 time 0.10\n",
      "Train batch 2472: loss: 13.46 gradNorm: 28.24 time 0.07\n",
      "Train batch 2473: loss: 20.06 gradNorm: 33.40 time 0.07\n",
      "Train batch 2474: loss: 15.36 gradNorm: 27.58 time 0.08\n",
      "Train batch 2475: loss: 16.69 gradNorm: 32.43 time 0.08\n",
      "Train batch 2476: loss: 21.78 gradNorm: 38.74 time 0.08\n",
      "Train batch 2477: loss: 11.93 gradNorm: 23.59 time 0.08\n",
      "Train batch 2478: loss: 15.85 gradNorm: 30.81 time 0.09\n",
      "Train batch 2479: loss: 15.17 gradNorm: 28.99 time 0.08\n",
      "Train batch 2480: loss: 24.30 gradNorm: 38.54 time 0.11\n",
      "Train batch 2481: loss: 17.80 gradNorm: 32.18 time 0.09\n",
      "Train batch 2482: loss: 14.24 gradNorm: 25.11 time 0.07\n",
      "Train batch 2483: loss: 20.89 gradNorm: 39.96 time 0.08\n",
      "Train batch 2484: loss: 14.89 gradNorm: 29.06 time 0.08\n",
      "Train batch 2485: loss: 13.30 gradNorm: 25.37 time 0.09\n",
      "Train batch 2486: loss: 27.38 gradNorm: 46.38 time 0.08\n",
      "Train batch 2487: loss: 18.00 gradNorm: 35.98 time 0.08\n",
      "Train batch 2488: loss: 15.60 gradNorm: 28.58 time 0.07\n",
      "Train batch 2489: loss: 22.96 gradNorm: 33.70 time 0.08\n",
      "Train batch 2490: loss: 13.99 gradNorm: 30.01 time 0.08\n",
      "Train batch 2491: loss: 18.57 gradNorm: 33.23 time 0.08\n",
      "Train batch 2492: loss: 20.61 gradNorm: 37.92 time 0.08\n",
      "Train batch 2493: loss: 17.12 gradNorm: 29.70 time 0.08\n",
      "Train batch 2494: loss: 15.53 gradNorm: 32.20 time 0.08\n",
      "Train batch 2495: loss: 17.22 gradNorm: 31.69 time 0.09\n",
      "Train batch 2496: loss: 13.24 gradNorm: 29.55 time 0.08\n",
      "Train batch 2497: loss: 16.03 gradNorm: 32.14 time 0.08\n",
      "Train batch 2498: loss: 53.04 gradNorm: 61.68 time 0.13\n",
      "Train batch 2499: loss: 15.80 gradNorm: 29.11 time 0.09\n",
      "Train batch 2500: loss: 17.24 gradNorm: 34.89 time 0.07\n",
      "Val batch 2500: CER: 0.24 time 2.64\n",
      "Checkpoint saved /oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease2/ckpt-2500\n",
      "Train batch 2501: loss: 14.94 gradNorm: 27.04 time 0.10\n",
      "Train batch 2502: loss: 17.56 gradNorm: 32.89 time 0.08\n",
      "Train batch 2503: loss: 16.51 gradNorm: 35.41 time 0.08\n",
      "Train batch 2504: loss: 13.84 gradNorm: 28.83 time 0.08\n",
      "Train batch 2505: loss: 11.77 gradNorm: 26.64 time 0.08\n",
      "Train batch 2506: loss: 58.96 gradNorm: 58.75 time 0.13\n",
      "Train batch 2507: loss: 16.34 gradNorm: 30.25 time 0.09\n",
      "Train batch 2508: loss: 18.78 gradNorm: 33.62 time 0.07\n",
      "Train batch 2509: loss: 14.81 gradNorm: 31.12 time 0.09\n",
      "Train batch 2510: loss: 19.22 gradNorm: 34.01 time 0.11\n",
      "Train batch 2511: loss: 28.72 gradNorm: 42.08 time 0.11\n",
      "Train batch 2512: loss: 27.66 gradNorm: 41.10 time 0.08\n",
      "Train batch 2513: loss: 20.65 gradNorm: 37.75 time 0.08\n",
      "Train batch 2514: loss: 20.52 gradNorm: 32.78 time 0.08\n",
      "Train batch 2515: loss: 32.07 gradNorm: 46.70 time 0.08\n",
      "Train batch 2516: loss: 20.03 gradNorm: 38.23 time 0.09\n",
      "Train batch 2517: loss: 49.71 gradNorm: 56.78 time 0.13\n",
      "Train batch 2518: loss: 17.39 gradNorm: 32.76 time 0.08\n",
      "Train batch 2519: loss: 19.64 gradNorm: 44.02 time 0.08\n",
      "Train batch 2520: loss: 16.90 gradNorm: 28.22 time 0.09\n",
      "Train batch 2521: loss: 53.91 gradNorm: 58.22 time 0.12\n",
      "Train batch 2522: loss: 16.57 gradNorm: 30.97 time 0.08\n",
      "Train batch 2523: loss: 16.40 gradNorm: 33.59 time 0.08\n",
      "Train batch 2524: loss: 16.11 gradNorm: 30.44 time 0.07\n",
      "Train batch 2525: loss: 15.40 gradNorm: 28.47 time 0.07\n",
      "Train batch 2526: loss: 28.92 gradNorm: 49.28 time 0.09\n",
      "Train batch 2527: loss: 32.19 gradNorm: 47.22 time 0.10\n",
      "Train batch 2528: loss: 13.54 gradNorm: 35.29 time 0.07\n",
      "Train batch 2529: loss: 16.22 gradNorm: 29.98 time 0.10\n",
      "Train batch 2530: loss: 15.94 gradNorm: 34.16 time 0.08\n",
      "Train batch 2531: loss: 16.34 gradNorm: 28.29 time 0.07\n",
      "Train batch 2532: loss: 11.60 gradNorm: 24.51 time 0.07\n",
      "Train batch 2533: loss: 17.16 gradNorm: 32.54 time 0.07\n",
      "Train batch 2534: loss: 19.94 gradNorm: 32.83 time 0.09\n",
      "Train batch 2535: loss: 16.06 gradNorm: 30.32 time 0.09\n",
      "Train batch 2536: loss: 15.77 gradNorm: 31.97 time 0.08\n",
      "Train batch 2537: loss: 21.68 gradNorm: 35.17 time 0.09\n",
      "Train batch 2538: loss: 13.53 gradNorm: 30.41 time 0.08\n",
      "Train batch 2539: loss: 12.60 gradNorm: 29.36 time 0.08\n",
      "Train batch 2540: loss: 15.09 gradNorm: 27.83 time 0.09\n",
      "Train batch 2541: loss: 24.73 gradNorm: 42.88 time 0.08\n",
      "Train batch 2542: loss: 14.50 gradNorm: 35.25 time 0.08\n",
      "Train batch 2543: loss: 12.81 gradNorm: 27.73 time 0.08\n",
      "Train batch 2544: loss: 17.76 gradNorm: 33.38 time 0.11\n",
      "Train batch 2545: loss: 16.92 gradNorm: 37.83 time 0.08\n",
      "Train batch 2546: loss: 12.90 gradNorm: 27.06 time 0.07\n",
      "Train batch 2547: loss: 18.13 gradNorm: 32.03 time 0.08\n",
      "Train batch 2548: loss: 16.63 gradNorm: 37.35 time 0.07\n",
      "Train batch 2549: loss: 15.49 gradNorm: 30.45 time 0.09\n",
      "Train batch 2550: loss: 50.24 gradNorm: 58.69 time 0.12\n",
      "Val batch 2550: CER: 0.24 time 2.71\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 -m neuralDecoder.main \\\n",
    "    dataset=speech_release_baseline \\\n",
    "    model=gru_stack_inputNet \\\n",
    "    learnRateDecaySteps=10000 \\\n",
    "    nBatchesToTrain=10000  \\\n",
    "    learnRateStart=0.02 \\\n",
    "    model.nUnits=1024 \\\n",
    "    model.stack_kwargs.kernel_size=32 \\\n",
    "    outputDir=/oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final/derived/rnns/baselineRelease"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096704dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use the language model, make sure you've unzipped the languageModel.tar.gz file\n",
    "#and have compiled the code in the LanguageModelDecoder folder\n",
    "aseDir = '/oak/stanford/groups/shenoy/fwillett/speechPaperRelease_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "583a68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import tensorflow as tf\n",
    "from neuralDecoder.neuralSequenceDecoder import NeuralSequenceDecoder\n",
    "import neuralDecoder.utils.lmDecoderUtils as lmDecoderUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "271c8970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I0707 14:32:25.127691 37509 brain_speech_decoder.h:52] Reading fst /oak/stanford/groups/henderj/stfan/code/nptlrig2/LanguageModelDecoder/examples/speech/s0/lm_order_exp/3gram/data/lang_test/TLG.fst\n",
      "I0707 14:33:34.774243 37509 brain_speech_decoder.h:58] Reading lm fst /oak/stanford/groups/henderj/stfan/code/nptlrig2/LanguageModelDecoder/examples/speech/s0/lm_order_exp/3gram/data/lang_test/G.fst\n",
      "I0707 14:33:51.572348 37509 brain_speech_decoder.h:70] Reading rescore fst /oak/stanford/groups/henderj/stfan/code/nptlrig2/LanguageModelDecoder/examples/speech/s0/lm_order_exp/3gram/data/lang_test/G_no_prune.fst\n",
      "I0707 14:36:48.752995 37509 brain_speech_decoder.h:81] Reading symbol table /oak/stanford/groups/henderj/stfan/code/nptlrig2/LanguageModelDecoder/examples/speech/s0/lm_order_exp/3gram/data/lang_test/words.txt\n"
     ]
    }
   ],
   "source": [
    "#loads the language model, could take a while and requires ~60 GB of memory\n",
    "lmDir = baseDir+'/languageModel'\n",
    "ngramDecoder = lmDecoderUtils.build_lm_decoder(\n",
    "    lmDir,\n",
    "    acoustic_scale=0.8,\n",
    "    nbest=1,\n",
    "    beam=18\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the RNN on the test partition and competitionHoldOut partition\n",
    "testDirs = ['test','competitionHoldOut']\n",
    "trueTranscriptions = [[],[]]\n",
    "decodedTranscriptions = [[],[]]\n",
    "for dirIdx in range(2):\n",
    "    ckptDir = baseDir + '/derived/rnns/baselineRelease'\n",
    "\n",
    "    args = OmegaConf.load(os.path.join(ckptDir, 'args.yaml'))\n",
    "    args['loadDir'] = ckptDir\n",
    "    args['mode'] = 'infer'\n",
    "    args['loadCheckpointIdx'] = None\n",
    "\n",
    "    for x in range(len(args['dataset']['datasetProbabilityVal'])):\n",
    "        args['dataset']['datasetProbabilityVal'][x] = 0.0\n",
    "\n",
    "    for sessIdx in range(4,19):\n",
    "        args['dataset']['datasetProbabilityVal'][sessIdx] = 1.0\n",
    "        args['dataset']['dataDir'][sessIdx] = baseDir+'/derived/tfRecords'\n",
    "    args['testDir'] = testDirs[dirIdx]\n",
    "\n",
    "    with tf.device('/CPU:0'):  # Change to GPU:0 to run on GPU\n",
    "    # Initialize model\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        nsd = NeuralSequenceDecoder(args)\n",
    "\n",
    "        # Inference\n",
    "        out = nsd.inference()\n",
    "    decoder_out = lmDecoderUtils.cer_with_lm_decoder(ngramDecoder, out, outputType='speech_sil', blankPenalty=np.log(2))\n",
    "\n",
    "    def _ascii_to_text(text):\n",
    "        endIdx = np.argwhere(text==0)\n",
    "        return ''.join([chr(char) for char in text[0:endIdx[0,0]]])\n",
    "\n",
    "    for x in range(out['transcriptions'].shape[0]):\n",
    "        trueTranscriptions[dirIdx].append(_ascii_to_text(out['transcriptions'][x,:]))  \n",
    "    decodedTranscriptions[dirIdx] = decoder_out['decoded_transcripts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923e06dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1854970124932102, 0.16925995754567183, 0.20179986165548894)\n"
     ]
    }
   ],
   "source": [
    "from neuralDecoder.utils.lmDecoderUtils import _cer_and_wer as cer_and_wer\n",
    "\n",
    "#get word error rate and phoneme error rate for the test set (cer is actually phoneme error rate here)\n",
    "cer, wer = cer_and_wer(decodedTranscriptions[0], trueTranscriptions[0], outputType='speech_sil', returnCI=True)\n",
    "\n",
    "#print word error rate\n",
    "print(wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b38c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the sentence predictions for the test set\n",
    "print(decodedTranscriptions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80671ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the predictions for the competition hold-out set (labels are unreleased)\n",
    "print(decodedTranscriptions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0441de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the predictions for competition submission. This generates a .txt file that can be submitted.\n",
    "with open('baselineCompetitionSubmission.txt', 'w') as f:\n",
    "    for x in range(len(decodedTranscriptions[1])):\n",
    "        f.write(decodedTranscriptions[1][x]+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d3ab35a",
   "metadata": {},
   "source": [
    "Optionally, if you have access to high-end machine with least **330GB of RAM**, and a **GPU with 12GB of RAM**, you can run the following rescoring step to get better decoding accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5208843c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I0711 12:18:06.942003 14533 brain_speech_decoder.h:52] Reading fst /scratch/users/stfan/lm_models/speech_5gram/TLG.fst\n",
      "I0711 12:22:48.091068 14533 brain_speech_decoder.h:58] Reading lm fst /scratch/users/stfan/lm_models/speech_5gram/G.fst\n",
      "I0711 12:23:47.952838 14533 brain_speech_decoder.h:70] Reading rescore fst /scratch/users/stfan/lm_models/speech_5gram/G_no_prune.fst\n",
      "I0711 12:39:09.893329 14533 brain_speech_decoder.h:81] Reading symbol table /scratch/users/stfan/lm_models/speech_5gram/words.txt\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/groups/henderj/stfan/.conda/env/py3.9/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /share/software/user/open/cuda/11.7.1/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/groups/henderj/stfan/.conda/env/py3.9/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/groups/henderj/stfan/.conda/env/py3.9/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/groups/henderj/stfan/.conda/env/py3.9 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/groups/henderj/stfan/.conda/env/py3.9/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/share/software/user/open/cuda/11.7.1/targets/x86_64-linux/lib64')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ce583d052e4023be6778edb1451623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delete the previous 3gram model\n",
    "del ngramDecoder\n",
    "\n",
    "## Load a 5gram model with an unpruned LM\n",
    "lmDir = '/scratch/users/stfan/lm_models/speech_5gram'\n",
    "ngramDecoder = lmDecoderUtils.build_lm_decoder(\n",
    "    lmDir,\n",
    "    acoustic_scale=0.5,\n",
    "    nbest=100,\n",
    "    beam=18\n",
    ")\n",
    "\n",
    "MODEL_CACHE_DIR = '/scratch/users/stfan/huggingface'\n",
    "# Load OPT 6B model\n",
    "llm, llm_tokenizer = lmDecoderUtils.build_opt(cacheDir=MODEL_CACHE_DIR,\n",
    "                                              device='auto',\n",
    "                                              load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd6ef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cd7ddc7ca34ecfa37f5ced411e2017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_weight=0.5 wer=(0.14584464964693103, 0.12977665854605186, 0.16272724464691138)\n"
     ]
    }
   ],
   "source": [
    "acoustic_scale = 0.5\n",
    "blank_penalty = np.log(7)\n",
    "llm_weight = 0.5\n",
    "\n",
    "\n",
    "nbest_outputs = []\n",
    "logits = lmDecoderUtils.rearrange_speech_logits(out['logits'], has_sil=True)\n",
    "logitLengths = out['logitLengths']\n",
    "for i in range(len(logits)):\n",
    "    nbest = lmDecoderUtils.lm_decode(ngramDecoder,\n",
    "                                     logits[i, :logitLengths[i]],\n",
    "                                     blankPenalty=blank_penalty,\n",
    "                                     returnNBest=True,\n",
    "                                     rescore=True)\n",
    "    nbest_outputs.append(nbest)\n",
    "\n",
    "llm_out = lmDecoderUtils.cer_with_gpt2_decoder(llm,\n",
    "                                               llm_tokenizer,\n",
    "                                               nbest_outputs,\n",
    "                                               acoustic_scale,\n",
    "                                               out,\n",
    "                                               outputType='speech_sil',\n",
    "                                               returnCI=True,\n",
    "                                               lengthPenalty=0,\n",
    "                                               alpha=llm_weight)\n",
    "\n",
    "print(f\"wer={llm_out['wer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "31dbc90ff8be8e0a8a1447c7a80e172ed4b63217a4717472cac324f0452da048"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
